%! EINLEITUNG
%!
\chapter{Einleitung}

\section{Motivation}
Der Klimawandel stellt eine der größten Herausforderungen unserer Zeit dar.
Seine Realität und die dringende Notwendigkeit, seine Hauptverursacher und Treibfaktoren zu bremsen, sind längst unbestreitbar.
Dies wird unter anderem durch den im März 2023 veröffentlichten Bericht des \ac{IPCC} deutlich bestätigt.
Der Bericht zeigt auf, dass die globale Oberflächentemperatur im Zeitraum von 2011 bis 2020 um 1,09 Grad Celsius höher lag als im Zeitraum von 1850 bis 1900 und identifiziert die nicht nachhaltige Energienutzung als einen der Haupttreiber dieser Veränderungen~\cite{IPCC.2023}.
Auch die neuesten Zahlen des EU-Erdbeobachtungsdienstes Copernicus bekräftigen diesen Trend.
Dieser gab bekannt, dass das Jahr 2023 das wärmste Jahr seit Beginn der globalen Temperaturaufzeichnungen (1850) ist~\cite{CopernicusClimateChangeService.09.01.2024}.
Die neuesten Veröffentlichungen des Beobachtungsdienstes von Anfang Februar diesen Jahres beschreiben außerdem, dass die 1,5-Grad-Marke des Pariser Abkommens in den vergangenen zwölf Monaten erstmals gebrochen wurde.
Die globale Durchschnittstemperatur der Monate Februar 2023 bis Januar 2024 liegt 1,52 Grad über dem vorindustriellen Niveau~\cite{Eichhorn.8.2.2024}.
%!Gleichzeitig war es lange Zeit üblich, quasi unendlich Rechenkapazität etc. zur Verfügung zu haben und auch zu nutzen

%In diesem Zusammenhang stellt sich natürlich die Frage nach der Ursache für diese Entwicklung.
In diesem Kontext spielt die Technologiebranche eine zweischneidige Rolle.
Der rasante technologische Fortschritt hat zu einem exponentiellen Wachstum der Rechenkapazitäten geführt.
%Besonders im Bereich der Softwareentwicklung ist ein stetiges Wachstum von Technologien und Anwendungen zu verzeichnen.
%!Kommentar Thomas: Übergang zu Technologiebranche zu groß. Technologie kann alles mögliche sein.
Einerseits geht dieses Wachstum mit einem erheblichen Anstieg des Energieverbrauchs einher.
Schätzungen zufolge trägt die Technologiebranche mit mehr als 10\% zu den jährlichen globalen Kohlenstoffemissionen bei~\cite{Buchanan.2023}.
Andererseits steckt in dieser Branche auch das Potenzial, erheblich zur Verringerung dieser Auswirkungen beizutragen.

Angesichts dieser Entwicklungen hat die Bedeutung nachhaltiger Technologien und Praktiken signifikant zugenommen.
Es ist von entscheidender Wichtigkeit, dass die \ac{IT}-Branche ihre Verantwortung wahrnimmt und aktiv dazu beiträgt, den Klimawandel durch innovative und nachhaltige Lösungen zu bekämpfen.
Die Entwicklung und Implementierung von energieeffizienten und kohlenstoffarmen Technologien und Methoden in der Softwareentwicklung ist daher nicht nur eine technologische Herausforderung, sondern auch eine moralische Verpflichtung gegenüber zukünftigen Generationen.

Zwar werden die Stromnetze weltweit auf kohlenstoffärmere Quellen umgestellt, doch dieser Wandel kann nicht unmittelbar geschehen, sondern benötigt Zeit und ist nur möglich, wenn die Stromnachfrage nach wie vor gedeckt werden kann.
Um dieser Schwierigkeit Abhilfe zu schaffen, soll betrachtet werden, wie der Übergang beschleunigt werden kann und fossile Brennstoffe verzichtbarer gemacht werden können, indem die Nutzung erneuerbarer Energiequellen priorisiert wird~\cite{GreenSoftwareFoundation.2022}.
%effizienter Code oft sowohl grüner als auch schneller ist als ineffizienter Code
%aber: keine Selbstverständlichkeit, "grün", "effizient" und "performant" sind keine Synonyme
%offensichtlichste Zweck der Code-Effizienz: Anzahl und Größe der Maschinen zu reduzieren, die für das Hosting Ihrer Dienste erforderlich sind --> Anzahl der Nutzer, Zuverlässigkeit und Leistungsniveau; = Maximierung der Hardwareproduktivität, sehr umweltfreundliches Konzept
%weniger Server --> weniger Strom für den Betrieb --> das System enthält weniger Kohlenstoff (jede Maschine enthält Kohlenstoff, der bei ihrer Herstellung und Entsorgung ausgestoßen wird)
%früher nicht viel Hardwarekapazität (Maschinen waren langsam, Anschaffung teuer, Unterbringung in Rechenzentren kostspielig) --> Verwendung von hocheffizienten Sprachen wie C, sodass ausführbare Dateien klein und minimale Anzahl der CPU-Zyklen pro Vorgang; Services haben nicht zu viele Nachrichten mit anderen Maschinen ausgetauscht, Daten auf der Festplatte wurden nicht ständig abgefragt
%Warum nicht einfach dahin zurückkehren?
%Maschinen und Netzwerke verdoppelten ihre Kapazität alle achtzehn Monate (Moore´s law), sind heute mindestens drei Größenordnungen schneller als in den 90er Jahren
%Dienste von Drittanbietern wurden innovativer und wertvoller
%Nutzerzahlen stiegen, erwarteten mehr Funktionen und schnellere Entwicklung
%Sicherheitsbedrohungen wurden immer beängstigender und häufiger
%Hauptziel Maschinenproduktivität hat sich in letzten zwei Jahrzenten hin zur Entwicklerproduktivität geändert
%!siehe "Nutzen Code-Effizienz" von Building Green Software

%Der Klimawandel ist eine der größten Herausforderungen der heutigen Zeit.
%Längst ist er nicht mehr abzustreiten und macht es notwendig, die Auslöser und Beschleuniger zu bremsen.
%Der vom Menschen gemachte Klimawandel ist nicht mehr abzustreiten.
%%Es wurde mehrfach bewiesen, dass der Klimawandel Realität ist.
%Er wurde mehrfach bewiesen, zuletzt zum Beispiel durch den im März 2023 veröffentlichten Bericht des IPCC (Intergovernmental Panel on Climate Change).
%Dieser hält fest, dass die globale Oberflächentemperatur 2011 bis 2020 um 1,09 Grad Celsius wärmer war, als in den Jahren 1850 bis 1900 und nennt als einen wichtigen Auslöser eine nicht nachhaltige Energienutzung~\cite{IPCC.2023}.
%
%Die Technologiebranche spielt eine wichtige Rolle beim Klimawandel.
%Der technologische Fortschritt hat einen scheinbar unendlichen Zugang zu Rechenleistung verschafft.
%Im Bereich der Softwareentwicklung wird seit längerer Zeit eine rasche Entwicklung von Technologien verzeichnet.
%Damit einher kommt ein steigender Energieverbrauch~\cite{Buchanan.2023}.
%Schätzungsweise beläuft sich der Anteil der Technologiebranche an den jährlichen Kohlenstoffemissionen auf mehr als 10\%.
%Angesichts dessen hat der Entwurf von nachhaltigen Technologien und Praktiken an Wichtigkeit gewonnen.
%Zustande kommen diese Emissionen hauptsächlich durch zwei Faktoren:
%Zum einen durch die Herstellung von Benutzergeräten für Anwendungen, sogenannter verkörperter (englisch embodied) Kohlenstoff, und zum anderen durch den Betrieb der Anwendungen in Rechenzentren~\cite{Currie.2024}.

%Das rasche Wachstum von Softwaretechnologien und die steigende Popularität von Cloud-Diensten haben einen scheinbar unendlichen Zugang zu Rechenleistung verschafft.
%Damit einher kommt ein steigender Energieverbrauch

\section{Forschungsziele und Fragestellungen}
Das primäre Ziel dieser Arbeit liegt darin, die Nachhaltigkeit von Softwareanwendungen durch die Entwicklung eines Prognosemodells zur Vorhersage der Kohlenstoffintensität zu verbessern.
Insbesondere soll untersucht werden, wie die genauen Vorhersagen genutzt werden können, um den Aufbau der Softwarearchitektur und den Betrieb der Anwendung möglichst nachhaltig zu gestalten.
Die Arbeit wird durch folgende spezifische Forschungsfragen definiert:

\begin{enumerate}
 \item Entwicklung eines Vorhersagemodells: Wie kann ein effektives \ac{KI}-basiertes Modell zur Vorhersage der Kohlenstoffintensität des Stromnetzes entwickelt werden? Welche Rolle spielen dabei verschiedene Datentypen, wie Klimadaten und historische Daten zur Kohlenstoffintensität?
 \item Nachhaltigkeitsoptimierung von Softwareanwendungen: Inwiefern können diese Vorhersagen dazu verwendet werden, um die Architektur und den Betrieb von Software nachhaltiger zu gestalten?
 \item Simulation und Bewertung: Wie kann die Wirksamkeit der entwickelten Strategien durch Simulationen getestet und bewertet werden?
 \item Beitrag zur nachhaltigen Softwareentwicklung: Wie trägt diese Arbeit zur Förderung der Nachhaltigkeit in der Softwareentwicklung bei? Welche praktischen Erkenntnisse und Empfehlungen können abgeleitet werden? Stehen die gewonnenen Verbesserungen der Nachhaltigkeit im Verhältnis zum Aufwand der Implementierung?
\end{enumerate}
%Das Ziel dieser Arbeit ist die Entwicklung eines KI-basierten Prognosemodells, das mithilfe von Zeitreihenanalysen die Kohlenstoffintensität des Stromnetzes von Ländern in Europa vorhersagt.
%Durch die Integration dieses Modells in eine \ac{API} soll eine Schnittstelle geschaffen werden, die es Softwareanwendungen ermöglicht, ihre Betriebszeiten an die Kohlenstoffintensität des Stromnetzes anzupassen.
%Die zentralen Forschungsfragen umfassen:
%
%Wie kann ein KI-basiertes Prognosemodell effektiv die Kohlenstoffintensität des Stromnetzes vorhersagen?
%Wie können die Vorhersagen genutzt werden, um nachhaltige Scheduling-Strategien in Softwarearchitekturen zu implementieren?
%Ist die dadurch gewonnene Verbesserung der Nachhaltigkeit von Softwareanwendungen lohnenswert?
\section{Struktur der Arbeit}
\section{Themenabgrenzung}
Im Folgenden werden gezielte Abgrenzungen vorgenommen, um den Umfang und Fokus der Arbeit zu definieren und gleichzeitig relevante, jedoch nicht zentrale Themen auszugrenzen.
Diese Abgrenzungen schaffen einen klaren und strukturierten Rahmen für die Forschung und legen den Grundstein für mögliche zukünftige Arbeiten.

Nachhaltigkeit ist ein vielschichtiges Konzept, jedoch liegt der Schwerpunkt dieser Arbeit auf der ökologischen Dimension, insbesondere auf der Reduktion der Kohlenstoffintensität.
Soziale und ökonomische Nachhaltigkeitsaspekte werden nur am Rande behandelt.
Neben den Einflüssen, die die Software ausmacht, ist die Hardware ein wichtiger Faktor.
Bei ihrer Herstellung und Entsorgung freigesetzte, sogenannte verkörperte, Emissionen können einen großen Anteil ausmachen.
%!Kommentar Thomas: Software hat auch Einfluss auf verkörperte Emissionen, weil effizienter die Software ist und je weniger Hardware sie benötigt, desto weniger verkörptere Emissionen, weil Hardware wird besser genutzt.
Dieser Aspekt und daraus resultierenden Maßnahmen sind zwar signifikant, werden jedoch in dieser Arbeit nicht direkt behandelt, sondern bieten einen Ansatzpunkt für weiterführende Forschung.
Indirekt werden jedoch auch die verkörperten Emissionen thematisiert, denn Maßnahmen wie der effiziente Betrieb von Software gehen beispielsweise oft mit einer Reduzierung dieser einher.
% Indirekt sind die jedoch auch die verkörperten Emissionen betroffen, denn je effizienter die Software gestaltet ist, desto weniger Hardware benötigt sie zur Ausführung und desto weniger verkörperte Emissionen sind damit verbunden.
Die Software betreffend werden mögliche Bereiche, die hinsichtlich Nachhaltigkeit betrachtet werden können, eingeschränkt.
Nicht Teil der Arbeit sind zum Beispiel Maßnahmen auf Code-Ebene, wie die Verwendung nachhaltiger Programmiersprachen und Frameworks, effiziente Datenverwaltung und -speicherung oder nachhaltig gestaltete Benutzeroberflächen.
Der Fokus liegt allein auf Nachhaltigkeits-Verbesserungen durch die Nutzung kohlestoffarmen Stroms.
%Wichtig ist auch die Betrachtung von Hardwareeinflüssen, wie den bei Herstellung und Entsorgung freigesetzten verkörperten Kohlenstoffemissionen.
%Diese sind zwar signifikant, werden jedoch in dieser Arbeit nicht behandelt, bieten jedoch interessante Ansatzpunkte für weiterführende Forschung.

%Obwohl Nachhaltigkeit ein vielschichtiges Konzept ist, fokussiert sich diese Arbeit primär auf die ökologische Dimension der Nachhaltigkeit, insbesondere auf die Reduktion der Kohlenstoffintensität.
%Soziale und ökonomische Aspekte der Nachhaltigkeit werden nur am Rande behandelt.

Des Weiteren ist sowohl der technologische, als auch der geografische Rahmen des Vorhersagemodells begrenzt.
Die Umsetzung erfolgt durch einen Temporal Fusion Transformer und weitere möglicherweise passende Ansätze zur Umsetzung werden nicht untersucht.
Die Analyse beschränkt sich auf europäische Stromnetze, weil ein weltweiter Umfang eine enorm größer Datenmenge bedeuten würde und in möglichen anschließenden Forschungen umgesetzt werden könnte.
\section{Related Work}

%!
%!
\chapter{Grundlagen zu nachhaltiger Softwarearchitektur}
\section{Definition von Nachhaltigkeit in der Softwareentwicklung}
Das Ziel der vorliegenden Arbeit besteht darin, eine Möglichkeit zu finden, die Nachhaltigkeit von Softwarearchitekturen zu optimieren.
An dieser Stelle wird deshalb als Grundlage der Begriff der Nachhaltigkeit definiert und erklärt, welche Rolle Nachhaltigkeit in der Softwareentwicklung spielt.

Vom Bundesministerium für wirtschaftliche Zusammenarbeit und Entwicklung~\cite{BundesministeriumWirtschaftlicheZusammenarbeitundEntwicklung} wird Nachhaltigkeit wie folgt definiert:
\glqq Nachhaltigkeit oder nachhaltige Entwicklung bedeutet, die Bedürfnisse der Gegenwart so zu befriedigen, dass die Möglichkeiten zukünftiger Generationen nicht eingeschränkt werden.\grqq{}
Dabei erstrecke sich Nachhaltigkeit gleichermaßen auf drei verschiedene Dimensionen, nämlich wirtschaftlich effizient, sozial gerecht und ökologisch tragfähig.
Laut Duden ist Nachhaltigkeit in Bezug auf Ökologie das \glqq Prinzip, nach dem nicht mehr verbraucht werden darf, als jeweils nachwachsen, sich regenerieren, künftig wieder bereitgestellt werden kann\grqq{}~\cite{Dudenredaktion.27.04.2018}.

Die im Jahr 1992 auf der Konferenz für Umwelt und Entwicklung der Vereinten Nationen verabschiedeten Agenda 21 legte Nachhaltigkeit als übergreifendes Ziel der Politik fest.
Außerdem wurden in der Agenda 2030, die 2015 von der Weltgemeinschaft beschlossen wurde, 17 Nachhaltigkeitsziele definiert.
Davon sieht Ziel zwölf Nachhaltigkeit in Produktion und Konsum vor.
Die deutsche Ausarbeitung dieses Ziels besagt, dass dazu unter anderem der Verzicht auf fossile Energieträger, wie Kohle, Gas und Öl und stattdessen die Nutzung von erneuerbaren Energien gehört~\cite{Bundesregierunginformiert}.

Betrachtet man den Begriff vor dem Aspekt der Softwareentwicklung, so lassen sich die Prinzipien nachhaltiger Software wie folgt zusammenfassen~\cite{Calero.2015}:
\begin{itemize}
 \item Überwachung, Messung, Bewertung, sowie Optimierung des Ressourcen- und Energieverbrauchs während Herstellung und Nutzung der Software
 %direkten (während Herstellung und Nutzung) und indirekten () Verbrauchs natürlicher Ressourcen, der durch den Einsatz und die Nutzung entsteht, bereits im Entwicklungsprozess;
 %wobei sich direkt auf den Verbrauch während der Herstellung und Nutzung bezieht und indirekt auf die Verwendung des Softwareprodukts zusammen mit anderen Prozessen und langfristigen systemischen Auswirkungen
\item Möglichkeit zur kontinuierlichen Auswertung und Optimierung des Einsatzes und der Nutzungsfolgen der Software
\item Zyklische Bewertung und Minimierung des Verbrauchs von natürlichen Ressourcen und Energie während der Entwicklungs- und Produktionsprozesse
\end{itemize}
Nachhaltige Software zeichnet sich also dadurch aus, dass die negativen Auswirkungen auf die drei Bereiche der Nachhaltigkeit (Wirtschaft, Gesellschaft und Umwelt) während Entwicklung, Einsatz und Nutzung der Software gering gehalten werden oder sich sogar positiv auf diese auswirken.

Die \ac{IT} spielt für das Thema Nachhaltigkeit eine wichtige Rolle und das sowohl als Teil des Problems, als auch als Teil der Lösung.
Diese Arbeit spricht von den drei genannten Ausprägungen der Nachhaltigkeit hauptsächlich die ökologische Tragfähigkeit an, die auch als \glqq grüne\grqq{} oder \glqq umweltfreundliche\grqq{} Dimension bezeichnet wird.
Betreffend dieser Dimension kann die \ac{IT} zwei verschiedene Rollen annehmen, bezeichnet als Green in \ac{IT} und Green by \ac{IT}\@.
Tabelle~\ref{tab:GreenInByIT} grenzt die beiden Begriffe voneinander ab.
Beschrieben wird jeweils die Rolle der \ac{IT}, das Ziel und das Potenzial der beiden Bereiche, untermauert durch je ein Beispiel~\cite{Calero.2015}.

\begin{table}[t]
 \centering\small
 \caption{Green in IT vs. Green by IT}
 \label{tab:GreenInByIT}
 \input{\tabledir/GreenInByIT.tex}
\end{table}

Die Unterteilung ist sowohl für Software als auch für Hardware relevant.
Demnach bestehen vier Hauptkategorien:
Green in Software, Green in Hardware, Green by Software, Green by Hardware.
Diese Arbeit konzentriert sich nicht auf die Hardware-bezogenen Aspekte.
Sie thematisiert die Kohlenstoffintensität und damit die Emissionen, die Software verursacht und ist deshalb der Kategorie Green in Software zuzuordnen.
Betrachtet man die Prognose der Kohlenstoffintensität mithilfe von Künstlicher Intelligenz, so könnte man dabei die \ac{IT} auch als Hilfsmittel zur Verbesserung der Nachhaltigkeit ansehen und somit Green by Software zuordnen~\cite{Calero.2015}.

Von der Green Software Foundation werden die Stellschrauben für grüne Software in drei Faktoren unterteilt, nämlich Energie-Effizienz, Hardware-Effizienz und Kohlenstoff-Bewusstsein.
Energieeffiziente Software zeichnet sich dadurch aus, dass sie so wenig Energie wie möglich verbraucht.
Hardware-Effizienz wird erreicht, wenn bei der ausführenden Hardware die Menge des verkörperten Kohlenstoffs auf ein Minimum beschränkt wird.
Diese beiden Faktoren sind zwar wichtig, aber werden in der vorliegenden Arbeit nicht weiter berücksichtigt, denn der Fokus liegt auf dem bewussten Umgang mit Kohlenstoff.
Ein bewusster Umgang mit Kohlenstoff setzt voraus zu verstehen, dass die gleiche Menge an verbrauchter Energie nicht immer die gleiche Kohlenstoffintensität hat, da diese je nach Zeitpunkt und Ort des Verbrauchs variiert~\cite{GreenSoftwareFoundation.2022}.

\section{Kohlenstoffintensität als Maß für Nachhaltigkeit}
Im Folgenden gilt es, Kohlenstoffintensität zu definieren und die Frage zu klären, wie diese mit Nachhaltigkeit zusammenhängt.
Der Begriff Kohlenstoff wird häufig als Synonym für alle Treibhausgase und als Oberbegriff für die Auswirkungen aller Arten von Emissionen und Aktivitäten auf die globale Erwärmung verwendet~\cite{GreenSoftwareFoundation.2022}.
Die Kohlenstoffintensität wird in Gramm \ac{CO2} pro \ac{kWh} angegeben und sagt aus, wie viel Kohlendioxid pro verbrauchter \ac{kWh} Strom emittiert wird~\cite{LyndonRuff.20220420T15:34:17.000Z}.
\ac{CO2} gehört zu den Treibhausgasen.
Diese kommen zwar natürlich in der Erdatmosphäre vor, um Wärme zurückzuhalten.
Jedoch sind sie durch menschliche und industrielle Aktivitäten im Überfluss vorhanden, was einen globalen Temperaturanstieg und Klimakatastrophen zur Folge hat~\cite{Currie.2024}.
Kohlenstoffintensität ist also ein Maß der Nachhaltigkeit des erzeugten oder verbrauchten Stroms.
Je geringer der Wert ist, desto nachhaltiger ist der Strom.
Genauer betrachtet führt ein verringerter Anteil von Energiequellen mit hoher \ac{CO2}-Emission (z.B.\ Braun- oder Steinkohle) am erzeugten Strom zu einem geringeren Emissionsfaktor des Stromnetzes.
Eine weitere Größe, die den Emissionsfaktor beeinflusst, ist der Wirkungsgrad des Energieträgers.
Ein Anstieg des Wirkungsgrads von Energiequellen mit hoher \ac{CO2}-Emission führt letztlich ebenfalls zu einem geringeren Emissionsfaktor des Stromnetzes, weil dann weniger dieser Quellen benötigt werden, um die gleiche Menge an Strom zu erzeugen~\cite{Icha.2020}.

Um Strom zu erzeugen, muss Energie einer anderen Form in elektrische Energie umgewandelt werden.
In Kohlekraftwerken wird zum Beispiel Kohle in einem Kessel verbrannt um Dampf zu erzeugen, der chemische Energie in elektrische Energie umwandelt.
Als Nebenprodukt wird viel Kohlendioxid freigesetzt, was die Stromerzeugung durch fossile Brennstoffe zu einer kohlenstoffintensiven Variante macht~\cite{Currie.2024}.
Stellt man sich vor, dass ein Gerät seinen Strom direkt aus einem Windkraftwerk beziehen würde, hätte diese Energie eine Kohlenstoffintensität von 0 g \ac{CO2}/\ac{kWh}, weil bei der Energieerzeugung durch Wind kein Kohlenstoff freigesetzt wird~\cite{GreenSoftwareFoundation.2022}.
Das Stromnetz ist jedoch ein Mix verschiedener Energiequellen (deshalb auch als Strommix bezeichnet), die sich unterschiedlich auf die Kohlenstoffintensität auswirken.
Bei der Verwendung von Strom kann nicht zwischen verschiedenen Erzeugungsarten ausgewählt werden, weshalb nur die Intensität des Gesamtnetzes betrachtet werden kann.
Durch erneuerbare Energien, wie Wind-, Solar-, oder Wasserenergie, erzeugter Strom ist weniger kohlenstoffintensiv, jedoch variiert seine Verfügbarkeit je nach Zeit und Ort der Stromerzeugung.
Die Zeit ist ein wichtiger Faktor, weil ein direkter Zusammenhang zwischen Zeit und Wetter besteht und vom Wetter wiederum die erneuerbaren Energien abhängig sind.
Außerdem verfügen verschiedene Länder und Regionen über eine unterschiedliche Zusammensetzung des Stromnetzes.
Um die \ac{CO2}-Bilanz des verwendeten Stroms so gering wie möglich zu halten, ist es deshalb wichtig zu wissen, wann und wo die Kohlenstoffintensität des Stromnetzes gering ist.
%So kann zum Beispiel, wenn es die Anforderungen erlauben, die Nutzung des Stroms auf kohlenstoffarme Zeiten verschoben werden.
Hinzu kommt, dass erneuerbare Energien in Betrieb und Wartung verhältnismäßig billig sind, was dazu führt, dass deren erzeugter Strom nicht nur umweltfreundlicher, sondern meist auch kostengünstiger ist~\cite{NationalGrid.20231106T13:28:05.000Z}.

Ein weiterer wichtiger Faktor ist das Zusammenspiel aus Energieangebot und -nachfrage.
Die Nachfrage kann stark schwanken, doch muss das Stromnetz trotzdem immer in der Lage sein, diese zu decken.
Optimal wäre es natürlich, das Angebot unverzüglich an die Nachfrage anpassen zu können, jedoch variiert die Flexibilität je nach Energiequelle stark.
Betrachtet man zum Beispiel Windenergie, liegt auf der Hand, dass diese über eine sehr geringe Flexibilität verfügt, weil die Windstärke nicht kontrolliert werden kann.
Bei Kohleenergie ist es weitaus einfacher, auf ein Angebotsdefizit zu reagieren.
Eine wichtige Größe ist dabei die marginale Kohlenstoffintensität~\cite{GreenSoftwareFoundation.2022}.
Übersteigt der Strombedarf plötzlich die vorhandene Energie(?), wird die benötigte Energie aus dem sogenannten Grenzkraftwerk bezogen.
%Steigt der Strombedarf plötzlich an,
Dieses zeichnet sich dadurch aus, dass es auf solche Änderungen schnell reagieren kann.
Um den Strompreis so gering wie möglich zu halten, werden die verfügbaren Energiequellen aufsteigend ihres Preises angeordnet~\cite{Corradi.20231207T10:48:51.000Z}.
Diese sogenannte Merit-Order (im Deutschen als Reihenfolge der Vorteilhaftigkeit bezeichnet) ist das Zusammenspiel aus Stromangebot, -preis und -bedarf.
Eine beispielhafte Merit-Order des deutschen Stromnetzes ist in Abbildung~\ref{FIG:merit-order} dargestellt.
\begin{figure}
 \caption{Die Merit-Order des deutschen Stromnetzes~\cite{Gro.5.10.2022}}
 {\includegraphics[width=1\textwidth]{\figdir/merit-order}}
 \label{FIG:merit-order}
\end{figure}
Die verschiedenen Energiequellen sind nach Preis aufsteigend angeordnet und durch den eingezeichneten Strombedarf wird ersichtlich, welche dieser Quellen je nach Bedarf herangezogen werden.
Eine ausreichende Stromversorgung wird durch Hinzuziehen einer geeigneten Menge an Kraftwerken garantiert, wobei die Quellen mit niedrigstem Preis die höchste Priorität haben.
Daraus lässt sich schließen, dass der Strom sowohl günstiger als auch grüner wird, je mehr erneuerbare Energien eingespeist werden~\cite{Gro.5.10.2022}.

Es bestehen verschiedene Ansätze für die Berechnung der Kohlenstoffintensität.
Allgemein stellt sich die Berechnung als anspruchsvoll heraus, vor allem weil jedes Land seine eigene Zusammenstellung verschiedener Kraftwerke hat~\cite{Currie.2024}.
Für eine Berechnungsweise (Formel~\ref{eq:ci}), die Forscher aus Großbritannien für die Carbon Intensity \ac{API} verwendet haben, wird der Anteil der verschiedenen Energiequellen, ein Intensitätsfaktor je Energiequelle und die Stromnachfrage benötigt.
\begin{equation}
 \label{eq:ci}
 C_t = \frac{\sum_{g=1}^{G} P_{g,t} \times c_g}{D_t}
\end{equation}
Die Kohlenstoffintensität Ct zur Zeit t ist das Produkt aus Energiequelle und Intensitätsfaktor summiert für alle verwendeten Energiequellen und anschließender Teilung durch die Nachfrage~\cite{LyndonRuff.20220420T15:34:17.000Z}.
Für \ac{CO2}-Emissionen durch erneuerbare Energien sowie durch Kernkraft wird der Intensitätsfaktor mit 0 berechnet, sodass für diese Energiequellen das Gesamtergebnis der Kohlenstoffintensität gleich 0 ist.

Ein weiterer Berechnungsansatz, der von der Green Software Foundation speziell für Software entwickelt wurde, ist der sogenannte \ac{SCI} Wert.
Er gibt die Kohlenstoffemissionen einer funktionalen Einheit R in g\ac{CO2}/\ac{kWh} an~\cite{GreenSoftwareFoundation.2022}.
In seine Berechnung fließen die benötigte Energie E, die standortbezogene marginale Kohlenstoffemissionen I und die verkörperten Emissionen M der zugrunde liegenden Hardware ein.
Die entsprechende Formel ist in Formel~\ref{eq:sci} zu sehen.
R ist eine Skaliereinheit, die für die zu messende Software relevant ist und kann zum Beispiel pro Benutzer oder pro Gerät sein~\cite{Buchanan.2023}.
\begin{equation}
 \label{eq:sci}
 SCI = ((E \times I) + M) pro R
\end{equation}

%!
%!
\chapter{Strategien zur Nachhaltigkeitsoptimierung}
\section{Scheduling}
\subsection{Grundlagen des Scheduling}
Unter Scheduling versteht man im Allgemeinen die optimierte Zuteilung von Ressourcen zu Aufgaben in bestimmten Zeiträumen.
Dabei kann die zum Ziel gesetzte Optimierung vielerlei Ausführungen haben, zum Beispiel die Fertigstellungszeit so gering wie möglich zu halten oder die Anzahl der zu spät vollendeten Aufgaben zu minimieren~\cite{Gawiejnowicz.2020}.

Ein Scheduling-Problem S wird durch vier verschiedene Größen definiert~\cite{Gawiejnowicz.2020}:
\begin{itemize}
 \item Die Menge an auszuführenden Arbeiten J
 \item Die Menge an ausführenden Einheiten M
 \item Die Menge an zusätzlichen für die Ausführung benötigten Einheiten R
 \item Ein Maß für die Qualität der Lösung %ϕ.
\end {itemize}
Außerdem hat ein Scheduling-Problem verschiedene Eigenschaften, die im Folgenden kurz erklärt werden~\cite{Pinedo.2022}:
\begin{itemize}
 \item Bearbeitungszeit: Die Bearbeitungszeit eines Auftrags auf einer Maschine. Diese kann abhängig von der jeweiligen Maschine sein.
 \item Freigabedatum: Der Zeitpunkt, an dem der Auftrag im System eintritt und somit der frühestmögliche Zeitpunkt für den Beginn des Auftrags
 \item Fälligkeitsdatum: Der Zeitpunkt der zugesagten Fertigstellung. Muss das Fälligkeitsdatum eingehalten werden, wird es als Frist bezeichnet.
 \item Gewicht: Ein Prioritätsfaktor, der die Bedeutung des Auftrags im Verhältnis zu den anderen Aufgaben im System angibt
\end{itemize}

\subsection{Möglicher Einfluss von Scheduling auf Kohlenstoffemissionen}
Die Green Software Foundation definiert Kohlenstoffbewusstsein als Modifizierung von Berechnungen, um die Umwelt so wenig wie möglich zu belasten~\cite{GreenSoftwareFoundation.2022}.
Als Lösung sollen Software-Workloads, die viel Rechenleistung benötigen, auf Zeiten und Orte verlagert werden, bei denen der Strommix zu den geringst möglichen Kohlenstoffemissionen führt.
Für viele Unternehmen bietet die Verlagerung in die Cloud einen Ansatz der Verbesserung durch zentrale Verwaltung, Konsolidierung von Ressourcen und Effizienzsteigerungen.
Durch zunehmendes Migrieren in die Cloud wird es umso wichtiger, dass die entsprechenden Betreiber sich um den ökologischen Fußabdruck ihres Services kümmern~\cite{Buchanan.2023}.

Zur Veranschaulichung, welchen Einfluss die zeitliche Verschiebung von Workloads haben kann, soll zunächst ein beispielhaftes Verhältnis von Stromangebot und -nachfrage betrachtet werden (Abbildung~\ref{FIG:grid-supply-demand}).
\begin{figure}
 \caption{Eine beispielfhafte Darstellung von Angebot und Nachfrage eines Stromnetzes~\cite{Peixeiro.2022}.}
 {\includegraphics[width=0.9\textwidth]{\figdir/grid-supply-demand}}
 \label{FIG:grid-supply-demand}
\end{figure}
In dieser vereinfachten Darstellung wird davon ausgegangen, dass zur Mittagszeit die Verfügbarkeit von Solarstrom besonders hoch ist.
Aus diesem Grund übersteigt das Angebot in diesem Zeitraum die Nachfrage.
Dieses überschüssige Angebot könnte genutzt werden und der höhere Stromverbrauch würde keine zusätzlichen Emissionen verursachen.
Andererseits muss zu den Zeiten, zu denen kein oder nur wenig Solarstrom zur Verfügung steht, auf Gas zurückgegriffen werden, um die Nachfrage decken zu können~\cite{Buchanan.2023}.
Eine Studie von Microsoft~\cite{Dodge.06212022} aus dem Jahr 2022 hat aufgezeigt, dass durch Zeitverschiebung durchschnittlich 15\% weniger Kohlenstoffintensität (gemessen am \ac{SCI}) erreicht werden kann.
Diese hat sich zwar auf Aufgaben von Künstlicher Intelligenz in der Cloud konzentriert, sie zeigt jedoch, dass die zeitliche Verlagerung von rechenintensiven Workloads ein großes Verbesserungspotenzial birgt.
Das größte Potenzial der Zeitverschiebung liegt demnach in der Verschiebung von Workloads mit kurzer Dauer und großem Volumen in Regionen mit großer Unstetigkeit~\cite{Buchanan.2023}.

Einen weitaus größeren Einfluss als die zeitliche Verlagerung kann laut der Studie jedoch die örtliche Verlagerung haben.
Die Kohlenstoffintensität kann dadurch um bis zu 75\% reduziert werden~\cite{Dodge.06212022}.

\subsection{Dynamische Scheduling Modelle}
Es sollen zwei zeitliche Optimierungsmethoden untersucht werden, die im folgenden kurz beschrieben werden~\cite{Dodge.06212022}:
\begin{itemize}
 \item Flexibler Start: Ein flexibler Workload wird zu einem Zeitpunkt mit minimaler marginalen Kohlenstoffintensität in den nächsten N Stunden gestartet.
 Dabei wird der Workload nicht unterbrochen, sondern wird bis zur Fertigstellung ausgeführt.
 Es werden alle möglichen Startzeiten im Zeitfenster N mit einer Granularität von fünf Minuten betrachtet.
 \item Anhalten und Fortsetzen: Ein Workload wird (möglicherweise mehrmals) angehalten und wieder neu gestartet, um sicherzustellen, dass er ausschließlich zu kohlenstoffarmen Zeiten läuft.
 Der Workload wird dadurch in den nächsten (N + Dauer des Auftrags) Stunden durchgeführt.
 Wichtige Voraussetzungen für diese Methode sind ebenfalls die zeitliche Flexibilität und zudem funktioniert sie nur für Workloads, die unterbrochen und wieder neu aufgenommen werden können.
 Zur Umsetzung werden genügend viele fünfminütige Intervalle mit geringsten Grenzemissionen während des Zeitfensters (N + Dauer des Auftrags) gesucht.
 Es wird davon ausgegangen, dass die Unterbrechung und der Neustart des Auftrags sofort erfolgen und keine zusätzliche Energie verbraucht wird.
 Dies ähnelt den Spot-Instanzen auf bestehenden Cloud-Plattformen, die einen vom Nutzer festgelegten Preis als Schwellenwert für automatisches Pausieren verwenden.
\end {itemize}
Zu beachten ist dabei, dass unterschiedliche Regionen sich durch verschieden starke Varianzen auszeichnen.
In manchen Regionen schwankt die Emissionsgröße innerhalb eines Tages stark, wodurch sich diese sehr gut für die Methode des Anhaltens und Fortsetzens eignen.
Andere Regionen haben dagegen relativ gleichbleibende Emissionen, sodass zeitliche Optimierungen nur einen geringen Einfluss haben können.
In Abbildung~\ref{FIG:flexible-start} wird die mögliche Emissionsverringerung durch einen flexiblen Start anhand des DenseNet 201 veranschaulicht.
DenseNet201 ist ein Convolutional Neural Network mit wenigen Schichten, das in der Studie von~\cite{Dodge.06212022} weniger als eine halbe Stunde benötigte.
Bei vielen Regionen ist eine Emissionsreduzierung um mehr als 20\% möglich, das beste Ergebnis liegt bei 80\% für die Region West US\@.
Die Wirksamkeit der eingesetzten Methode hängt demnach auch stark von der betrachteten Region ab~\cite{Dodge.06212022}.
\begin{figure}
 \caption{Die zu erwarteten Emissionseinsparungen durch Verschiebung der Startzeit um bis zu 24 Stunden (Flexible Start) für Dense 201~\cite{Dodge.06212022}}
 {\includegraphics[width=0.7\textwidth]{\figdir/flexible-start_measuring-carbon-intensity-of-ai}}
 \label{FIG:flexible-start}
\end{figure}
Betrachtet man ein zeitintensiveres Experiment, wie das Training eines Language Modells mit sechs Millionen Parametern, ist das Gegenteil zu beobachten.
Beim flexiblen Start ist das Optimierungspotential sehr gering (meist weniger als 1\%), jedoch bietet das Anhalten und Fortsetzen gute Optimierungsergebnisse.
In vielen Regionen sind Emissionsreduzierungen um mehr als 10\% und in der Region West US erneut am meisten mit ungefähr 28\% möglich~\cite{Dodge.06212022}.

\section{Scaling (vertikal und horizontal)}
Skalierung bedeutet im Kontext des Softwarebetriebs eine Erhöhung oder Verringerung der Rechen-, Speicher- oder Netzwerkressourcen, die für einen bestimmten Workload genutzt werden.
Wohingegen man in herkömmlichen, dedizierten Hosting-Umgebungen durch die verfügbaren Hardwareressourcen begrenzt ist, bietet eine Cloud-basierte Anwendung diese Möglichkeit.
Es kann zudem zwischen vertikaler Skalierung (Scaling Up) und horizontaler Skalierung (Scaling Out) unterschieden werden.
Bei der vertikalen Skalierung wird die Rechenleistung erhöht und die Infrastruktur beibehalten~\cite{AlibabaCloudCommunity.20240118T09:19:15.000Z}.

\section{Demand Shifting}
Eine Software-Anwendung kann nachhaltiger betrieben werden, indem der Strombedarf auf die Kohlenstoffintensität ausgerichtet wird.
Durch sogenanntes Demand-Shifting, also der Verschiebung der Nachfrage je nach Kohlenstoffintensität, kann Studien zufolge zwischen 45\% und 99\% Kohlenstoff eingespart werden.
Diese Verlagerung kann sowohl auf zeitlicher Ebene als auch auf örtlicher Ebene stattfinden.
Bei der örtlichen Verlagerung wird ein Ort, bei dem der Strom im Moment kohlenstoffärmer ist, gewählt.
Für die örtliche Verlagerung kann man sich die unterschiedlichen Wetterverhältnisse weltweit zu nutzen machen, denn wenn es z.B. in Deutschland im Moment wolkig und windstill ist, können an einem anderen Ort bei wolkenfreiem Himmel und Sonnenschein die perfekten Verhältnisse für die Solarstrom-Erzeugung herrschen.
Für die zeitliche Verlagerung ist ein umfangreiches Wissen über die Kohlenstoffintensität bzw. die Wetterverhältnisse im Verlauf der Zeit nötig.
Auf Grundlage dessen können Workloads in Zeiten mit geringer Kohlenstoffintensität verlagert werden~\cite{GreenSoftwareFoundation.2022}.
Das Demand Shifting auf örtlicher Ebene kann als eine Form des Scheduling betrachtet werden, denn beide verfolgen das Grundprinzip der optimalen zeitlichen Planung der Ausführung eines Workloads.

\section{Demand Shaping}
Eine ähnliche Methode, die ebenfalls das Ziel verfolgt, die Kohlenstoffemissionen zu senken, ist das Demand-Shaping.
Dabei wird bei geringer Kohlenstoffintensität die volle Funktionalität geboten und bei hoher Kohlenstoffintensität Einschränkungen getroffen oder die Funktionalität reduziert~\cite{Currie.2024}.

Man kann Demand Shifting und Demand Shaping also voneinander abgrenzen, indem man beim Demand Shifting einen bestimmten Workload betrachtet und die Verhältnisse dafür entweder auf örtlicher oder auf zeitlicher Ebene anpasst.
Beim Demand Shaping werden die aktuellen Verhältnisse betrachtet (Ort und Zeit sind unveränderlich) und der Workload, genauer gesagt die Höhe der Stromnachfrage, wird an diese angepasst.

Am besten anwendbar sind diese Methoden bei nicht-zeitkritischen Aufgaben.
Diese können auf Zeiten mit niedriger Kohlenstoffintensität verschoben werden (Demand-Shifting) und zu solchen Zeiten können viele dieser Aufgaben abgearbeitet werden, wodurch sich der Bedarf erhöht (Demand-Shaping)~\cite{Currie.2024}.
Das Demand Shaping kann als eine Form des Scalings betrachtet werden, weil beide Prinzipien die Betriebsgröße bzw. Kapazität unter bestimmten Voraussetzungen anpassen.

\section{Herausforderungen und vorhandene Tools}
Zwei konkrete Beispiele für Demand Shifting ist zum einen ein Projekt von Google, bei dem ein Modell entwickelt wurde, um die Kohlenstoffintensität und die Arbeitslast für den jeweils folgenden Tag vorherzusagen und entsprechend die Arbeitslast an die Kohlenstoffintensität zu binden.
Zum anderen hat Microsoft für Windows 11 einen Ansatz etabliert, bei dem sich die Ausführung von Windows-Updates nach der Kohlenstoffintensität richtet~\cite{GreenSoftwareFoundation.2022}.

Ein allgegenwärtiges Beispiel für Demand Shaping ist der bei Geräten wie Autos, Waschmaschinen oder Spülmaschinen verfügbare Eco-Modus.
Das Prinzip dahinter ist, dass auf Leistung verzichtet wird für weniger Ressourcenverbrauch.
Dies könnte auch auf Software-Anwendungen ausgeweitet werden.
Wichtig ist dabei, dass der Eco-Modus lediglich eine Option ist, aber der Leistungsverzicht nicht dauerhaft oder verpflichtend ist~\cite{GreenSoftwareFoundation.2022}.

Verwaltete Cloud-Dienste bieten aufgrund ihrer hohen Rechendichte und ihrer enormen Hardware- und Energienutzung ein großes Potenzial zur Effizienzverbesserung~\cite{Currie.2024}.
Die drei großen Cloud-Anbieter Google Cloud, Azure und AWS haben sich zur Kohlenstoffneutralität verpflichtet.
Dies bedeutet jedoch nicht, dass es allein ausreichend ist, eine Anwendung auf einer dieser Plattformen bereitzustellen und sich nicht weiter mit dem Thema Nachhaltigkeit auseinander setzten muss.
Auch die Anwendung selbst muss für Nachhaltigkeit optimiert sein und die vorhanden Möglichkeiten müssen genutzt werden~\cite{Newman.2023}.
Der deutsche Bitkom hat in einer Metastudie auf Grundlage von mehreren Studien aus den Jahren 2012 bis 2019 zwei Hauptkategorien für Energieensparungen definiert: Anwendungen und Infrastruktur.
In Abbildung~\ref{FIG:sustainability-infrastructure} werden die Facetten der Nachhaltigkeit auf Anwendungs- und Infrastrukturebene dargestellt.
Von Amazon~\cite{Mokhtari.2023} wird dieses Modell als Modell der geteilten Verantwortungen vorgestellt.
Der Cloud-Anbieter ist für die Nachhaltigkeit der Cloud verantwortlich, dies beinhaltet unter anderem die Bereitstellung einer effizienten, gemeinsam genutzten Infrastruktur, die Wasserverantwortung und die Beschaffung erneuerbarer Energien.
Der Cloud-Nutzer ist für die Nachhaltigkeit in der Cloud verantwortlich, also zum Beispiel das Optimieren von Workloads und Ressourcennutzung und das Minimieren der Gesamtressourcen, die für die Workloads bereitgestellt werden müssen~\cite{Mokhtari.2023}.
\begin{figure}
 \caption{Nachhaltigkeit auf Anwendungs- und Infrastrukturebene, eigene Darstellung nach ~\cite{Mokhtari.2023}}
 {\includegraphics[width=1\textwidth]{\figdir/SustainabilityOfAndOnInfrastructure}}
 \label{FIG:sustainability-infrastructure}
\end{figure}

Für Großbritannien besteht ein Zusammenschluss zwischen dem nationalen Stromnetz, mehreren Nichtregierungsorganisationen und akademischen Einrichtungen, der eine \ac{API} konzipiert hat, um die Kohlenstoffintensität für verschiedene Regionen Großbritanniens vorherzusagen.
Diese Schnittstelle basiert auf maschinellem Lernen und ermöglicht Anwendern, ihren Stromverbrauch so zu planen, dass die Kohlenstoffemissionen auf regionaler Ebene minimiert werden~\cite{Currie.2024}.
Dafür wurde der Berechnungsansatz, der im vorherigen Kapitel (Formel~\ref{eq:ci}) erklärt wurde, verwendet.
Der Intensitätsfaktor der verschiedenen Energiequellen ist dabei spezifisch für die jeweilige Region oder das jeweilige Land.

Carbon Aware \ac{SDK} ist ein experimentelles Projekt der Green Software Foundation, welches als Hilfsmittel für Software gedacht ist, um die umweltfreundlichsten Energiequellen für deren Betrieb zu verwenden.
Es wird sowohl die zeitliche als auch die örtliche Komponente für die Verlagerung von Workloads berücksichtigt.
Carbon Aware \ac{SDK} kann als Web\ac{API} und als Command Line Interface verwendet werden und steht als Open-Source-Projekt zur Verfügung~\cite{GreenSoftwareFoundation.20231212T09:58:27.000Z}.

Electricity Maps bietet eine \ac{API} an, über die verschiedene Daten wie z.B.\ die Kohlenstoffintensität oder die Zusammensetzung des Strommixes abgefragt werden können.
Die Abdeckung umfasst über 160 Regionen weltweit und es sind historische, aktuelle und zukünftige (24 Stunden im Voraus) Werte verfügbar~\cite{ElectricityMaps.20231220T09:16:49.000Z}.

WattTime stellt ebenfalls eine \ac{API} zur Verfügung.
Über diese kann z.B.\ die marginale und die durchschnittliche Kohlenstoffintensität in der Vergangenheit, in Echtzeit oder 24 Stunden im Voraus abgefragt werden.
Die Daten sind in einem Intervall von fünf Minuten erhältlich~\cite{WattTime.20231130T19:28:06+00:00}.

%!
%!
\chapter{Grundlagen zur Zeitreihenprognose}
\section{Einführung in die Zeitreihenprognose}\label{CAP:intor-time-series-forecasting}
Zeitreihenprognose lässt sich unterteilen in die beiden Begriffe Zeitreihe und Prognose.
Eine Zeitreihe ist eine Reihe von Datenpunkten in zeitlicher Abfolge~\cite{Peixeiro.2022}.
Es ist eine Art von Datensatz, die angibt, wie sich ein bestimmter Sachverhalt im Laufe der Zeit verhält.
Die wichtigste Spalte eines Zeitreihendatensatzes ist die Zeitspalte, anhand derer er geordnet werden kann.
Um Zeitreihendaten verarbeiten zu können, müssen oft spezielle Techniken zur Vorverarbeitung und zum Feature-Engineering angewandt werden~\cite{Lazzeri.2021}.
Eine Prognose ist eine Vorhersage für die Zukunft, basierend auf Daten aus der Vergangenheit und mithilfe von Wissen über mögliche künftige Ereignisse, von denen die Entwicklung beeinflusst werden kann~\cite{Peixeiro.2022}.

Dadurch, dass man bei der Zeitreihenprognose -- wie typisch für Regressionsprobleme -- historische Daten hernimmt und zukünftige Werte als Funktion der vergangenen ausdrückt, könnte man die beiden im ersten Moment gleichsetzen.
Eine Zeitreihenprognose unterscheidet sich jedoch von anderen Regressionsaufgaben hauptsächlich in zwei Eigenschaften:
Zeitreihen haben eine Ordnung und sie müssen nicht zwingend über andere Merkmale als die Zielvariable verfügen.
Sie können durchaus lediglich aus der Zeitspalte und einer zugehörigen Wertespalte bestehen.
Das kann ausreichend sein, weil die Zeit in diesem Fall als einzige Variable die Zielspalte definiert.
Es ist wichtig, die zeitliche Reihenfolge der Werte einzuhalten, denn sie zeichnet die Beziehung der Werte untereinander aus und ist somit ausschlaggebend für das Ergebnis.
Bei klassischen Regressionsaufgaben wird oft eine Vielzahl von Merkmalen verwendet, mit deren Hilfe der zu bestimmende Wert definiert werden kann.
Im Gegensatz dazu können die Datensätze von Zeitreihenprognosen wesentlich schlanker sein~\cite{Peixeiro.2022}.

Jede Zeitreihe kann in drei verschiedene Komponenten zerlegt werden.
Der zugrunde liegende Vorgang wird als Dekomposition bezeichnet.
Diese Komponenten enthüllen Merkmale, die die Daten aufweisen:
Der Trend beschreibt die Veränderungen in einer Zeitreihe, die Saisonalität stellt sich wiederholende Zyklen über einen bestimmten Zeitraum dar und die Residuen, auch als Noise bezeichnet, definieren nicht durch den Trend oder die Saisonalität abzubildende, zufällige Fehler.
Zeitreihenvorhersagen eignen sich besonders gut, um den Verlauf und die Entwicklung von Werten über die Zeit analysieren zu können~\cite{Peixeiro.2022}.

\section{SARIMAX als statistisches Basismodell}
Für die Zeitreihenprognose soll ein statistisches Modell namens SARIMAX als Basismodell dienen.
SARIMAX (Seasonal Auto-Regressive Integrated Moving Average with exogenous variables) kommt folgendermaßen zustande:
Für das ARMA-Modell werden die autoregressive (AR) und die Moving-Average- (MA) Komponente zusammengefügt.
Ein autoregressiver Prozess geht davon aus, dass die Zielvariable linear von ihren eigenen Werten in der Vergangenheit abhängen.
Beim Moving-Average-Prozess (zu Deutsch gleitender Durchschnittsprozess) wird der aktuelle Wert als lineare Kombination des Mittelwerts der aktuellen und vergangenen Fehlerterme dargestellt.
Das \ac{ARIMA}-Modell enthält zusätzlich noch einen Vorverarbeitungsschritt (I) für die Modellierung nicht-stationärer Zeitreihen.
Das ARIMA-Modell in Abhängigkeit von der Ordnung des AR-Prozesses $p$ (linear Regression auf die letzten $p$ Werte der Zeitreihe), der Ordnung der Integration $d$ und der Ordnung des MV-Prozesses $q$ (lineare Regression auf die letzten $q$ Fehlerwerte), kann durch Formel~\ref{eq:arima} ausgedrückt werden~\cite{Peixeiro.2022}.
$p$ gibt an, wie viele zeitversetzte Werte hinzugegeben werden, wohingegen $q$ die Anzahl der zeitversetzten Fehlerterme definiert.
\begin{equation}
 \label{eq:arima}
 y'_t = C + \phi_1 y'_{t-1} + \ldots + \phi_p y'_{t-p} + \theta_1 \varepsilon_{t-1} + \ldots + \theta_q \varepsilon_{t-q} + \varepsilon_t
\end{equation}
Der aktuelle Wert der differenzierten Reihe $y'_t$ ist somit die Summe aus einer Konstanten $C$, den Vergangenheitswerten dieser differenzierten Reihe $\phi_p y'_{t-p}$, vergangenen Fehlertermen $\theta_1 \varepsilon_{t-1}$ und dem aktuellen Fehlerterm $\varepsilon_t$.
$d$ ist hier nicht explizit angegeben, definiert jedoch die Anzahl der Differenzierungen, bis eine Reihe stationär wird.

Um saisonale Muster berücksichtigen zu können, wird aus dem \ac{ARIMA}-Modell das SARIMA-Modell, welches schließlich um exogene Variablen (X), also die Zielvariable zusätzlich beeinflussende Variablen, erweitert wird.
Basierend auf Formel~\ref{eq:arima} ergibt sich Formel~\ref{eq:sarimax} für das SARIMAX-Modell.
\begin{equation}
 \label{eq:sarimax}
 d_t = C + \sum_{n=1}^{p} \alpha_n d_{t-n} + \sum_{n=1}^{q} \theta_n e_{t-n} + \sum_{n=1}^{P} \phi_n d_{t-sn} + \sum_{n=1}^{Q} \eta_n e_{t-sn} + \sum_{n=1}^{r} \beta_n x_{t-n} + \varepsilon_t
\end{equation}
Ergänzt wurde die Formel durch die Saisonalität in Form einer zusätzlichen AR- ($\phi_n d_{t-sn}$) und MA- ($\eta_n e_{t-sn}$) Komponente verschoben um die Frequenz der Saisonalität ($s$, z.B.\ 12: monatlich oder 24: stündlich) und um die exogenen Variablen ($\sum_{n=1}^{r} \beta_n x_{t-n}$)~\cite{Artley.26.4.2022}.

\section{Temporal Fusion Transformer als KI-basiertes Modell}
Neben dem Einsatz von SARIMAX als Basismodell, soll ein sogenannter \ac{TFT} angewendet werden.

Wissenschaftlern von Google gelang 2017 eine Forschungsarbeit~\cite{Vaswani.2017}, die die Forschung zur künstlichen Intelligenz vor allem im Bereich der natürlichen Sprachverarbeitung maßgeblich beeinflusste.
Sie stellten eine neue Architektur für neuronale Netze vor, sogenannte Transformer, die zunächst nur für die Sprachverarbeitung angewendet und getestet wurden, aber als grundlegendes Konzept auch andere Bereiche revolutionierten.
Anstelle von Feedback-Schleifen durch \ac{RNN}s, die üblicherweise in Encoder-Decoder-Architekturen verwendet wurden, setzten sie auf eine Methode namens \glqq Multi-headed Self-Attention\grqq{}.
Es stellte sich heraus, dass dieser neue Architekturansatz sich für verschiedenste Zwecke eignet und große Vorteile bietet, wie zum Beispiel die Parallelisierbarkeit, die Vielseitigkeit und nicht zuletzt die hohe Qualität der erzielten Ergebnisse~\cite{Vaswani.2017}.
Heute basieren bedeutende Modelle wie ChatGPT unter anderem auf Transformern.

Ein \ac{TFT} ist ein \ac{DNN} mit besonderer Architektur für das Multi-Horizon Forecasting, also die Vorhersage für mehrere zukünftige Zeitschritte.
Zunehmend wurden für diesen Anwendungsfall \ac{DNN}s eingesetzt, da sie die Leistung herkömmlicher Modelle übertreffen~\cite{Lim.19.12.2019}.
Die Idee des Attention-Mechanismus, der den Kern der Transformer-Architektur bildet, wurde für \ac{TFT} übernommen.
Um die Architektur auf die Varietät der Eingabedaten und deren zeitliche Beziehungen auszurichten, werden Codierer für die statischen Kovariaten, also Variablen zusätzlich zur Zielvariable, die Informationen über diese enthalten, eingesetzt, Gating-Mechanismen und stichprobenabhängige Variablenauswahl zur Minimierung irrelevanter Eingaben, eine Sequence-to-Sequence-Schicht zur Verarbeitung der Known und Observed Inputs und ein Self-Attention Decodierer zur Analyse der Eingabedaten auf langfristige Abhängigkeiten.
Bei anderen aufmerksamkeitsbasierten Methoden, die auch Transformer-basierte Modelle mit einschließen, besteht oft das Problem, dass die verschiedenen Datenquellen (z.B. statische Kovariaten) nicht ausreichend berücksichtigt werden.
Außerdem können sie oft die Bedeutung verschiedener Daten in einem bestimmten Zeitabschnitt unterscheiden.
Andere \ac{DNN}s neben dem \ac{TFT} haben zum Beispiel den Nachteil, dass sie die zeitliche Anordnung der Eingangsmerkmale nicht berücksichtigen (konventionelle Formen von LIME und SHAP), obwohl diese Abhängigkeit in Zeitreihen signifikant sind~\cite{Lim.19.12.2019}.
%\ac{DNN}s werden zunehmen für diese Aufgabe herangezogen, weil sie verglichen mit herkömmlichen Zeitreihenmodellen große Leistungsverbesserungen aufzeigen.

Auch wenn der \ac{TFT} ein großes Modell ist und am besten für große Datenmengen geeignet ist, erzielt er auch für kleine Datensätze mit ca. 20 000 Samples gute Ergebnisse~\cite{Beitner.2020}.
Er verwendet als Input verschiedenste Daten, wie in Abbildung~\ref{FIG:tft-forecasting} dargestellt~\cite{Lim.19.12.2019}:
\begin{itemize}
 \item Werte der Zielvariablen aus der Vergangenheit (Past Targets)
 \item Zeitlich abhängige bekannte Informationen sowohl für die Vergangenheit, als auch für die Zukunft (Known Inputs)
 \item Zeitlich abhängige jedoch nur in der Vergangenheit bekannte Eingabewerte (Observed Inputs)
 \item Feststehende, zeitunabhängige Daten, die einen kontextuellen Zusammenhang bieten (statische Kovariaten)
\end{itemize}
\begin{figure}
 \caption{Multi-horizon Forecasting mit dem Temporal Fusion Transformer~\cite{Lim.19.12.2019}.}
 {\includegraphics[width=0.7\textwidth]{\figdir/TFT_multi-horizon-forecasting}}
 \label{FIG:tft-forecasting}
\end{figure}
Die Ausgabe des Modells ist nicht ein einziger Wert, sondern ein Werteintervall (Prediction Interval).
Die Multi-Horizon Vorhersage ist deshalb so komplex, weil trotz weniger Informationen darüber, das Zusammenspiel und die Abhängigkeit der Daten untereinander erlernt werden müssen~\cite{Lim.19.12.2019}.

Jeder Wert der Zielvariablen $i$ ist an eine oder mehrere statische Kovariaten $s_i$, an zeitabhängige Variablen $\chi_{i,t}$ und skalare Zielvariablen $y_{i,t}$ zu jedem Zeitschritt $t$ gebunden.
Wie in vorheriger Auflistung deutlich wurde, können die zeitabhängigen Variablen weiter unterteilt werden in Known Inputs ($x_{i,t}$) und Observed Inputs ($z_{i,t}$).
Wichtig sind außerdem sogenannte Quantile, denn sie ermöglichen die Vorhersage von Ergebnisintervallen, anstatt nur einzelner Werte und bieten auf diese Weise die Aussage über Best- und Worst-Case Werte (z.B. Ausgabe des 10., 50. und 90. Perzentil jedes Zeitschrittes)~\cite{Lim.19.12.2019}.

Formel~\ref{eq:tft} beschreibt die Prognose des $q$-ten Quantils des $\tau$-ten Vorhersageschrittes zum Zeitpunkt $t$.
Dabei werden alle Informationen innerhalb des endlichen Look-Back-Windows $k$ mit einbezogen, inklusive der Zielvariablen $y$ bis einschließlich des Startpunkts der Vorhersage $t$ ($y_{i,t-k:t} = \{y_{j,t-k}, \ldots, y_{j,t}\}$) und bekannter Variablen $x$ für den gesamten Zeitraum ($x_{i,t-k:t+\tau} = \{x_{i,t-k}, \ldots, x_{i,t}, \ldots, x_{i,t+\tau}\}$)~\cite{Lim.19.12.2019}.
\begin{equation}
 \label{eq:tft}
 \hat{y}_i(q, \tau, T) = f_q(\tau, y_{j,t-k:t}, z_{i,t-k:t}, x_{j,t-k:t+T}, s_i)
\end{equation}

Abbildung~\ref{FIG:tft-architecture} zeigt den Aufbau eines \ac{TFT}s, wovon die wichtigsten Bestandteile im Folgenden kurz erklärt werden.
\begin{figure}
 \caption{Vereinfachte Darstellung des Aufbaus eines Temporal Fusion Transformers~\cite{Labiadh.2023}.}
 {\includegraphics[width=0.90\textwidth]{\figdir/TFT-architecture}}
 \label{FIG:tft-architecture}
\end{figure}

Gating-Mechanismen in Form von \ac{GRN}s verfolgen das Ziel, nicht-lineare Verarbeitung nur dort anzuwenden, wo sie nötig ist.
Sie dienen dazu, ungenutzte Komponenten zu überspringen, sodass die Tiefe und Komplexität des Netzwerkes anpassungsfähig ist, um ein breites Spektrum an Szenarien und Datensätzen abdecken zu können~\cite{Lim.19.12.2019}.

Ein Variable Selection Network dient dazu, die relevanten Input-Variablen für jeden Zeitschritt auszuwählen.
Dadurch, dass eine Vielzahl von Variablen verfügbar ist, ist es essenziell, die Relevanz und den Beitrag der einzelnen Variablen zu untersuchen.
Die Variablen Auswahl, in der Architektur-Abbildung unten zu finden, wird auf statische und zeitabhängige Kovariaten angewendet.
Sie hilft, die auffälligsten Merkmale herauszufinden und diejenigen, die sich negativ auf das Ergebnis auswirken, herauszufiltern.
Dadurch kann sie die Leistung des Modells erheblich verbessern~\cite{Lim.19.12.2019}.

Ein wichtiger Bestandteil sind außerdem die statischen Kovariaten-Kodierer (Static Covariate Encoders), die in Abbildung~\ref{FIG:tft-architecture} links zu finden sind.
Mit Hilfe von diesen werden verschiedene Kontextvektoren erstellt, nämlich für die Auswahl zeitlicher Variablen, die lokale Verarbeitung zeitlicher Variablen und das Anreichern von zeitlichen Merkmale mit statischen Variablen.
Diese werden an den verschiedenen Stellen im Decoder verknüpft, um statische Merkmale in das Netzwerk zu integrieren.
Das unterscheidet das \ac{TFT}-Netzwerk von anderen Architekturen für die Zeitreihenprognose, die für statische Metadaten keine separaten Kodierer verwenden~\cite{Lim.19.12.2019, Labiadh.2023}.

Sowohl kurzfristige als auch langfristige zeitliche Zusammenhänge zu erfassen ist eine der Hauptaufgaben bei der Zeitreihenvorhersage.
Um beides ausreichend abdecken zu können, werden bei einem \ac{TFT} Sequence-to-Sequence-Schichten (kurzfristige Prognosen) und ein Multi-Head-Attention-Block (langfristige Prognosen) eingesetzt.
Die Sequence-to-Sequence-Schichten sind die Alternative zur Positionskodierung, die in Transformern üblicherweise verwendet wird.
Solche Schichten überzeugen durch ihre Fähigkeit, zeitliche Muster durch rekurrente Verbindungen festzuhalten~\cite{Labiadh.2023}.
Der in anderen Transformer-Architekturen verwendete Multi-Attention-Block wurde leicht abgewandelt, um die zeitlichen Beziehungen über verschiedene Zeitschritte zu erlernen.
Dafür wird ein sogenannter Self-Attention-Mechanismus eingesetzt, der es ermöglicht, Beziehungen über mehrere Zeitschritte hinweg zu erlernen.
Bei diesem wurde im Vergleich zu anderen Transformer-basierten Architekturen die Erklärbarkeit verbessert~\cite{Lim.19.12.2019}.
Er bewertet die Wichtigkeit einzelner Werte anhand von Beziehungen zwischen Schlüsseln und Abfragen~\cite{Labiadh.2023}.

Besonders ist zudem die verwendete Loss-Funktion.
Ein \ac{TFT} wird dadurch trainiert, dass die Quantilverluste über alle Quantilausgaben, d.h. Aussagen über die Verteilung des Zielwerts, minimiert werden.
Die spezielle Quantile Loss-Funktion, auch Pinball Regression genannt, ist durch Formel~\ref{eq:tft-quantile-loss} beschreibbar.
Bei Unterschätzungen wird der erste Summand der Funktion für obere Quantile hoch gewichtet und bei Überschätzungen bekommt der zweite Summan mehr Gewicht~\cite{Labiadh.2023}.
\begin{equation}
 \label{eq:tft-quantile-loss}
 QL(y, \hat{y}, q) = q \max(0, y - \hat{y}) + (1 - q) \max(0, \hat{y} - y)
\end{equation}

Die neue Form der Interpretierbarkeit eines \ac{TFT} ermöglicht ein einfaches Zurückverfolgen der relevantesten Zeitschritte für jede Prognose, was traditionelle Saisonalitäts- und Autokorrelationsanalysen ersetzen kann.
Außerdem hilft sie, signifikante Veränderungen zu erkennen, indem ein durchschnittlicher Aufmerksamkeitswert pro Vorhersagebereich berechnet wird und zu jedem Zeitpunkt als Vergleich dient~\cite{Labiadh.2023}.

Die Vorteile der Architektur eines \ac{TFT} sind unter anderem die Möglichkeit zur Verwendung der Kovariaten, was auch dazu führt, dass es auch für kurze Zeitreihen gute Ergebnisse erzielen kann.
Weitere Vorteile sind, dass mehrere Ziele gleichzeitig und sogar heterogene Ziele mit kontinuierlichen und kategorischen Variablen, also Regression und Klassifikation zur selben Zeit, unterstützt werden.
Die Anwendung von sowohl Regression als auch Klassifikation unterscheidet ihn zum Beispiel von DeepAR, einem Modell zur Zeitreihenprognose von Amazon, welches ausschließlich die Regression anwendet.
Der \ac{TFT} berücksichtigt zudem die Unsicherheit in Form von Quantilen, was ihn von RecurrentNetwork mit einfacher \ac{LSTM}- oder \ac{GRU}-Schicht und Ausgabeschicht oder von NBeats und NHiTS unterscheidet.
Durch die Nutzung statischer Kovariaten kann der \ac{TFT} auch mit sogenannten \glqq Cold-Start-Problemen \grqq{}, also Prognosen ohne oder mit nur sehr wenigen historischen Daten, umgehen.
Die komplexe Architektur kann mit Blick auf Anforderungen an die Rechenleistung zum Nachteil werden.
Im Vergleich zu anderen Aufgaben zum Beispiel im Bereich Computer Vision oder Sprachverarbeitung sind Zeitreihendaten meist kleiner, wodurch eine höhere Komplexität des verwendeten Modells den positiven Effekt haben kann, dass die verwendete Hardware ausgereizt wird~\cite{PytorchForecastingDocumentation.20230410T20:05:43.000Z}.
Dies ist auch im Hinblick auf Nachhaltigkeit ein guter Ansatz, weil die Hardware effizient genutzt wird.
%!
%!
\chapter{Prognose der Kohlenstoffintensität}
\section{Auswahl, Vorverarbeitung und Analyse der Daten}
Die Kohlenstoffintensität von Strom hängt stark von der Verfügbarkeit erneuerbarer Energien ab und diese sind wiederum auf bestimmte Wetterverhältnisse angewiesen.
Um eine aussagekräftige Vorhersage machen zu können ist es deshalb sinnvoll, sowohl eine Beobachtung der Kohlenstoffintensitätswerte in der Vergangenheit anzustellen, als auch den Einfluss des Wetters zu beobachten.
Für einen Zeitpunkt in der Zukunft kann dann ein möglichst wahrscheinlicher Wert ausgehend vom zu erwartenden Wetter vorhergesagt werden.

Zum einen werden Werte der Kohlenstoffintensität aus der Vergangenheit benötigt, um die Kohlenstoffintensität abhängig von der Zeit vorhersagen zu können.
Als Maß wird dabei die marginale Kohlenstoffintensität verwendet.
Diese gibt die Emissionsintensität der Grenzkraftwerke an, die auf die veränderte Stromnachfrage reagieren~\cite{Buchanan.2023}.
Dadurch ist diese sehr gut für den vorliegenden Anwendungsfall geeignet.


Als Datengrundlage dient das Datenmodell zur \ac{MOER}, zu Deutsch Grenzbetriebsemissionsrate, von WattTime.
Die \ac{MOER} wird für einen bestimmten Zeitpunkt und Ort angegeben und kann dadurch gut verglichen werden.
Ziel ist es, die elektrische Last auf Zeiten mit niedrigen Werten zu verlagern, um die Umwelt weniger zu belasten.
Wie gut die Daten dafür geeignet sind, hängt von verschiedenen Faktoren ab, zum Beispiel ob \ac{MOER}-Werte in Echtzeit vorliegen oder wie genau diese Werte sind.
Die Schwierigkeit besteht darin, dass zwei Fälle miteinander verglichen werden müssen, nämlich dass die Lastverschiebung stattfindet und dass sie nicht stattfindet, von denen nicht beide gleichzeitig umgesetzt werden können.
Durch schwankende Bedingungen im Stromnetz ändert sich die \ac{MOER} ständig, weshalb man den Fall der Lastverschiebung und den Fall, in dem keine Last verschoben wird, nicht einfach hintereinander ausprobieren kann.
Eine mögliche Lösung dieses Problems ist, die Netzbedingungen mitzubetrachten und davon auszugehen, dass bei gleichen Bedingungen auch die MOER gleich ist.
Diesen Ansatz nutzt WattTime und überprüft dessen Genauigkeit durch verschiedenste Experimente~\cite{WattTime.2022}.
% pro kWh wird soviel mehr g CO2 ausgestossen
% Electricity Maps Beitrag zu MOER !

Die \ac{MOER}-Werte werden über die \ac{API} von WattTime pro Monat für die Länder Deutschland und Norwegen abgefragt.
Um auf die Daten zugreifen zu können, registrieren sich berechtigte Nutzer mit ihrer E-Mail-Adresse über einen Endpunkt und loggen sich anschließend über einen Login-Endpunkt ein, um einen Token zu erhalten.
Dieser Token ist eine halbe Stunde lang gültig und ermöglicht durch Setzen im Header die Abfrage der Daten über die \ac{API}~\cite{.20240220T17:59:19.000Z}.
Der Vorgang der Datenabfrage und Speicherung ist in Code-Ausschnitt~\ref{CODE:watttime_get_moer} zu sehen.
Die Daten werden ab dem frühst möglichen Zeitpunkt, also dem Zeitpunkt ab dem WattTime mit der Datenerfassung begonnen hat, abgefragt.
Für Deutschland ist das ab Oktober 2022 und für Norwegen ab Januar 2021.
Zur Verwendung der Daten für die Vorhersage sind ein paar Vorverarbeitungsschritte nötig.
Die Daten stehen in einem Intervall von fünf Minuten zur Verfügung, was insgesamt zu einer sehr großen Datenmenge führen würde, z.B. für Deutschland $12 * 24 * 457 = 131 616$ Datenpunkte.
Außerdem wird ein stündliches Intervall für die Vorhersage als ausreichend angesehen.
Die \ac{MOER}-Werte werden von der Einheit lbs/\ac{kWh} in g/MWh umgerechnet und die Informationen über die Zeitzone werden entfernt, da sich die Daten bereits in \ac{UTC} befinden.
Diese Zeitzone wird als Standard für alle Daten verwendet, sodass später ein einfaches Zusammenfügen aus einzelnen Datenquellen und Vergleichen der Daten untereinander ohne Konvertierung möglich ist.
\lstinputlisting[language=Python, caption=Python-Code zur Abfrage der MOER-Werte, label=CODE:watttime_get_moer]{\codedir/wattime_get_moer.m}
Die \ac{MOER}-Werte der beiden Länder werden separat in eine CSV-Datei eingelesen.

Die Daten zur \ac{MOER} sollen zunächst für die beiden Länder analysiert werden, um ein Bild über die Beschaffenheit der Daten zu erhalten.
Zwischen den beiden Ländern ist ein deutlicher Unterschied für die MOER-Werte zu erkennen.
In Abbildung~\ref{FIG:moer_distribution} wird ersichtlich, dass Norwegen weitaus niedrigere Werte hat als Deutschland.
\begin{figure}
 \caption{Die Verteilung der MOER-Werte von Deutschland und Norwegen (eigene Darstellung)}
 {\includegraphics[width=0.6\textwidth]{\figdir/moer_distribution}}
 \label{FIG:moer_distribution}
\end{figure}
Der Mittelwert für Deutschland ist mit rund 762g/kWh mehr als dreimal so hoch wie der Mittelwert Norwegens (rund 236g/kWh).
Bei Betrachtung der Häufigkeit der Werte muss berücksichtigt werden, dass für die beiden Länder nicht die gleiche Anzahl an Werten vorliegt.
Für Deutschland liegen insgesamt 10986 Werte vor und für Norwegen 26280.

Eine Zerlegung in Trend, Saisonalität und Residual, wie in Kapitel~\ref{CAP:intor-time-series-forecasting} beschrieben, soll mehr Aufschluss über die Daten geben.
Die Zerlegung für die Werte von Deutschland ist in Abbildung~\ref{FIG:moer_decomposition_DE} zu sehen.
\begin{figure}
 \caption{Die Dokomposition der MOER-Werte von Deutschland (eigene Darstellung)}
 {\includegraphics[width=0.99\textwidth]{\figdir/moer_decomposition_DE}}
 \label{FIG:moer_decomposition_DE}
\end{figure}
Bei der Trend-Komponente ist zu erkennen, dass die Werte zwar teils starke, teils weniger starke Schwankungen haben, jedoch über die Zeit kein klarer Aufwärts- oder Abwärtstrend erkennbar ist.
Für die Saisonalität wird lediglich ein Ausschnitt der Daten (Oktober 2022 bis Dezember 2022) abgebildet, weil sonst die Menge der Daten zu groß wäre, um die Saisonalität zu erkennen.
Die Daten weisen deutlich eine tägliche Saisonalität auf, weil sich je Tag immer der gleiche Zyklus wiederholt.
Zuerst fällt der Wert stark ab und steigt anschließend wieder stark an.
Die Residuals sind die durch den Trend und die Saisonalität noch nicht abgedeckte Komponente, die vorhandene Schwankungen in den Daten abbildet, doch insgesamt zeigt, dass die Daten stationär sind.

Für Vorhersagen der \ac{MOER} soll die Zusammensetzung der Stromproduktion betrachtet werden, um wichtige Faktoren ausfindig machen zu können.
Die Quellen der Stromproduktion in Deutschland und Norwegen im Jahr 2023 sind in Tabelle~\ref{TAB:energyproduction_de_no} aufgelistet.
\begin{table}[t]
 \centering\small
 \caption{Der Anteil der Energiequellen an der Stromproduktion in Deutschland und Norwegen im Jahr 2023 (\cite{ElectricityMaps.20240305T20:54:29.000Z})}
 \label{TAB:energyproduction_de_no}
 \input{\tabledir/energyproduction_de_no.tex}
\end{table}

Die Verfügbarkeit erneuerbarer Energien hängt maßgeblich vom Wetter ab.
Dieser Zusammenhang kann für die Vorhersage genutzt werden, weshalb Wetterdaten zum Datensatz hinzugefügt werden.
Dafür wird auf den Copernicus Climate Change Service~\cite{Copernicus.20231212T14:09:40.000Z} zurückgegriffen, der umfangreiche Datensätze rund um das weltweite Klima und den Klimawandel zur Verfügung stellt.
Konkret wurden drei mögliche Einflussfaktoren für die Kohlenstoffintensität als wichtig identifiziert und ausgewählt: die Lufttemperatur in 2 m Höhe, die Windgeschwindigkeit in 100 m Höhe und die globale horizontale Bestrahlungsstärke (im Englischen als \ac{GHI} bezeichnet).
Letztere wird durch die atmosphärische Zusammensetzung mit Aerosolen, Wasserdampf und Ozon, hauptsächliche jedoch von Wolken beeinflusst~\cite{KallioMyers.2020}.
Für die Stromerzeugung durch Photovoltaik ist in erster Linie die Sonneneinstrahlung, die auf das Solarmodul einfällt, entscheidend~\cite{James.}.
Die Erzeugung durch Windkraft hängt hauptsächlich von der Windgeschwindigkeit ab.
Für die Wasserkraft ist unter anderem der Niederschlag ausschlaggebend.
Der Verlauf der drei Kennwerte in Deutschland ist in den Abbildungen~\ref{FIG:temp_de_2015-2022},~\ref{FIG:ws_de_2015-2022} und~\ref{FIG:ghi_de_2015-2022} dargestellt, er wurde alle drei Stunden aufgezeichnet.
Die Lufttemperatur wurde in zwei Metern Höhe gemessen und die Windgeschwindigkeit in 100 Metern Höhe~\cite{Copernicus.20231212T14:09:40.000Z}.

Bei den Werten handelt es sich um Daten des ERA5 Reanalysis Datensatzes.
Dies sind von verschiedenen Quellen, wie Satelliten oder Wetterstationen, gemessene Daten, bei denen fehlende Werte mit durch physikalische Gesetze oder historische Verlaufsmuster generierte Daten ergänzt wurden.
Grundsätzlich sind die Daten also echt gemessene Daten, die zur Vollständigkeit mit generierten Daten ergänzt wurden~\cite{CopernicusClimateChangeService.2020}~\cite{CopernicusKnowledgeBase.20231009}.

Außerdem werden zeitliche Variablen hinzugefügt, die den MOER-Wert ebenfalls beeinflussen können: Wochentag, Jahreszeit und, ob es sich um einen Feiertag handelt.
Die jeweiligen Werte für die Variablen werden kodiert, sodass sie verarbeitet werden können.

In einer sogenannten Correlation Matrix soll analysiert werden, ob und wie die einzelnen Variablen zusammenhängen.
Für Deutschland und Norwegen wird je eine dieser Matrizen erstellt, zu sehen in Abbildung~\ref{FIG:correlation_matrix}.
\begin{figure}
 \caption{Der Zusammenhang zwischen den verwendeten Variablen für Deutschland und Norwegen dargestellt in je einer Correlation Matrix (eigene Darstellung)}
 {\includegraphics[width=0.99\textwidth]{\figdir/correlation_matrix}}
 \label{FIG:correlation_matrix}
\end{figure}
Insgesamt geht hervor, dass der lineare Zusammenhang zwischen den \ac{MOER}-Werten und den anderen Variablen eher gering ist.
Der größte Zusammenhang besteht mit der Windgeschwindigkeit (-0,31) und der Sonneneinstrahlung (-0,24) in Deutschland.
Das bedeutet, dass sich bei höheren Werten für diese beiden Variablen, niedrigere Werte für die \ac{MOER} verzeichnen lassen.
Vor dem Hintergrund der Energiequellen des deutschen Stromnetzes scheint die Interpretation sinnvoll, dass bei steigender Windstärke und Sonneneinstrahlung mehr Strom durch Solarstrom und Strom aus Windkraftwerken gewonnen werden kann und somit die Emissionsgröße sinkt.

Für die Implementierung soll eine Pipeline verwendet werden, um sich wiederholende Schritte problemlos und ohne nötigen Mehraufwand ausführen zu können.
Die Idee dabei ist, dass die Modell-übergreifenden Schritte, wie Datenvorverarbeitung und Feature-Engineering, für die beiden Modelle gleichermaßen verwendet werden, wohingegen Modell-spezifische Schritte separat umgesetzt werden.
Der Vorteil der Pipeline-Struktur besteht darin, dass

Der zuvor beschriebene Datensatz wird aufgeteilt in einen Trainingsdatensatz, einen Validierungsdatensatz und einen Testdatensatz, wie in Abbildung~\ref{FIG:moer_train_validation_test} sichtbar wird.
Als Unterteilung wird das Datum 30.09.2023, bzw. 30.11.2023 verwendet.
So wird sichergestellt, dass die Saisonalität vom Modell erkannt werden kann, denn es bekommt im Training genau ein Jahr an Daten für Deutschland zu sehen.
\begin{figure}
 \caption{Die Unterteilung der MOER-Werte in Trainings-, Validierungs- und Testdatensatz}
 {\includegraphics[width=0.99\textwidth]{\figdir/moer_train_validation_test}}
 \label{FIG:moer_train_validation_test}
\end{figure}

\section{Modellentwicklung und Training mit SARIMAX}
Für die Verwendung von auf ARIMA basierenden Modellen ist es zu aller Erst nötig, die Daten auf Stationarität zu untersuchen, weil diese eine Voraussetzung für die Modelle ist.
Stationär ist eine Zeitreihe dann, wenn ihr Mittelwert, ihre Varianz und ihre Autokorrelation konstant und zeitunabhängig sind.
Mit Hilfe eines \ac{ADF} Tests werden die Daten auf eine Einheitswurzel getestet (Nullhypothese).
Gibt es keine Einheitswurzel in der Zeitreihe (Alternativhypothese), so ist sie stationär.
Dafür wird die Implementierung von statsmodels verwendet, bei der die Daten lediglich in die \lstinline[columns=fixed]{adfuller} Funktion eingesetzt werden müssen.
Der Test liefert die ADF-Statistik, eine negative Zahl, die je kleiner sie ist desto stärker die Nullhypothese ablehnt und den p-Wert, der der Nullhypothese widerspricht, wenn er kleiner als 0,05 ist.
Code-Ausschnitt~\ref{CODE:adf-test_moer-de} zeigt den \ac{ADF} Test für die \ac{MOER}-Zeitreihe für Deutschland.
\lstinputlisting[language=Python, caption=Python-Code zur Überprüfung der MOER-Werte für Deutschland auf Stationarität mithilfe des ADF-Tests, label=CODE:adf-test_moer-de]{\codedir/adf-test_moer-de.m}
Das Ergebnis zeigt, dass die Nullhypothese aufgrund des p-Wertes abgelehnt wird und die Daten somit stationär sind.
Die \ac{MOER}-Werte für Norwegen sind ebenfalls stationär, jedoch ist die Ablehnung der Nullhypothese mit einer ADF-Statisik von rund -4 weniger stark.
Wären die Daten nicht stationär, müssten sie solange differenziert werden, bis sie Stationarität aufweisen~\cite{Peixeiro.2022}~\cite{Rahmadhan.8.5.2023}.

Zur Anwendung des SARIMAX-Modells wird die Implementierung von \glqq statsmodels\grqq{} verwendet.
Ein SARIMAX-Modell muss, anders als beim \ac{TFT} mit der Möglichkeit separater Gruppen-Ids, für jedes der beiden Länder separat erstellt werden
Definiert wird das Modell durch die Angabe der Zielvariable, der exogenen Variablen, der Parameter für den nicht-saisonalen Teil des Modells (\lstinline[columns=fixed]{p,d,q}) und diejenigen für den saisonalen Teil des Modells (\lstinline[columns=fixed]{P,D,Q,s}).

Als nächstes gilt es, die Parameter für SARIMAX festzulegen.
Diese können durch Analyse der \ac{ACF} und der \ac{PACF} gewählt werden.
Eine Alternativmethode ist, verschiedene Werte für die Parameter automatisch überprüfen zu lassen.
Dafür kann die \lstinline[columns=fixed]{auto_arima} Funktion der pmdarima Bibliothek verwendet werden.
Sie hilft, die optimalen Parameterwerte zu finden, indem sie die Kombination ausfindig macht, bei der das \ac{AIC} am niedrigsten ist~\cite{Rahmadhan.8.5.2023}.
Das \ac{AIC} bewertet die Qualität eines Modells im Vergleich zu anderen Modellen durch Messen der durch das Modell verlorenen Informationen.
Ein niedrigerer \ac{AIC}-Wert bedeutet demnach, dass weniger Informationen verloren gingen und das Modell deshalb besser abschneidet.
Für die Funktion kann unter anderem die Testmethode angegeben werden, hier wird der \ac{ADF} Test verwendet.
Außerdem werden für \lstinline[columns=fixed]{p} und \lstinline[columns=fixed]{q} Start- und Maximalwerte definiert.
Als Startwert wird für beide Parameter 0 gewählt und als Maximalwert 24, um dem Modell zu ermöglichen, 24 Zeitschritte zurück zu gehen und so die Saisonalität von einem Tag abzubilden~\cite{Peixeiro.2022}.
Die Frequenz wird in dieser Implementierung als Parameter m angegeben, entspricht aber dem sonst verwendet s und wird der Saisonalität wegen ebenfalls auf 24 gesetzt.
Der Test ergibt, dass das Modell am besten als \lstinline[columns=fixed]{ARIMA(2,0,0)(0,0,0)[12] intercept} funktioniert.
Zu beachten ist, dass bei Setzen von \lstinline[columns=fixed]{P, D} und \lstinline[columns=fixed]{Q} auf 0 das Modell keine Saisonalität berücksichtigt und somit einem ARIMA-Modell entspricht.

Basierend auf diesem Ergebnis, wird das Modell mit diesen Parametern erstellt und trainiert, zu sehen in Code-Ausschnitt~\ref{CODE:sarimax_model-train}
\lstinputlisting[language=Python, caption=Python-Code zum Erstellen und Trainieren des SARIMAX-Modells, label=CODE:sarimax_model-train]{\codedir/sarimax_model-train.m}

Nach automatischer Wahl der Parameter soll nun trotzdem noch die \ac{ACF} und \ac{PACF} der Zeitreihe analysiert werden.
Korrelation ist ein Maß, um die lineare Beziehung zweier Variablen zu messen.
Die \ac{ACF} misst für eine Zeitreihe diese Beziehung zwischen zwei um eine bestimmte Anzahl an Zeitschritten versetzten Werten.
Sie misst dadurch die Korrelation der Zeitreihe mit sich selbst~\cite{Peixeiro.2022}.
Das \ac{ACF}-Diagramm für die \ac{MOER}-Werte für Deutschland ist in Abbildung~\ref{FIG:acf_moer_de} zu sehen.
\begin{figure}
 \caption{Die ACF der MOER-Werte für Deutschland}
 {\includegraphics[width=0.5\textwidth]{\figdir/acf_moer_de}}
 \label{FIG:acf_moer_de}
\end{figure}
Die x-Achse des Diagramms repräsentiert die Anzahl der zeitlichen Verzögerungen, während die y-Achse das Ausmaß der Korrelation zwischen den Zeitpunkten darstellt.
Zu sehen ist eine Abnahme der Korrelation bei den ersten Zeitversetzungen, was auf eine starke Korrelation der zeitlich nahe beieinander liegenden Werte hinweist.
Das bedeutet, dass die Messwerte aus den vergangenen Stunden einen signifikanten Einfluss auf die der folgenden Stunden ausüben.
Die \ac{ACF} zeigt außerdem, dass die Daten trotz ihrer Stationarität saisonale Muster aufweisen.
Nach 24 Zeitschritten, also einem Tag, ist eine wiederkehrende Spitze in den Daten erkennbar und das schwingende Muster scheint sich mit einem täglichen Ryhtmus zu wiederholen.
Dies weist auf einen täglichen Zyklus der Zeitreihe hin.
Gegen Ende der aufgezeigten Zeitverzögerungen nähern sich die Korrelationswerte der Nulllinie an und liegen innerhalb des Konfidenzintervalls (als hellblauer Bereich gekennzeichnet), was bestätigt, dass die Daten stationär sind.
Statistisch signifikante Werte außerhalb des Konfidenzintervalls deuten darauf hin, dass die Daten mithilfe vergangener Werte vorhersagbar sind.

\section{Modellentwicklung und Training mit Temporal Fusion Transformer}
Für die Implementierung des \ac{TFT} wird das Modell von \glqq pytorch forecasting\grqq{}, einem Package für Zeitreihenvorhersage mit neuronalen Netzen, verwendet~\cite{PytorchForecastingDocumentation.20230410T20:05:46.000Z}.
Zunächst muss dem Zeitreihen-Datensatz ein Zeitindex hinzugefügt werden, der sich bei jedem Zeitschritt (hier eine Stunde) um eins erhöht.
Die Daten werden in einen Trainings-, einen Validierungs- und einen Testdatensatz eingeteilt und je ein dataloader dafür erstellt.
Code-Ausschnitt~\ref{CODE:tft_training-dataset} zeigt das Vorgehen.
\lstinputlisting[language=Python, caption=Python-Code zur Erstellung des Trainingsdatensatzes für den TFT, label=CODE:tft_training-dataset]{\codedir/tft_training-dataset.m}
Dem Trainingsdatensatz werden Parameter mitgegeben, um den Aufbau und die Struktur der Eingabedaten für das Modell zu definieren.
Wichtig ist vor allem das richtige Zuordnen der Variablen zu den verschiedenen Variablenarten~\cite{GitHub}.
Im betrachteten Anwendungsfall der Vorhersage von \ac{MOER}-Werten, werden diese als Zielvariable gesetzt.
Das Land wird als Gruppen-Id gesetzt, um zu definieren, dass ausgehend vom Land zwei verschiedene Zeitreihen existieren.
Zwei für die Vorhersage wichtige Angaben sind die Länge der verwendeten Vergangenheitswerte (\lstinline[columns=fixed]{max_encoder_length}) und wie weit in die Zukunf vorhergesagt wird (\lstinline[columns=fixed]{max_predicition_length}).
Da die vorherige Analyse der Daten eine deutliche tägliche Saisonalität in den Daten ersichtlich machte, wird für beide größen die Länge eines Tages (24 Stunden) gesetzt.
Das Land wird außerdem auch als statische, kategorische Variable \lstinline[columns=fixed]{static_categoricals} angegeben, da sich die Werte im Lauf der Zeit nicht verändern.
Kategorische Werte, die sich über die Zeit ändern, aber im Voraus bekannt sind (Jahreszeit, Wochentag und, ob es sich um einen Feiertag handelt), werden als \lstinline[columns=fixed]{time_varying_known_categoricals} gesetzt.
Sich über die Zeit ändernde, im Voraus bekannte, aber kontinuierliche Werte, sind die Wetterdaten (Temperatur, Windgeschwindigkeit, \ac{GHI} und Niederschlag).
Sie werden als \lstinline[columns=fixed]{time_varying_known_reals} gesetzt, nachdem sie durch Normalisierung mithilfe des \lstinline[columns=fixed]{StandardScaler} vom Package \glqq sktlearn\grqq auf eine ähnliche Werteskala gebracht wurden.
Gibt es für diese Art von Werten Variablen, die im Voraus nicht bekannt sind, werden sie als \lstinline[columns=fixed]{time_varying_unknown_reals} gesetzt.
Welche Variable zu dieser Kategorie auf jeden Fall dazu gehört ist die die Zielvariable.
Die statische Kovariate ist das Land, welches gleichzeitig auch
Zu diesem Zweck wird außerdem eine Normalisierung anhand der Gruppen durch die Softplus-Funktion durchgeführt, um positive Ausgabewerte zu garantieren.
Durch die Angabe sogenannter \lstinline[columns=fixed]{lags} wird die Eingabe um vergangene Werte der Zielvariablen, verschoben um den angegebenen Zeitraum, erweitert.
Ausgehend von der Saisonalität der Daten werden hier die Werte verschoben um einen Tag verwendet.
Zuletzt werden dem Modell zusätzliche Merkmale und Kontext hinzugefügt, wie ein relativer Zeitindex um das Verhältnis jeden Datenpunkts zum Vorhersagepunkt zu erfassen, das Ausmaß der Zielvariable in Form von Mittelwert und Standardabweichung, und die Größe des Vorhersagefensters.
Außerdem wird festgelegt, dass fehlende Zeitschritte geduldet werden.
Die Datensätze zur Validierung und zum Testen werden dann ausgehend vom Trainingsdatensatz erstellt, wobei für diese angegeben wird, dass eine Vorhersage ausgeführt werden soll (\lstinline[columns=fixed]{predict=True})~\cite{Labiadh.2023}~\cite{GitHub}.

Zunächst soll der \ac{MAE} des Baseline-Modells errechnet werden, um eine Ausgangslage zu haben.

Für die optimierte Einstellung der Parameter des Modells wird das Hyperparamter-Training von pytorch forecasting getestet.
Dafür werden die Trainings- und Validierungsdaten benötigt und Werteräume für die zu analysierenden Parameter definiert.
Die am besten funktionierenden Parameter sind laut erstellter Studie folgende:
\lstinline[columns=fixed]{gradient_clip_val: 0.7704201821245052, hidden_size: 101, dropout: 0.16435100042063222, hidden_continuous_size: 14, attention_head_size: 2, learning_rate: 0.00560374967}.


\section{Evaluierung und Vergleich der Modelle}
%!
%!
\chapter{Anwendung der Prognose}
\section{API zur Abfrage der Prognose}
Die Prognose soll dazu dienen, für einen fiktiven Workload die Startzeit auszuwählen, bei der die Summe der \ac{MOER}-Werte über die Dauer des Workloads am geringsten sind.
Dazu sollen die prognostizierten Werte über eine \ac{API} abgefragt werden können.
Um dies mit so wenig Overhead wie möglich zu machen, wird Flask, ein Mikro-Frontend zur einfachen Erstellung von Webanwendungen und ~\ac{API}s mit Python, verwendet~\cite{.20240203T21:13:11.000Z}.
Zur Konfiguration der Abfrage sollen folgende Parameter angegeben werden: Welches Prognose-Modell verwendet werden soll (SARIMAX oder \ac{TFT}, default ist \ac{TFT}), die geschätzte Dauer des Workloads in Minuten, ein Fälligkeitsdatum und für welche Länder die \ac{MOER}-Wert abgefragt werden sollen.
Die Erstellung des Prognose-Endpunkt mit Parametern ist in Code-Ausschnitt~\ref{CODE:prediction_endpoint} zu sehen.
\lstinputlisting[language=Python, caption=Python-Code zur Erstellung der Prognose API mit Flask, label=CODE:prediction_endpoint]{\codedir/flask_prediction_endpoint.m}

\section{Grenzen}
%Für welche Architektur geeignet
Es gilt zu beachten, dass die analysierten Optimierungen ihre Grenzen haben.
Es handelt sich nicht um allgemein gültige, immer anwendbare Lösungen, sondern vielmehr um Vorschläge oder Richtlinien, die unter bestimmten Umständen durchaus positive Auswirkungen haben können.
Eine wichtige Einschränkung kann ein mögliches Fälligkeitsdatum oder eine hohe Dringlichkeit sein~\cite{Dodge.06212022}.
Ist bekannt, dass eine Aufgabe zu einem bestimmten Zeitpunkt angefangen oder sogar bereits abgeschlossen sein muss, hat es keinen Nutzen, wenn der nächste kohlenstoffarme Zeitraum nach diesem Fälligkeitsdatum liegt.
Allgemein können die zeitlichen Verschiebungen zu Verspätungen führen und diese können wiederum einen Anstieg von Emissionen durch andere Teile des Projekts verursachen.
Es ist deshalb stets wichtig, die Vorteile der eingesetzten Maßnahmen gegenüber ihrem Aufwand abzuwägen~\cite{Dodge.06212022}.

%!
%!
\chapter{Ergebnisse und Diskussion}
Eine große Herausforderung besteht darin, das Thema Nachhaltigkeit in der \ac{IT} anzubringen.
Lange wurde diesem keine Aufmerksamkeit geschenkt und Optimierungen fanden hauptsächlich für anfallende Kosten statt.
Wenn der Umweltaspekt nicht Motivation genug ist, können noch zwei weitere Aspekte hilfreich sein.
Zum einen gehen mit der Optimierung des Energieverbrauchs und der Umstellung auf grüne Energiequellen oft verminderte Stromkosten einher, was als Anreiz dienen kann.
Zum anderen werden immer mehr Vorschriften und Berichtsanforderungen in Bezug auf verursachte Emissionen eingeführt~\cite{GreenSoftwareFoundation.2023}.
Dies sind weitere Gründe, sich mit der Energienutzung zu beschäftigen.

Der primäre Antrieb für den Übergang zu erneuerbaren Energien mag ökonomischer, nicht ökologischer Natur sein
%!
%!
\chapter{Fazit und Ausblick}

%Geplante Skalierung
%Dynamische Skalierung
%Dynamische Ressourcenanpassung
%Optimierung des Datenzugriffs


