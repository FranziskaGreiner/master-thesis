%! EINLEITUNG
%!
\chapter{Einleitung}

\section{Motivation}
Der Klimawandel stellt eine der größten Herausforderungen unserer Zeit dar.
Seine Realität und die dringende Notwendigkeit, seine Hauptverursacher und Treibfaktoren zu bremsen, sind längst unbestreitbar.
Dies wird unter anderem durch den im März 2023 veröffentlichten Bericht des \ac{IPCC} deutlich bestätigt.
Der Bericht zeigt auf, dass die globale Oberflächentemperatur im Zeitraum von 2011 bis 2020 um 1,09 Grad Celsius höher lag als im Zeitraum von 1850 bis 1900 und identifiziert die nicht nachhaltige Energienutzung als einen der Haupttreiber dieser Veränderungen~\cite{IPCC.2023}.
Auch die neuesten Zahlen des EU-Erdbeobachtungsdienstes Copernicus bekräftigen den Trend.
Dieser gab bekannt, dass das Jahr 2023 das wärmste Jahr seit Beginn der globalen Temperaturaufzeichnungen im Jahr 1850 war~\cite{CopernicusClimateChangeService.09.01.2024}.
Die neuesten Veröffentlichungen des Beobachtungsdienstes von Anfang Februar diesen Jahres beschreiben außerdem, dass die 1,5-Grad-Marke des Pariser Abkommens in den vergangenen zwölf Monaten erstmals gebrochen wurde.
Die globale Durchschnittstemperatur der Monate Februar 2023 bis Januar 2024 liegt 1,52 Grad über dem vorindustriellen Niveau~\cite{Eichhorn.8.2.2024}.
%!Gleichzeitig war es lange Zeit üblich, quasi unendlich Rechenkapazität etc. zur Verfügung zu haben und auch zu nutzen

%In diesem Zusammenhang stellt sich natürlich die Frage nach der Ursache für diese Entwicklung.
In diesem Kontext spielt die Technologiebranche eine zweischneidige Rolle.
Der rasante technologische Fortschritt hat zu einem exponentiellen Wachstum der Rechenkapazitäten geführt.
%Besonders im Bereich der Softwareentwicklung ist ein stetiges Wachstum von Technologien und Anwendungen zu verzeichnen.
%!Kommentar Thomas: Übergang zu Technologiebranche zu groß. Technologie kann alles mögliche sein.
Einerseits geht dieses Wachstum mit einem erheblichen Anstieg des Energieverbrauchs einher.
Schätzungen zufolge trägt die Technologiebranche mit mehr als 10\% zu den jährlichen globalen Kohlenstoffemissionen bei~\cite{Buchanan.2023}.
Andererseits steckt in dieser Branche auch das Potenzial, erheblich zur Verringerung der Auswirkungen beizutragen.

Angesichts dieser Entwicklungen hat die Bedeutung nachhaltiger Technologien und Praktiken signifikant zugenommen.
Es ist von großer Wichtigkeit, dass die \ac{IT}-Branche ihre Verantwortung wahrnimmt und aktiv dazu beiträgt, den Klimawandel durch innovative und nachhaltige Lösungen zu bekämpfen.
Die Entwicklung und Implementierung von energieeffizienten und kohlenstoffarmen Technologien und Methoden in der Softwareentwicklung ist daher nicht nur eine technologische Herausforderung, sondern auch eine moralische Verpflichtung gegenüber zukünftigen Generationen.

Zwar werden die Stromnetze weltweit auf kohlenstoffärmere Quellen umgestellt, doch dieser Wandel kann nicht unmittelbar geschehen, sondern benötigt Zeit und ist nur möglich, wenn die Stromnachfrage trotz der Umstrukturierung nach wie vor gedeckt werden kann.
Um dieser Schwierigkeit Abhilfe zu schaffen, soll betrachtet werden, wie der Übergang beschleunigt werden kann und fossile Brennstoffe verzichtbarer gemacht werden können, indem die Nutzung erneuerbarer Energiequellen priorisiert wird~\cite{GreenSoftwareFoundation.2022}.
%effizienter Code oft sowohl grüner als auch schneller ist als ineffizienter Code
%aber: keine Selbstverständlichkeit, "grün", "effizient" und "performant" sind keine Synonyme
%offensichtlichste Zweck der Code-Effizienz: Anzahl und Größe der Maschinen zu reduzieren, die für das Hosting Ihrer Dienste erforderlich sind --> Anzahl der Nutzer, Zuverlässigkeit und Leistungsniveau; = Maximierung der Hardwareproduktivität, sehr umweltfreundliches Konzept
%weniger Server --> weniger Strom für den Betrieb --> das System enthält weniger Kohlenstoff (jede Maschine enthält Kohlenstoff, der bei ihrer Herstellung und Entsorgung ausgestoßen wird)
%früher nicht viel Hardwarekapazität (Maschinen waren langsam, Anschaffung teuer, Unterbringung in Rechenzentren kostspielig) --> Verwendung von hocheffizienten Sprachen wie C, sodass ausführbare Dateien klein und minimale Anzahl der CPU-Zyklen pro Vorgang; Services haben nicht zu viele Nachrichten mit anderen Maschinen ausgetauscht, Daten auf der Festplatte wurden nicht ständig abgefragt
%Warum nicht einfach dahin zurückkehren?
%Maschinen und Netzwerke verdoppelten ihre Kapazität alle achtzehn Monate (Moore´s law), sind heute mindestens drei Größenordnungen schneller als in den 90er Jahren
%Dienste von Drittanbietern wurden innovativer und wertvoller
%Nutzerzahlen stiegen, erwarteten mehr Funktionen und schnellere Entwicklung
%Sicherheitsbedrohungen wurden immer beängstigender und häufiger
%Hauptziel Maschinenproduktivität hat sich in letzten zwei Jahrzenten hin zur Entwicklerproduktivität geändert
%!siehe "Nutzen Code-Effizienz" von Building Green Software

%Der Klimawandel ist eine der größten Herausforderungen der heutigen Zeit.
%Längst ist er nicht mehr abzustreiten und macht es notwendig, die Auslöser und Beschleuniger zu bremsen.
%Der vom Menschen gemachte Klimawandel ist nicht mehr abzustreiten.
%%Es wurde mehrfach bewiesen, dass der Klimawandel Realität ist.
%Er wurde mehrfach bewiesen, zuletzt zum Beispiel durch den im März 2023 veröffentlichten Bericht des IPCC (Intergovernmental Panel on Climate Change).
%Dieser hält fest, dass die globale Oberflächentemperatur 2011 bis 2020 um 1,09 Grad Celsius wärmer war, als in den Jahren 1850 bis 1900 und nennt als einen wichtigen Auslöser eine nicht nachhaltige Energienutzung~\cite{IPCC.2023}.
%
%Die Technologiebranche spielt eine wichtige Rolle beim Klimawandel.
%Der technologische Fortschritt hat einen scheinbar unendlichen Zugang zu Rechenleistung verschafft.
%Im Bereich der Softwareentwicklung wird seit längerer Zeit eine rasche Entwicklung von Technologien verzeichnet.
%Damit einher kommt ein steigender Energieverbrauch~\cite{Buchanan.2023}.
%Schätzungsweise beläuft sich der Anteil der Technologiebranche an den jährlichen Kohlenstoffemissionen auf mehr als 10\%.
%Angesichts dessen hat der Entwurf von nachhaltigen Technologien und Praktiken an Wichtigkeit gewonnen.
%Zustande kommen diese Emissionen hauptsächlich durch zwei Faktoren:
%Zum einen durch die Herstellung von Benutzergeräten für Anwendungen, sogenannter verkörperter (englisch embodied) Kohlenstoff, und zum anderen durch den Betrieb der Anwendungen in Rechenzentren~\cite{Currie.2024}.

%Das rasche Wachstum von Softwaretechnologien und die steigende Popularität von Cloud-Diensten haben einen scheinbar unendlichen Zugang zu Rechenleistung verschafft.
%Damit einher kommt ein steigender Energieverbrauch

\section{Forschungsziele und Fragestellungen}
Das primäre Ziel dieser Arbeit liegt darin, die Nachhaltigkeit von Softwareanwendungen durch die Entwicklung eines Prognosemodells zur Vorhersage der Kohlenstoffintensität zu verbessern.
Insbesondere soll untersucht werden, wie die genauen Vorhersagen genutzt werden können, um den Aufbau der Softwarearchitektur und den Betrieb der Anwendung möglichst nachhaltig zu gestalten.
Die Arbeit wird durch folgende spezifische Forschungsfragen definiert:

\begin{enumerate}
 \item Entwicklung eines Vorhersagemodells: Wie kann ein effektives Modell zur Vorhersage der Kohlenstoffintensität des Stromnetzes entwickelt werden? Ist ein statistisches Modell dafür ausreichend oder erzielt ein auf \ac{KI} basierendes Modell bessere Ergebnisse? Welche Rolle spielen dabei verschiedene Eingabedaten, wie zum Beispiel bestimmte Klimadaten?
 \item Nachhaltigkeitsoptimierung von Softwareanwendungen: Welche Strategien zur Nachhaltigkeitsoptimierung von Software gibt es? Inwiefern können die Vorhersagen der Kohlenstoffintensität dazu verwendet werden, um die Architektur und den Betrieb von Software nachhaltiger zu gestalten?
 \item Simulation und Bewertung: Wie kann die Wirksamkeit der entwickelten Strategien durch Simulationen getestet und bewertet werden?
 \item Beitrag zur nachhaltigen Softwareentwicklung: Wie trägt diese Arbeit zur Förderung der Nachhaltigkeit in der Softwareentwicklung bei? Welche praktischen Erkenntnisse und Empfehlungen können abgeleitet werden? Stehen die gewonnenen Verbesserungen der Nachhaltigkeit im Verhältnis zum Aufwand der Implementierung?
\end{enumerate}
%Das Ziel dieser Arbeit ist die Entwicklung eines KI-basierten Prognosemodells, das mithilfe von Zeitreihenanalysen die Kohlenstoffintensität des Stromnetzes von Ländern in Europa vorhersagt.
%Durch die Integration dieses Modells in eine \ac{API} soll eine Schnittstelle geschaffen werden, die es Softwareanwendungen ermöglicht, ihre Betriebszeiten an die Kohlenstoffintensität des Stromnetzes anzupassen.
%Die zentralen Forschungsfragen umfassen:
%
%Wie kann ein KI-basiertes Prognosemodell effektiv die Kohlenstoffintensität des Stromnetzes vorhersagen?
%Wie können die Vorhersagen genutzt werden, um nachhaltige Scheduling-Strategien in Softwarearchitekturen zu implementieren?
%Ist die dadurch gewonnene Verbesserung der Nachhaltigkeit von Softwareanwendungen lohnenswert?
\section{Struktur der Arbeit}
\section{Themenabgrenzung}
Im Folgenden werden gezielte Abgrenzungen vorgenommen, um den Umfang und Fokus der Arbeit zu definieren und gleichzeitig relevante, jedoch nicht zentrale Themen auszugrenzen.
Diese Abgrenzungen schaffen einen klaren und strukturierten Rahmen für die Forschung und legen den Grundstein für mögliche zukünftige Arbeiten.

Nachhaltigkeit ist ein vielschichtiges Konzept, jedoch liegt der Schwerpunkt dieser Arbeit auf der ökologischen Dimension, insbesondere auf der Reduktion der Kohlenstoffintensität.
Soziale und ökonomische Nachhaltigkeitsaspekte werden nur am Rande behandelt.

Neben den Einflüssen der Software auf die Umwelt ist die Hardware ein wichtiger Faktor.
Bei ihrer Herstellung und Entsorgung freigesetzte, sogenannte verkörperte, Emissionen können einen großen Anteil ausmachen.
%!Kommentar Thomas: Software hat auch Einfluss auf verkörperte Emissionen, weil effizienter die Software ist und je weniger Hardware sie benötigt, desto weniger verkörptere Emissionen, weil Hardware wird besser genutzt.
Dieser Aspekt und daraus resultierenden Maßnahmen sind zwar signifikant, werden jedoch in dieser Arbeit nicht direkt behandelt, sondern bieten einen Ansatzpunkt für weiterführende Forschung.
Indirekt werden jedoch auch die verkörperten Emissionen thematisiert, denn Maßnahmen wie der effiziente Betrieb von Software gehen beispielsweise oft mit einer Reduzierung der verkörperten Emissionen einher.
% Indirekt sind die jedoch auch die verkörperten Emissionen betroffen, denn je effizienter die Software gestaltet ist, desto weniger Hardware benötigt sie zur Ausführung und desto weniger verkörperte Emissionen sind damit verbunden.

Die Software betreffend werden mögliche Bereiche, die hinsichtlich Nachhaltigkeit betrachtet werden können, eingeschränkt.
Nicht Teil der Arbeit sind zum Beispiel Maßnahmen auf Code-Ebene, wie die Verwendung nachhaltiger Programmiersprachen und Frameworks, effiziente Datenverwaltung und -speicherung oder nachhaltig gestaltete Benutzeroberflächen.
Der Fokus liegt allein auf Nachhaltigkeits-Verbesserungen durch die Nutzung kohlestoffarmen Stroms.
%Wichtig ist auch die Betrachtung von Hardwareeinflüssen, wie den bei Herstellung und Entsorgung freigesetzten verkörperten Kohlenstoffemissionen.
%Diese sind zwar signifikant, werden jedoch in dieser Arbeit nicht behandelt, bieten jedoch interessante Ansatzpunkte für weiterführende Forschung.

%Obwohl Nachhaltigkeit ein vielschichtiges Konzept ist, fokussiert sich diese Arbeit primär auf die ökologische Dimension der Nachhaltigkeit, insbesondere auf die Reduktion der Kohlenstoffintensität.
%Soziale und ökonomische Aspekte der Nachhaltigkeit werden nur am Rande behandelt.

Des Weiteren ist sowohl der technologische, als auch der geografische Rahmen des Vorhersagemodells begrenzt.
Die Umsetzung erfolgt durch ein \ac{SARIMAX}- und ein \ac{TFT}-Modell.
Weitere möglicherweise passende Ansätze zur Umsetzung werden nicht untersucht.
Die Analyse beschränkt sich auf die Stromnetze Deutschlands und Norwegens.
Diese beiden Länder eignen sich gut für den Vergleich, wie in Kapitel~\ref{CAP:data-preparation-analysis} erklärt wird.
Außerdem würde ein größerer Umfang eine enorm größer Datenmenge bedeuten, was dem Rahmen dieser Arbeit nicht gerecht werden würde.
\section{Verwandte Arbeiten}
Nachfolgend soll der Stand der Forschung betrachtet und verwandte Arbeiten identifiziert werden, um die Relevanz und den Beitrag dieser Arbeit aufzuzeigen.
Zur Vorhersage der Kohlenstoffintensität gibt es einige Arbeiten mit unterschiedlilchem Umfang.
Electricity Maps und WattTime sind die wohl am häufigsten verwendeten und gängigsten Projekte.
Sie verfügen beide nicht nur über ein umfangreiches Datenportfolio und Übersichten, sondern machen auch Vorhersagen.

Electricity Maps bietet eine \ac{API} an, über die verschiedene Daten wie z.B.\ die Kohlenstoffintensität oder die Zusammensetzung des Strommixes abgefragt werden können.
Die Abdeckung umfasst über 160 Regionen weltweit und es sind historische, aktuelle und zukünftige (24 Stunden im Voraus) Werte verfügbar~\cite{ElectricityMaps.20231220T09:16:49.000Z}.

WattTime stellt ebenfalls eine \ac{API} zur Verfügung.
Über diese kann z.B.\ die marginale und die durchschnittliche Kohlenstoffintensität in der Vergangenheit, in Echtzeit oder 24 Stunden im Voraus abgefragt werden.
Die Daten sind in einem Intervall von fünf Minuten erhältlich~\cite{WattTime.20231130T19:28:06+00:00}.

Längerfristige Vorhersagen bietet nach jetzigem Stand lediglich die Carbon Intensity \ac{API}~\cite{LyndonRuff.20220420T15:34:17.000Z}.
Diese ~\ac{API} ist ein Projekt aus und für Großbritannien.
Es ist entstanden aus einem Zusammenschluss zwischen dem nationalen Stromnetz, mehreren Nichtregierungsorganisationen und akademischen Einrichtungen, um die Kohlenstoffintensität für verschiedene Regionen Großbritanniens vorherzusagen.
Die Schnittstelle basiert auf maschinellem Lernen und ermöglicht Anwendern, ihren Stromverbrauch so zu planen, dass die Kohlenstoffemissionen auf regionaler Ebene minimiert werden~\cite{Currie.2024}.
Die Vorhersagen sind 96 Stunden im Voraus verfügbar.
Dafür wurde der Berechnungsansatz, der im vorherigen Kapitel (Formel~\ref{eq:ci}) erklärt wurde, verwendet.
Der Intensitätsfaktor der verschiedenen Energiequellen ist dabei spezifisch für die jeweilige Region oder das jeweilige Land.

Es existieren auch Arbeiten zur Anwendung der Vorhersagen.
Ein Projekt, dass sich konkret mit der Anwendung der Vorhersagen zur Kohlenstoffintensität befasst, wurde von der Green Software Foundation ins Leben gerufen.
Es bietet eine erleichterte Integration von Vorhersagen in bestehende Anwendungen.
Es wurde eine \ac{SDK} entwickelt, mit Hilfe dessen herausgefunden werden kann, wann und wo ein Workload mit der geringsten Menge an Kohlenstoff ausgeführt werden kann.
Carbon Aware \ac{SDK} kann als Web\ac{API} und als Command Line Interface verwendet werden und steht als Open-Source-Projekt zur Verfügung.
Es kann dabei helfen, die Ausführung nicht zeitkritischer Workloads auf Zeiten mit geringeren Emissionen zu verschieben und in andere Rechenzentren zu verschieben~\cite{GreenSoftwareFoundation.20231212T09:58:27.000Z}.
Angeboten werden unterschiedliche Abfragen, zum Beispiel die Ausgabe der besten Startzeit und des besten Ortes unter Angabe eines Zeitintervalls und der Workload-Dauer oder die Ausgabe der aktuellsten Vorhersagedaten inklusive Berechnung des Zeitfensters mit der geringsten \ac{MOER}~\cite{carbon-aware-sdk}.

Ein weiteres relevantes Projekt ist der Carbon Aware Scheduler~\cite{siemers}.
Es handelt sich dabei um ein Command Line Tool für das Scheduling von Unix Jobs.
Basierend auf der Kohlenstoffintensität des Stromnetzes wird die beste Zeit für das Ausführen eines Jobs ausfindig gemacht.
Zum jetzigen Stand ist der Carbon Aware Scheduler jedoch nur für die Niederlande verfügbar.

Microsoft, UBS, WattTime und die Green Software Foundation haben zusammen ein Paper über kohlenstoffbewusste Software veröffentlicht~\cite{Buchanan.2023}.
Darin beschreiben sie, wie Kohlenstoffintensität gemessen und Software nach dieser ausgerichtet werden kann.
Sie verwenden die Carbon Aware \ac{SDK}, um einen Prototypen zur zeitlichen Verschiebung von Workloads zu entwerfen.
Das Projekt zeigt, wie das Tool im laufenden Betrieb eingesetzt werden kann.

Die örtliche Verlagerung von Workloads kann mit einem Kostenmehraufwand verbunden sein, der nicht nur die Verlagerung an sich sondern möglicherweise auch den höheren Preis der ausgewählten Region umfasst.
Diese zusätzlichen Kosten können von der Umsetzung der Verlagerung abhalten, weshalb Microsoft für Azure ein Werkzeug zusätzlich zur Carbon Aware \ac{SDK} entwickelt hat das auf diesem aufbaut.
Die Carbon Economy \ac{SDK} errechnet für die von der Carbon Aware \ac{SDK} ausgegebene optimale Zeit und Region die Kosten, um einen Workload in diese andere Region zu verlagern.
Unter Verwendung einer normalisierten Gewichtung werden geringere Kohlenstoffemissionen gegenüber zusätzlichen Kosten priorisiert.
Anhand diesem Gewicht kann entschieden werden, wie viel einem die geringeren Emissionen Wert sind~\cite{Norlander}.

Als bedeutend wurde vor allem die Arbeit~\cite{Wiesner.2021} identifiziert.
Diese untersucht das Potenzial zeitlicher Verlagerung von Workloads in Rechenzentren, um emissionsarme Energie zu verwenden.
Die Autoren konzentrierten sich dabei insbesondere darauf, über welche Eigenschaften Workloads verfügen müssen, sodass sie gut für die Verschiebung geeignet sind und auf die Kohlenstoffintensität vom Stromnetz vier verschiedener Länder.
Sie kamen zu dem Ergebnis, dass das Verlagerungspotenzial in Ländern mit einem hohen Anteil von Solarenergie vor Sonnenaufgang und abends hoch ist und dass eine Verlagerung auf die Wochenenden zu Einsparungen von mehr als 20\% führen können.
Außerdem stellte es sich als besonders hilfreich heraus, wenn für Workloads die zeitlichen Beschränkungen gelockert und mögliche Unterbrechungen genutzt werden~\cite{while}.
%!
%!
\chapter{Grundlagen nachhaltiger Softwarearchitektur}
\section{Definition von Nachhaltigkeit in der Softwareentwicklung}
Das Ziel der vorliegenden Arbeit besteht darin, eine Möglichkeit zu finden, die Nachhaltigkeit von Softwarearchitekturen zu optimieren.
An dieser Stelle wird deshalb als Grundlage der Begriff der Nachhaltigkeit definiert und erklärt, welche Rolle Nachhaltigkeit in der Softwareentwicklung spielt.

Vom Bundesministerium für wirtschaftliche Zusammenarbeit und Entwicklung~\cite{BundesministeriumWirtschaftlicheZusammenarbeitundEntwicklung} wird Nachhaltigkeit wie folgt definiert:
\glqq Nachhaltigkeit oder nachhaltige Entwicklung bedeutet, die Bedürfnisse der Gegenwart so zu befriedigen, dass die Möglichkeiten zukünftiger Generationen nicht eingeschränkt werden.\grqq{}
Dabei erstrecke sich Nachhaltigkeit gleichermaßen auf drei verschiedene Dimensionen, nämlich wirtschaftlich effizient, sozial gerecht und ökologisch tragfähig.
Laut Duden ist Nachhaltigkeit in Bezug auf Ökologie das \glqq Prinzip, nach dem nicht mehr verbraucht werden darf, als jeweils nachwachsen, sich regenerieren, künftig wieder bereitgestellt werden kann\grqq{}~\cite{Dudenredaktion.27.04.2018}.

Die im Jahr 1992 auf der Konferenz für Umwelt und Entwicklung der Vereinten Nationen verabschiedeten Agenda 21 legte Nachhaltigkeit als übergreifendes Ziel der Politik fest.
Außerdem wurden in der Agenda 2030, die 2015 von der Weltgemeinschaft beschlossen wurde, 17 Nachhaltigkeitsziele definiert.
Davon sieht Ziel zwölf Nachhaltigkeit in Produktion und Konsum vor.
Die deutsche Ausarbeitung dieses Ziels besagt, dass dazu unter anderem der Verzicht auf fossile Energieträger, wie Kohle, Gas und Öl und stattdessen die Nutzung von erneuerbaren Energien gehört~\cite{Bundesregierunginformiert}.

Betrachtet man den Begriff vor dem Aspekt der Softwareentwicklung, so lassen sich die Prinzipien nachhaltiger Software wie folgt zusammenfassen~\cite{Calero.2015}:
\begin{itemize}
 \item Überwachung, Messung, Bewertung, sowie Optimierung des Ressourcen- und Energieverbrauchs während Herstellung und Nutzung der Software
 %direkten (während Herstellung und Nutzung) und indirekten () Verbrauchs natürlicher Ressourcen, der durch den Einsatz und die Nutzung entsteht, bereits im Entwicklungsprozess;
 %wobei sich direkt auf den Verbrauch während der Herstellung und Nutzung bezieht und indirekt auf die Verwendung des Softwareprodukts zusammen mit anderen Prozessen und langfristigen systemischen Auswirkungen
\item Möglichkeit zur kontinuierlichen Auswertung und Optimierung des Einsatzes und der Nutzungsfolgen der Software
\item Zyklische Bewertung und Minimierung des Verbrauchs von natürlichen Ressourcen und Energie während der Entwicklungs- und Produktionsprozesse
\end{itemize}
Nachhaltige Software zeichnet sich also dadurch aus, dass die negativen Auswirkungen auf die drei Bereiche der Nachhaltigkeit (Wirtschaft, Gesellschaft und Umwelt) während Entwicklung, Einsatz und Nutzung der Software gering gehalten werden oder sich sogar positiv auf diese auswirken.

Die \ac{IT} spielt für das Thema Nachhaltigkeit eine wichtige Rolle und das sowohl als Teil des Problems, als auch als Teil der Lösung.
Diese Arbeit spricht von den drei genannten Ausprägungen der Nachhaltigkeit hauptsächlich die ökologische Tragfähigkeit an, die auch als \glqq grüne\grqq{} oder \glqq umweltfreundliche\grqq{} Dimension bezeichnet wird.
Betreffend dieser Dimension kann die \ac{IT} zwei verschiedene Rollen annehmen, bezeichnet als Green in \ac{IT} und Green by \ac{IT}\@.
Tabelle~\ref{tab:GreenInByIT} grenzt die beiden Begriffe voneinander ab.
Beschrieben wird jeweils die Rolle der \ac{IT}, das Ziel und das Potenzial der beiden Bereiche, untermauert durch je ein Beispiel~\cite{Calero.2015}.

\begin{table}[t]
 \centering\small
 \caption{Green in IT vs. Green by IT}
 \label{tab:GreenInByIT}
 \input{\tabledir/GreenInByIT.tex}
\end{table}

Die Unterteilung ist sowohl für Software als auch für Hardware relevant.
Demnach bestehen vier Hauptkategorien:
Green in Software, Green in Hardware, Green by Software, Green by Hardware.
Diese Arbeit konzentriert sich nicht auf die Hardware-bezogenen Aspekte.
Sie thematisiert die Kohlenstoffintensität und damit die Emissionen, die Software verursacht und ist deshalb der Kategorie Green in Software zuzuordnen.
Betrachtet man die Prognose der Kohlenstoffintensität mithilfe von Künstlicher Intelligenz, so könnte man dabei die \ac{IT} auch als Hilfsmittel zur Verbesserung der Nachhaltigkeit ansehen und somit Green by Software zuordnen~\cite{Calero.2015}.

Von der Green Software Foundation werden die Stellschrauben für nachhaltige Software in drei Faktoren unterteilt, nämlich Energie-Effizienz, Hardware-Effizienz und Kohlenstoff-Bewusstsein.
\begin{figure}
 \caption{Die unterschiedlichen Dimensionen nachhaltiger Software laut~\cite{GreenSoftwareFoundation}}
 {\includegraphics[width=0.8\textwidth]{\figdir/types-of-green-software}}
 \label{FIG:types-green-software}
\end{figure}
Energieeffiziente Software zeichnet sich dadurch aus, dass sie so wenig Energie wie möglich verbraucht.
Hardware-Effizienz wird erreicht, wenn weniger Hardware für die gleiche Aufgabe benötigt wird.
Diese beiden Faktoren sind zwar wichtig, aber werden in der vorliegenden Arbeit nicht weiter berücksichtigt, denn der Fokus liegt auf dem bewussten Umgang mit Kohlenstoff.
Ein bewusster Umgang mit Kohlenstoff setzt voraus zu verstehen, dass die gleiche Menge an verbrauchter Energie nicht immer die gleiche Kohlenstoffintensität hat, da diese je nach Zeitpunkt und Ort des Verbrauchs variiert~\cite{GreenSoftwareFoundation.2022}.

\section{Kohlenstoffintensität als Maß für Nachhaltigkeit}
Im Folgenden gilt es, Kohlenstoffintensität zu definieren und die Frage zu klären, wie diese mit Nachhaltigkeit zusammenhängt.
Der Begriff Kohlenstoff wird häufig als Synonym für alle Treibhausgase und als Oberbegriff für die Auswirkungen aller Arten von Emissionen und Aktivitäten auf die globale Erwärmung verwendet~\cite{GreenSoftwareFoundation.2022}.
Die Kohlenstoffintensität wird in Gramm \ac{CO2} pro \ac{kWh} angegeben und sagt aus, wie viel Kohlendioxid pro \ac{kWh} Strom emittiert wird~\cite{LyndonRuff.20220420T15:34:17.000Z}.
Treibhausgase wie \ac{CO2} kommen zwar natürlich in der Erdatmosphäre vor, um Wärme zurückzuhalten.
Jedoch sind sie durch menschliche und industrielle Aktivitäten im Überfluss vorhanden, was einen globalen Temperaturanstieg und Klimakatastrophen zur Folge hat~\cite{Currie.2024}.

Kohlenstoffintensität ist also ein Maß der Nachhaltigkeit des erzeugten oder verbrauchten Stroms.
Je geringer der Wert ist, desto nachhaltiger ist der Strom.
Genauer betrachtet führt ein verringerter Anteil von Energiequellen mit hoher \ac{CO2}-Emission (z.B.\ Braun- oder Steinkohle) am erzeugten Strom zu einem geringeren Emissionsfaktor des Stromnetzes.
Eine weitere Größe, die den Emissionsfaktor beeinflusst, ist der Wirkungsgrad des Energieträgers.
Ein Anstieg des Wirkungsgrads von Energiequellen mit hoher \ac{CO2}-Emission führt letztlich ebenfalls zu einem geringeren Emissionsfaktor des Stromnetzes, weil dann weniger dieser Quellen benötigt werden, um die gleiche Menge an Strom zu erzeugen~\cite{Icha.2020}.

Um Strom zu erzeugen, muss Energie einer anderen Form in elektrische Energie umgewandelt werden.
In Kohlekraftwerken wird zum Beispiel Kohle in einem Kessel verbrannt um Dampf zu erzeugen, der chemische Energie in elektrische Energie umwandelt.
Als Nebenprodukt wird viel Kohlendioxid freigesetzt, was die Stromerzeugung durch fossile Brennstoffe zu einer kohlenstoffintensiven Variante macht~\cite{Currie.2024}.
Stellt man sich vor, dass ein Gerät seinen Strom direkt aus einem Windkraftwerk beziehen würde, hätte diese Energie eine Kohlenstoffintensität von 0 g \ac{CO2}/\ac{kWh}, weil bei der Energieerzeugung durch Wind kein Kohlenstoff freigesetzt wird~\cite{GreenSoftwareFoundation.2022}.
Das Stromnetz ist jedoch ein Mix verschiedener Energiequellen (deshalb auch als Strommix bezeichnet), die sich unterschiedlich auf die Kohlenstoffintensität auswirken.
Bei der Verwendung von Strom kann nicht zwischen verschiedenen Erzeugungsarten ausgewählt werden, weshalb nur die Intensität des Gesamtnetzes betrachtet werden kann.
Durch erneuerbare Energien, wie Wind-, Solar-, oder Wasserenergie, erzeugter Strom ist weniger kohlenstoffintensiv, jedoch variiert seine Verfügbarkeit je nach Zeit und Ort.
Die Zeit ist ein wichtiger Faktor, weil ein direkter Zusammenhang zwischen Zeit und Wetter besteht und vom Wetter wiederum die erneuerbaren Energien abhängig sind.
Außerdem verfügen verschiedene Länder und Regionen über eine unterschiedliche Zusammensetzung des Stromnetzes.
Um die \ac{CO2}-Bilanz des verwendeten Stroms so gering wie möglich zu halten, ist es deshalb wichtig zu wissen, wann und wo die Kohlenstoffintensität des Stromnetzes gering ist.
%So kann zum Beispiel, wenn es die Anforderungen erlauben, die Nutzung des Stroms auf kohlenstoffarme Zeiten verschoben werden.
Hinzu kommt, dass erneuerbare Energien in Betrieb und Wartung verhältnismäßig billig sind, was dazu führt, dass deren erzeugter Strom nicht nur umweltfreundlicher, sondern meist auch kostengünstiger ist~\cite{NationalGrid.20231106T13:28:05.000Z}.

Ein weiterer wichtiger Faktor ist das Zusammenspiel aus Energieangebot und -nachfrage.
Die Nachfrage kann stark schwanken, doch muss das Stromnetz trotzdem immer in der Lage sein, diese zu decken.
Optimal wäre es natürlich, das Angebot unverzüglich an die Nachfrage anpassen zu können, jedoch variiert die Flexibilität je nach Energiequelle stark.
Betrachtet man zum Beispiel Windenergie, liegt auf der Hand, dass diese über eine sehr geringe Flexibilität verfügt, weil die Windstärke nicht kontrolliert werden kann.
Mit Kohleenergie ist es hingegen weitaus einfacher, auf ein Angebotsdefizit zu reagieren.
Eine wichtige Größe ist dabei die marginale Kohlenstoffintensität~\cite{GreenSoftwareFoundation.2022}.
Übersteigt der Strombedarf plötzlich die vorhandene Energie(?), wird die benötigte Energie aus dem sogenannten Grenzkraftwerk bezogen.
%Steigt der Strombedarf plötzlich an,
Dieses zeichnet sich dadurch aus, dass es auf solche Änderungen schnell reagieren kann.
Um den Strompreis so gering wie möglich zu halten, werden die verfügbaren Energiequellen aufsteigend ihres Preises angeordnet~\cite{Corradi.20231207T10:48:51.000Z}.
Diese sogenannte Merit-Order (im Deutschen als Reihenfolge der Vorteilhaftigkeit bezeichnet) ist das Zusammenspiel aus Stromangebot, -preis und -bedarf.
Eine beispielhafte Merit-Order des deutschen Stromnetzes ist in Abbildung~\ref{FIG:merit-order} dargestellt.
\begin{figure}
 \caption{Die Merit-Order des deutschen Stromnetzes~\cite{Gro.5.10.2022}}
 {\includegraphics[width=0.99\textwidth]{\figdir/merit-order}}
 \label{FIG:merit-order}
\end{figure}
Die verschiedenen Energiequellen sind nach Preis aufsteigend angeordnet und durch den eingezeichneten Strombedarf wird ersichtlich, welche dieser Quellen je nach Bedarf herangezogen werden.
Eine ausreichende Stromversorgung wird durch Hinzuziehen einer geeigneten Menge an Kraftwerken garantiert, wobei die Quellen mit niedrigstem Preis die höchste Priorität haben.
Daraus lässt sich schließen, dass der Strom sowohl günstiger als auch grüner wird, je mehr erneuerbare Energien eingespeist werden~\cite{Gro.5.10.2022}.

Es bestehen verschiedene Ansätze für die Berechnung der Kohlenstoffintensität.
Allgemein stellt sich die Berechnung als anspruchsvoll heraus, vor allem weil jedes Land seine eigene Zusammenstellung verschiedener Kraftwerke hat~\cite{Currie.2024}.
Für eine Berechnungsweise (Formel~\ref{eq:ci}), die Forscher aus Großbritannien für die Carbon Intensity \ac{API} verwendet haben, wird der Anteil der verschiedenen Energiequellen, ein Intensitätsfaktor je Energiequelle und die Stromnachfrage benötigt.
\begin{equation}
 \label{eq:ci}
 C_t = \frac{\sum_{g=1}^{G} P_{g,t} \times c_g}{D_t}
\end{equation}
Die Kohlenstoffintensität Ct zur Zeit t ist das Produkt aus Energiequelle und Intensitätsfaktor summiert für alle verwendeten Energiequellen und anschließender Teilung durch die Nachfrage~\cite{LyndonRuff.20220420T15:34:17.000Z}.
Für \ac{CO2}-Emissionen durch erneuerbare Energien sowie durch Kernkraft wird der Intensitätsfaktor mit 0 berechnet, sodass für diese Energiequellen das Gesamtergebnis der Kohlenstoffintensität gleich 0 ist.

Ein weiterer Berechnungsansatz, der von der Green Software Foundation speziell für Software entwickelt wurde, ist der sogenannte \ac{SCI} Wert.
Er gibt die gesamten Kohlenstoffemissionen einer funktionalen Einheit R in g\ac{CO2}eq/\ac{kWh} an~\cite{GreenSoftwareFoundation.2022}.
Die entsprechende Formel ist Formel~\ref{eq:sci}.
\begin{equation}
 \label{eq:sci}
 SCI = ((E \times I) + M) pro R
\end{equation}
In die Berechnung fließen folgende Werte ein~\cite{Buchanan.2023}~\cite{GreenSoftwareFoundation.2022}:
\begin{itemize}
 \item die von der Software benötigte Energie E (Strom für virtuelle Maschine, Kühlung, etc.)
 \item die standortbezogene marginale Kohlenstoffemissionen I (emittierter Kohlenstoff pro \ac{kWh})
 \item der verkörperte Kohlenstoff M der zugrunde liegenden Hardware (emittierter Kohlenstoff während der Herstellung und der Entsorgung der Hardware)
 \item die Skaliereinheit R, die für die zu messende Software relevant ist (z.B. pro zusätzlichem Benutzer, Gerät, ~\ac{API}-Aufruf, etc.)
\end{itemize}
Anhand dieser Formel ergeben sich die Optimierungspunkte des \ac{CO2}-Fußabdrucks eines Workloads, nämlich Energie, Kohlenstoffintensität, verkörperter Kohlenstoff und Skaliereinheit.
%!
%!
\chapter{Strategien zur Nachhaltigkeitsoptimierung}\label{CAP:strategies}
Verwaltete Cloud-Dienste bieten aufgrund ihrer hohen Rechendichte und ihrer enormen Hardware- und Energienutzung ein großes Potenzial zur Effizienzverbesserung~\cite{Currie.2024}.
Die drei großen Cloud-Anbieter Google Cloud, Azure und AWS haben sich zur Kohlenstoffneutralität verpflichtet.
Dies bedeutet jedoch nicht, dass es allein ausreichend ist, eine Anwendung auf einer dieser Plattformen bereitzustellen und sich nicht weiter mit dem Thema Nachhaltigkeit auseinander setzten muss.
Auch die Anwendung selbst muss für Nachhaltigkeit optimiert sein und die vorhanden Möglichkeiten müssen genutzt werden~\cite{Newman.2023}.
Der deutsche Bitkom hat in einer Metastudie auf Grundlage von mehreren Studien aus den Jahren 2012 bis 2019 zwei Hauptkategorien für Energieensparungen definiert: Anwendungen und Infrastruktur.
In Abbildung~\ref{FIG:sustainability-infrastructure} werden die Facetten der Nachhaltigkeit auf Anwendungs- und Infrastrukturebene dargestellt.
Von Amazon~\cite{Mokhtari.2023} wird dieses Modell als Modell der geteilten Verantwortungen vorgestellt.
Der Cloud-Anbieter ist für die Nachhaltigkeit der Cloud verantwortlich, dies beinhaltet unter anderem die Bereitstellung einer effizienten, gemeinsam genutzten Infrastruktur, die Wasserverantwortung und die Beschaffung erneuerbarer Energien.
Der Cloud-Nutzer ist für die Nachhaltigkeit in der Cloud verantwortlich, also zum Beispiel das Optimieren von Workloads und Ressourcennutzung und das Minimieren der Gesamtressourcen, die für die Workloads bereitgestellt werden müssen~\cite{Mokhtari.2023}.
\begin{figure}
 \caption{Nachhaltigkeit auf Anwendungs- und Infrastrukturebene, eigene Darstellung nach ~\cite{Mokhtari.2023}}
 {\includegraphics[width=0.99\textwidth]{\figdir/SustainabilityOfAndOnInfrastructure}}
 \label{FIG:sustainability-infrastructure}
\end{figure}


even a static selection of where to host your workload can make a big difference~\cite{Norlander}

Eine Strategie im Cloud-Umfeld ist die Verwendung sogenannter Spot Virtual Machines.
Der Gedanke dahinter ist, nicht verwendete Instanzen von anderen virtuellen Maschinen herzunehmen, wenn diese nicht benötigt werden, anstatt eigene anzufragen.
Das wirkt sich nicht nur positiv auf die Kosten (bei Azure laut Microsoft bis zu 90\% Preisnachlass), sondern auch auf den Energieverbrauch aus, weil die Kapazität der VMs genutzt wird, anstatt dass sie sich im Leerlauf befinden.
Dieser Ansatz hat jedoch den Nachteil, dass der Workload bei fehlenden Kapazitäten nicht gestartet oder möglicherweise abgebrochen und nicht fortgesetzt werden kann~\cite{Norlander}
\section{Scheduling}
\subsection{Grundlagen des Scheduling}
Unter Scheduling versteht man im Allgemeinen die optimierte Zuteilung von Ressourcen zu Aufgaben in bestimmten Zeiträumen.
Dabei kann die zum Ziel gesetzte Optimierung vielerlei Ausführungen haben, zum Beispiel die Fertigstellungszeit so gering wie möglich zu halten oder die Anzahl der zu spät vollendeten Aufgaben zu minimieren~\cite{Gawiejnowicz.2020}.

Ein Scheduling-Problem S wird durch vier verschiedene Größen definiert~\cite{Gawiejnowicz.2020}:
\begin{itemize}
 \item Die Menge an auszuführenden Arbeiten J
 \item Die Menge an ausführenden Einheiten M
 \item Die Menge an zusätzlichen für die Ausführung benötigten Einheiten R
 \item Ein Maß für die Qualität der Lösung %ϕ.
\end {itemize}
Außerdem hat ein Scheduling-Problem verschiedene Eigenschaften, die im Folgenden kurz erklärt werden~\cite{Pinedo.2022}:
\begin{itemize}
 \item Bearbeitungszeit: Die Bearbeitungszeit eines Auftrags auf einer Maschine. Diese kann abhängig von der jeweiligen Maschine sein.
 \item Freigabedatum: Der Zeitpunkt, an dem der Auftrag im System eintritt und somit der frühestmögliche Zeitpunkt für den Beginn des Auftrags
 \item Fälligkeitsdatum: Der Zeitpunkt der zugesagten Fertigstellung. Muss das Fälligkeitsdatum eingehalten werden, wird es als Frist bezeichnet.
 \item Gewicht: Ein Prioritätsfaktor, der die Bedeutung des Auftrags im Verhältnis zu den anderen Aufgaben im System angibt
\end{itemize}

\subsection{Möglicher Einfluss von Scheduling auf Kohlenstoffemissionen}
Die Green Software Foundation definiert Kohlenstoffbewusstsein als Modifizierung von Berechnungen, um die Umwelt so wenig wie möglich zu belasten~\cite{GreenSoftwareFoundation.2022}.
Als Lösung sollen Software-Workloads, die viel Rechenleistung benötigen, auf Zeiten und Orte verlagert werden, bei denen der Strommix zu den geringst möglichen Kohlenstoffemissionen führt.
Für viele Unternehmen bietet die Verlagerung in die Cloud einen Ansatz der Verbesserung durch zentrale Verwaltung, Konsolidierung von Ressourcen und Effizienzsteigerungen.
Durch zunehmendes Migrieren in die Cloud wird es umso wichtiger, dass die entsprechenden Betreiber sich um den ökologischen Fußabdruck ihres Services kümmern~\cite{Buchanan.2023}.

Zur Veranschaulichung, welchen Einfluss die zeitliche Verschiebung von Workloads haben kann, soll zunächst ein beispielhaftes Verhältnis von Stromangebot und -nachfrage betrachtet werden (Abbildung~\ref{FIG:grid-supply-demand}).
\begin{figure}
 \caption{Eine beispielfhafte Darstellung von Angebot und Nachfrage eines Stromnetzes~\cite{Peixeiro.2022}.}
 {\includegraphics[width=0.9\textwidth]{\figdir/grid-supply-demand}}
 \label{FIG:grid-supply-demand}
\end{figure}
In dieser vereinfachten Darstellung wird davon ausgegangen, dass zur Mittagszeit die Verfügbarkeit von Solarstrom besonders hoch ist.
Aus diesem Grund übersteigt das Angebot in diesem Zeitraum die Nachfrage.
Dieses überschüssige Angebot könnte genutzt werden und der höhere Stromverbrauch würde keine zusätzlichen Emissionen verursachen.
Andererseits muss zu den Zeiten, zu denen kein oder nur wenig Solarstrom zur Verfügung steht, auf Gas zurückgegriffen werden, um die Nachfrage decken zu können~\cite{Buchanan.2023}.
Eine Studie von Microsoft~\cite{Dodge.06212022} aus dem Jahr 2022 hat aufgezeigt, dass durch Zeitverschiebung durchschnittlich 15\% weniger Kohlenstoffintensität (gemessen am \ac{SCI}) erreicht werden kann.
Die Studie hat sich zwar auf Aufgaben von Künstlicher Intelligenz in der Cloud konzentriert, sie zeigt jedoch, dass die zeitliche Verlagerung von rechenintensiven Workloads ein großes Verbesserungspotenzial birgt.
Das größte Potenzial der Zeitverschiebung liegt demnach in der Verschiebung von Workloads mit kurzer Dauer und großem Volumen in Regionen mit großer Unstetigkeit~\cite{Buchanan.2023}.

Einen weitaus größeren Einfluss als die zeitliche Verlagerung kann laut der Studie jedoch die örtliche Verlagerung haben.
Die Kohlenstoffintensität kann dadurch um bis zu 75\% reduziert werden~\cite{Dodge.06212022}.

Unterschied Kohlenstoffintensität auf zeitlicher und örtlicher Ebene!

\subsection{Dynamische Scheduling Modelle}\label{CAP:dynamic-scheduling-models}
Es sollen zwei zeitliche Optimierungsmethoden untersucht werden, die im folgenden kurz beschrieben werden~\cite{Dodge.06212022}:
\begin{itemize}
 \item Flexibler Start: Ein flexibler Workload wird zu einem Zeitpunkt mit minimaler marginalen Kohlenstoffintensität in den nächsten N Stunden gestartet.
 Dabei wird der Workload nicht unterbrochen, sondern wird bis zur Fertigstellung ausgeführt.
 Es werden alle möglichen Startzeiten im Zeitfenster N mit einer Granularität von fünf Minuten betrachtet.
 \item Anhalten und Fortsetzen: Ein Workload wird (möglicherweise mehrmals) angehalten und wieder neu gestartet, um sicherzustellen, dass er ausschließlich zu kohlenstoffarmen Zeiten läuft.
 Der Workload wird dadurch in den nächsten (N + Dauer des Auftrags) Stunden durchgeführt.
 Wichtige Voraussetzungen für diese Methode sind ebenfalls die zeitliche Flexibilität und zudem funktioniert sie nur für Workloads, die unterbrochen und wieder neu aufgenommen werden können.
 Zur Umsetzung werden genügend viele fünfminütige Intervalle mit geringsten Grenzemissionen während des Zeitfensters (N + Dauer des Auftrags) gesucht.
 Es wird davon ausgegangen, dass die Unterbrechung und der Neustart des Auftrags sofort erfolgen und keine zusätzliche Energie verbraucht wird.
 Dies ähnelt den Spot-Instanzen auf bestehenden Cloud-Plattformen, die einen vom Nutzer festgelegten Preis als Schwellenwert für automatisches Pausieren verwenden.
\end {itemize}
Zu beachten ist dabei, dass unterschiedliche Regionen sich durch verschieden starke Varianzen auszeichnen.
In manchen Regionen schwankt die Emissionsgröße innerhalb eines Tages stark, wodurch sich diese sehr gut für die Methode des Anhaltens und Fortsetzens eignen.
Andere Regionen haben dagegen relativ gleichbleibende Emissionen, sodass zeitliche Optimierungen nur einen geringen Einfluss haben können.
In Abbildung~\ref{FIG:flexible-start} wird die mögliche Emissionsverringerung durch einen flexiblen Start anhand des DenseNet 201 veranschaulicht.
DenseNet201 ist ein Convolutional Neural Network mit wenigen Schichten, das in der Studie von~\cite{Dodge.06212022} weniger als eine halbe Stunde benötigte.
Bei vielen Regionen ist eine Emissionsreduzierung um mehr als 20\% möglich, das beste Ergebnis liegt bei 80\% für die Region West US\@.
Die Wirksamkeit der eingesetzten Methode hängt demnach auch stark von der betrachteten Region ab~\cite{Dodge.06212022}.
\begin{figure}
 \caption{Die zu erwarteten Emissionseinsparungen durch Verschiebung der Startzeit um bis zu 24 Stunden (Flexible Start) für Dense 201~\cite{Dodge.06212022}}
 {\includegraphics[width=0.7\textwidth]{\figdir/flexible-start_measuring-carbon-intensity-of-ai}}
 \label{FIG:flexible-start}
\end{figure}
Betrachtet man ein zeitintensiveres Experiment, wie das Training eines Language Modells mit sechs Millionen Parametern, ist das Gegenteil zu beobachten.
Beim flexiblen Start ist das Optimierungspotential sehr gering (meist weniger als 1\%), jedoch bietet das Anhalten und Fortsetzen gute Optimierungsergebnisse.
In vielen Regionen sind Emissionsreduzierungen um mehr als 10\% und in der Region West US erneut am meisten mit ungefähr 28\% möglich~\cite{Dodge.06212022}.

\section{Scaling}
Skalierung bedeutet im Kontext des Softwarebetriebs eine Erhöhung oder Verringerung der Rechen-, Speicher- oder Netzwerkressourcen, die für einen bestimmten Workload genutzt werden.
Wohingegen man in herkömmlichen, dedizierten Hosting-Umgebungen durch die verfügbaren Hardwareressourcen begrenzt ist, bietet eine Cloud-basierte Anwendung diese Möglichkeit.
Es kann zudem zwischen vertikaler Skalierung (Scaling Up) und horizontaler Skalierung (Scaling Out) unterschieden werden.
Bei der vertikalen Skalierung wird die Rechenleistung erhöht und die Infrastruktur beibehalten~\cite{AlibabaCloudCommunity.20240118T09:19:15.000Z}.
% Load shifting
\section{Demand Shifting}
Eine Software-Anwendung kann nachhaltiger betrieben werden, indem der Strombedarf auf die Kohlenstoffintensität ausgerichtet wird.
Durch sogenanntes Demand-Shifting, also der Verschiebung der Nachfrage je nach Kohlenstoffintensität, kann Studien zufolge zwischen 45\% und 99\% Kohlenstoff eingespart werden.
Diese Verlagerung kann sowohl auf zeitlicher Ebene als auch auf örtlicher Ebene stattfinden.
Bei der örtlichen Verlagerung wird ein Ort, bei dem der Strom im Moment kohlenstoffärmer ist, gewählt.
Für die örtliche Verlagerung kann man sich die unterschiedlichen Wetterverhältnisse weltweit zu nutzen machen, denn wenn es z.B. in Deutschland im Moment wolkig und windstill ist, können an einem anderen Ort bei wolkenfreiem Himmel und Sonnenschein die perfekten Verhältnisse für die Solarstrom-Erzeugung herrschen.
Für die zeitliche Verlagerung ist ein umfangreiches Wissen über die Kohlenstoffintensität bzw. die Wetterverhältnisse im Verlauf der Zeit nötig.
Auf Grundlage dessen können Workloads in Zeiten mit geringer Kohlenstoffintensität verlagert werden~\cite{GreenSoftwareFoundation.2022}.
Eine beispielhafte Darstellung der zeitlichen Verlagerung ist in Abbildung~\ref{FIG:time-shifting} dargestellt.
\begin{figure}
 \caption{Zeitliche Verschiebung eines Workloads, um die Variabilität der Kohlenstoffintensität zu nutzen~\cite{Currie.2024}}
 {\includegraphics[width=0.7\textwidth]{\figdir/time-shifting}}
 \label{FIG:time-shifting}
\end{figure}
Die Kohlenstoffintensität (blaue Linie) schwankt im Laufe der Zeit stark.
Der Beispiel-Workload dauert eine Stunde lang und wird lediglich um eine Stunde verschoben, verbraucht aber dennoch deutlich weniger Kohlensoff.
Ein Szenario, das von der zeitlichen Verschiebung profitieren kann ist zum Beispiel das Batch Processing.
Ein Batch-Job eignet sich besonders gut dafür, da mehrere Aufgaben gruppiert und nacheinander ohne Benutzereingabe ausgeführt werden.
Außerdem sind sie oft zu einer beliebigen Zeit innerhalb eines bestimmten Zeitraums ausführbar.
Dazu zählen unter anderem Software-Updates und System-Backups.
Microsoft nutzt diesen Ansatz für Windows-Updates und erzielt damit eine Verringerung der Kohlenstoffemissionen um bis zu 99\%~\cite{Currie.2024}.
Das Demand Shifting auf örtlicher Ebene kann als eine Form des Scheduling betrachtet werden, denn beide verfolgen das Grundprinzip der optimalen zeitlichen Planung der Ausführung eines Workloads.

\section{Demand Shaping}
Eine ähnliche Methode, die ebenfalls das Ziel verfolgt, die Kohlenstoffemissionen zu senken, ist das Demand-Shaping.
Dabei wird bei geringer Kohlenstoffintensität die volle Funktionalität geboten und bei hoher Kohlenstoffintensität Einschränkungen getroffen oder die Funktionalität reduziert~\cite{Currie.2024}.

Man kann Demand Shifting und Demand Shaping also voneinander abgrenzen, indem man beim Demand Shifting einen bestimmten Workload betrachtet und die Verhältnisse dafür entweder auf örtlicher oder auf zeitlicher Ebene anpasst.
Beim Demand Shaping werden die aktuellen Verhältnisse betrachtet (Ort und Zeit sind unveränderlich) und der Workload, genauer gesagt die Höhe der Stromnachfrage, wird an diese angepasst.

Das Demand Shaping kann als eine Form des Scalings betrachtet werden, weil beide Prinzipien die Betriebsgröße bzw. Kapazität unter bestimmten Voraussetzungen anpassen.

Ein Beispiel für die Anwendung von Demand Shaping zeigt Google mit der optimierten Lastverteilung seiner Datencenter.
Täglich vergleicht Google die Vorhersage der erwarteten durchschnittlichen Kohlenstoffintensität im Verlauf des nächsten Tages (verwendet wird die Vorhersage von Electricity Maps) mit der Vorhersage der auf dem System zu erwartenden Last und passt diese beiden Größen aufeinander an.
Die Compute Tasks werden Stunde für Stunde an die Kohlenstoffwerte angepasst, um möglichst viel Last auf Zeiten mit geringen Emissionen zu verschieben.
Rundum verfügbare Services, wie die Google-Suche oder der Kartendienst Google-Maps sollen davon nicht beeinträchtigt werden, doch für nicht zeitkritische Updates, wie das Hinzufügen neuer Wörter zum Google-Übersetzer ist die Verschiebung geeignet.
Laut Google zeigt die Lastverteilung Erfolge und die Verwendung kohlenstoffarmer Energiequellen kann durch diesen Ansatz erfolgreich erhöht werden~\cite{Radovanovic.22.4.2020}.

Ein allgegenwärtiges Beispiel für Demand Shaping ist der bei Geräten wie Autos, Waschmaschinen oder Spülmaschinen verfügbare Eco-Modus.
Das Prinzip dahinter ist, dass auf Leistung verzichtet wird für weniger Ressourcenverbrauch.
Dies könnte auch auf Software-Anwendungen ausgeweitet werden.
Wichtig ist dabei, dass der Eco-Modus lediglich eine Option ist, aber der Leistungsverzicht nicht dauerhaft oder verpflichtend ist~\cite{GreenSoftwareFoundation.2022}.
Das Konzept eines Eco-Modus könnte auch für Software-Anwendungen basierend auf der Kohlenstoffintensität angewandt werden.
Dieser könnte eingeschränkte Funktionalitäten

\section{Geeignete Workloads}
Im Folgenden sollen die Eigenschaften für Workloads, die für zeitliche Verschiebungen geeignet sind, analysiert werden.
Konkret werden die Dauer, der Zeitpunkt der Ausführung und die Möglichkeit für Unterbrechungen untersucht.
Bezüglich der Dauer lassen sich Workloads grob in drei Kategorien unterteilen: kurze, lange und kontinuierliche Workloads.
Für die vorliegende Arbeit werden folgende Definitionen verwendet:
Als kurz wird ein Workload dann definiert, wenn er eine Laufzeit von wenigen Minuten bis hin zu ein paar Stunden hat.
Lang dauernde Workloads benötigen bis zu ihrer Fertigstellung mehrere Stunden bis hin zu einigen Tagen.
Bei kontinuierlichen Workloads ist die Ausführungszeit mehrere Wochen oder Monate lang oder es gibt kein definiertes Ende der Ausführung~\cite{Wiesner.2021}.

Die kurzen Workloads machen den Großteil aus, wie Untersuchungen für Google oder Alibaba gezeigt haben.
Diese haben analysiert, dass bei Google die meisten Workloads nur wenige Minuten dauern und bei Alibaba 90\% der Batch-Jobs weniger als 15 Minuten benötigen.
Wichtig für die Verschiebung sind vor allem zeitliche Beschränkungen.
Haben die Workloads eine baldige Frist, wie zum Beispiel CI-/CD-Läufe, so eignen sie sich nicht zur späteren Ausführung.
Jedoch können einige Batch-Jobs, darunter zum Beispiel nächtliche Backups, an bestimmte Vereinbarungen gekoppelt sein und deshalb eine höhere Flexibilität hinsichtlich des Ausführungszeitpunkts bieten.
Für länger laufende Workloads wie zum Beispiel das Trainieren von Machine-Learning-Modellen besteht das Problem, dass sie oft mehr Ressourcen anfragen, als eigentlich nötig.
Wegen ihrer oft Energie-intensiven Arbeit und der Tatsache, dass auf ihre Fertigstellung manchmal ein menschlicher Eingriff erfolgt, sind sie gut für die zeitliche Verschiebung geeignet.
Ist es dafür beispielsweise nicht entscheidend, ob der Workload direkt gestartet wird und dann nachts fertig ist, oder später gestartet wird und bis zum nächsten Morgen vollendet ist, ist das die perfekte Vorassetzung für eine mögliche spätere Ausführung.
Kontinuierliche Workloads umfassen zum Beispiel Blockchain-Mining, langandauernde wissenschaftliche Simulationen oder durchgehend verfügbare Benutzerschnittstellen.
Sie bieten zwar den Vorteil, dass in ihnen möglicherweise ein großes Einsparungspotenzial steckt, jedoch macht die lange oder unendliche Laufzeit eine zeitliche Verschiebung nur schwer oder gar nicht möglich~\cite{Wiesner.2021}

Betrachtet man den Zeitpunkt der Ausführung, können im wesentlichen zwei Unterscheidungen getroffen werden:
Ad-hoc-, also sofort auszuführende, und geplante Workloads.
Der Ausführungszeitpunkt und wie wichtig dessen Einhaltung ist, hat eine große Auswirkung auf die Eignung für zeitliche Verschiebung.
Ein Großteil der kurzen und langen Workloads sind Ad-hoc-Workloads.
Die Schwierigkeit besteht darin, die Ausstellung eines solchen Workloads im Voraus zu wissen.
Erst wenn der Workload erstellt wurde, kann anhand seiner zeitlichen Beschränkungen die Ausführung geplant werden.
Beispiel sind erneut CI-/CD-Läufe und Machine-Learning Trainings und zudem von bestimmten Ereignissen ausgelöste Workloads oder von Nutzern direkt für die Ausführung erstellte Workloads.
Geplante Workloads sind solche, deren Ausführung für einen Zeitpunkt in der Zukunft geplant ist.
Typisch dafür sind periodisch auftretende Workloads, wie beispielsweise nachts ausgeführte Integrationstests und Builds oder regelmäßige Backups, Aktualisierung von Suchindizes in Datenbanken und automatisch erstellte Berichte.
Der große Vorteil dieser ist, dass sie oft zeitlich sowohl zurück, als auch vor verschoben werden können.
Bei einem regelmäßig in der Nacht ausgeführten Integrationstests mag es zum Beispiel egal sein, ob er statt um 1 Uhr schon um 23 Uhr oder erst um 3 Uhr ausgeführt wird.
Diese Art von Workload eignet sich also meist hervorragend für eine zeitliche Verschiebung und kommt darüber hinaus auch oft vor.
Bei Microsoft sind bis zu 60\% der auf großen Klustern laufenden Workloads wiederkehrende Batch-Jobs, wovon mehr als 40\% täglich ausgeführt werden.
Andere gängige Ausführungsperioden sind 15 Minuten, eine Stunde und zwölf Stunden.
Bei Google zeigte sich zwischen 2011 und 2019 eine Veränderung hin zu mehr geplanten Workloads und einer deutlichen Erhöhung der Ausführungsrate~\cite{Wiesner.2021}.
\begin{figure}
 \centering
 \subfloat[\centering Zeitliche Verschiebung von Ad-hoc- und geplanten Workloads]{{\includegraphics[width=.45\textwidth]{\figdir/ScheduledVsAdhocWorkloads} }}%
 \qquad
 \subfloat[\centering Aufteilung von unterbrechbaren Workloads]{{\includegraphics[width=.45\textwidth]{\figdir/InterruptibleWorkloads} }}%
 \caption{Zeitpunkt der Ausführung und Unterbrechbarkeit von Workloads}%
 \label{FIG:workloads-execution-time-interruptibility}%
\end{figure}

Eine Unterscheidung kann außerdem zwischen unterbrechbaren und nicht unterbrechbaren Workloads gemacht werden.
Unterbrechbare Checkpoints verfügen über einen Checkpoint-Mechanismus oder speichern Zwischenergebnisse, sodass sie pausiert und später fortgeführt werden können.
Ein Beispiel dafür ist ein Training eines Machine-Learning-Modells, bei dem in regelmäßigen Abständen Checkpoints gespeichert werden, die später zur Analyse, zur Fortführung oder zur Fehlerbehebung hergenommen werden können.
Dabei handelt es sich vermehrt um lange Workloads.
Ein weiteres Beispiel könnte die Erstellung von monatlichen Geschäftsberichten für verschiedene Kunden sein, wobei die Workloads hier aus vielen kleineren Aufgaben bestehen und somit gut aufgeteilt werden können.
Daneben kann für andere Workloads die Unterbrechung nicht möglich oder nicht sinnvoll sein.
Bei bestimmten CI-/CD-Läufen oder Kompilierungsprozessen überwiegt die benötigte Energie zum Starten und Stoppen der Workloads der erzielbaren Einsparung, weil sie oft eine lange Setup- und Tear-Down-Zeit haben.
Bei Datenbankmigrationen oder -backups gilt es, Dateninkonsistenzen zu vermeiden, weshalb ein Abbruch nicht möglich ist.
Aufgrund der Notwendigkeit, solche Workloads am Stück auszuführen, sind diese weniger flexibel, wenn es um die Vermeidung von Emissionshöchstwerten des Stromnetzes geht~\cite{Wiesner.2021}.
%!
%!
\chapter{Grundlagen zur Zeitreihenprognose}
\section{Einführung in die Zeitreihenprognose}\label{CAP:intor-time-series-forecasting}
Eine Zeitreihe ist eine Sequenz von Datenpunkten in zeitlicher Abfolge~\cite{Peixeiro.2022}.
Diese spezielle Art von Datensatz dokumentiert, wie sich ein bestimmter Sachverhalt im Laufe der Zeit verhält.
Die wichtigste Spalte eines Zeitreihendatensatzes ist die Zeitspalte, welche die Grundlage für die Sortierung der Daten bildet.
Um Zeitreihendaten verarbeiten zu können, müssen oft spezielle Techniken zur Vorverarbeitung und zum Feature-Engineering angewandt werden~\cite{Lazzeri.2021}.

Eine Prognose ist eine Vorhersage zukünftiger Zustände und Entwicklungen, basierend auf Daten aus der Vergangenheit und mithilfe von Wissen über mögliche künftige Ereignisse, von denen die Entwicklung beeinflusst werden kann~\cite{Peixeiro.2022}.

Obwohl die Methodik der Zeitreihenprognose oberflächlich einer klassischen Regressionsanalyse ähnelt, indem sie vergangene Daten heranzieht, um zukünftige Werte als Funktion der Vergangenheit zu modellieren, weist sie doch wesentliche Unterscheidungsmerkmale auf.
Insbesondere die sequentielle Reihenfolge und die Möglichkeit, nicht zwingend über andere Merkmale als die Zielvariable verfügen zu müssen, unterscheidet sie von anderen Regressionsaufgaben.
Es ist durchaus üblich, dass eine Zeitreihe lediglich aus einer Zeitkomponente und einer dazugehörigen Wertspalte besteht, da die Zeitdimension alleine die Zielvariable definieren kann.
Die zeitliche Reihenfolge einzuhalten ist entscheidend, denn sie zeichnet die Beziehung der Werte untereinander aus und ist damit essentiell für die Gültigkeit der prognostizierten Ergebnisse~\cite{Peixeiro.2022}.

Eine Zeitreihe lässt sich über den Prozess der Dekomposition in drei Komponenten untergliedern:
Trend, Saisonalität und Residuen.
Der Trend beschreibt die langfristigen Veränderungen und Tendenzen in einer Zeitreihe, während die Saisonalität sich wiederholende Zyklen über einen bestimmten Zeitraum darstellt.
Residuen, auch als Rauschen bezeichnet, repräsentieren nicht durch den Trend oder die Saisonalität abzubildende, zufällige Schwankungen.
Die Analyse von Zeitreihen auf diese Weise eignet sich besonders gut, um Muster und Entwicklungen von Daten über die Zeit hinweg zu untersuchen und zu interpretieren~\cite{Peixeiro.2022}.

\section{SARIMAX als statistisches Basismodell}
Für die Zeitreihenprognose soll ein statistisches Modell namens SARIMAX als Basismodell dienen.
SARIMAX (Seasonal Auto-Regressive Integrated Moving Average with exogenous variables) kommt folgendermaßen zustande:
Für das ARMA-Modell werden die autoregressive (AR) und die Moving-Average- (MA) Komponente zusammengefügt.
Ein autoregressiver Prozess geht davon aus, dass die Zielvariable linear von ihren eigenen Werten in der Vergangenheit abhängt.
Beim Moving-Average-Prozess (zu Deutsch gleitender Durchschnittsprozess) wird der aktuelle Wert als lineare Kombination des Mittelwerts der aktuellen und vergangenen Fehlerterme dargestellt.
Das \ac{ARIMA}-Modell enthält zusätzlich noch einen Vorverarbeitungsschritt (I) für die Modellierung nicht-stationärer Zeitreihen.
Das ARIMA-Modell in Abhängigkeit von der Ordnung des AR-Prozesses $p$ (linear Regression auf die letzten $p$ Werte der Zeitreihe), der Ordnung der Integration $d$ und der Ordnung des MV-Prozesses $q$ (lineare Regression auf die letzten $q$ Fehlerwerte), kann durch Formel~\ref{eq:arima} ausgedrückt werden~\cite{Peixeiro.2022}.
$p$ gibt an, wie viele zeitversetzte Werte hinzugegeben werden, wohingegen $q$ die Anzahl der zeitversetzten Fehlerterme definiert.
\begin{equation}
 \label{eq:arima}
 y'_t = C + \phi_1 y'_{t-1} + \ldots + \phi_p y'_{t-p} + \theta_1 \varepsilon_{t-1} + \ldots + \theta_q \varepsilon_{t-q} + \varepsilon_t
\end{equation}
Der aktuelle Wert der differenzierten Reihe $y'_t$ ist somit die Summe aus einer Konstanten $C$, den Vergangenheitswerten dieser differenzierten Reihe $\phi_p y'_{t-p}$, vergangenen Fehlertermen $\theta_1 \varepsilon_{t-1}$ und dem aktuellen Fehlerterm $\varepsilon_t$.
$d$ ist hier nicht explizit angegeben, definiert jedoch die Anzahl der Differenzierungen, bis eine Reihe stationär wird.

Um saisonale Muster berücksichtigen zu können, wird aus dem \ac{ARIMA}-Modell das SARIMA-Modell, welches schließlich um exogene Variablen (X), also die Zielvariable zusätzlich beeinflussende Variablen, erweitert wird.
Basierend auf Formel~\ref{eq:arima} ergibt sich Formel~\ref{eq:sarimax} für das SARIMAX-Modell.
\begin{equation}
 \label{eq:sarimax}
 d_t = C + \sum_{n=1}^{p} \alpha_n d_{t-n} + \sum_{n=1}^{q} \theta_n e_{t-n} + \sum_{n=1}^{P} \phi_n d_{t-sn} + \sum_{n=1}^{Q} \eta_n e_{t-sn} + \sum_{n=1}^{r} \beta_n x_{t-n} + \varepsilon_t
\end{equation}
Ergänzt wurde die Formel durch die Saisonalität in Form einer zusätzlichen AR- ($\phi_n d_{t-sn}$) und MA- ($\eta_n e_{t-sn}$) Komponente verschoben um die Frequenz der Saisonalität $s$ (z.B.\ 12: monatlich oder 24: stündlich) und durch die exogenen Variablen ($\sum_{n=1}^{r} \beta_n x_{t-n}$)~\cite{Artley.26.4.2022}.

\section{Temporal Fusion Transformer als KI-basiertes Modell}
Neben dem Einsatz von SARIMAX als Basismodell, soll ein sogenannter \ac{TFT} angewendet werden.
Wissenschaftlern von Google gelang 2017 eine Forschungsarbeit~\cite{Vaswani.2017}, die die Forschung zur künstlichen Intelligenz vor allem im Bereich der natürlichen Sprachverarbeitung maßgeblich beeinflusste.
Sie stellten eine neue Architektur für neuronale Netze vor, sogenannte Transformer, die zunächst nur für die Sprachverarbeitung angewendet und getestet wurden, aber als grundlegendes Konzept auch andere Bereiche revolutionierten.
Anstelle von Feedback-Schleifen durch \ac{RNN}s, die üblicherweise in Encoder-Decoder-Architekturen verwendet wurden, setzten sie auf eine Methode namens \glqq Multi-headed Self-Attention\grqq{}.
Es stellte sich heraus, dass sich dieser neue Architekturansatz für verschiedenste Zwecke eignet und große Vorteile bietet, wie zum Beispiel die Parallelisierbarkeit, die Vielseitigkeit und nicht zuletzt die hohe Qualität der erzielten Ergebnisse~\cite{Vaswani.2017}.
Heute basieren bedeutende Modelle wie ChatGPT unter anderem auf Transformern.

Ein \ac{TFT} ist ein \ac{DNN} mit besonderer Architektur für das Multi-Horizon Forecasting, also die Vorhersage für mehrere zukünftige Zeitschritte.
Zunehmend wurden für diesen Anwendungsfall \ac{DNN}s eingesetzt, da sie die Leistung herkömmlicher Modelle übertreffen~\cite{Lim.19.12.2019}.
Die Idee des Attention-Mechanismus, der den Kern der Transformer-Architektur bildet, wurde für \ac{TFT} übernommen.
Um die Architektur auf die Varietät der Eingabedaten und deren zeitliche Beziehungen auszurichten, werden Codierer für die statischen Kovariaten, also Variablen zusätzlich zur Zielvariable, die Informationen über diese enthalten, eingesetzt, Gating-Mechanismen und stichprobenabhängige Variablenauswahl zur Minimierung irrelevanter Eingaben, eine Sequence-to-Sequence-Schicht zur Verarbeitung der Known und Observed Inputs und ein Self-Attention Decodierer zur Analyse der Eingabedaten auf langfristige Abhängigkeiten.
Bei anderen aufmerksamkeitsbasierten Methoden, die auch Transformer-basierte Modelle mit einschließen, besteht oft das Problem, dass die verschiedenen Datenquellen (z.B. statische Kovariaten) nicht ausreichend berücksichtigt werden.
Außerdem können sie oft nicht die Bedeutung verschiedener Daten in einem bestimmten Zeitabschnitt unterscheiden.
Andere \ac{DNN}s neben dem \ac{TFT} haben zum Beispiel den Nachteil, dass sie die zeitliche Anordnung der Eingangsmerkmale nicht berücksichtigen (konventionelle Formen von LIME und SHAP), obwohl diese Abhängigkeit in Zeitreihen signifikant sind~\cite{Lim.19.12.2019}.
%\ac{DNN}s werden zunehmen für diese Aufgabe herangezogen, weil sie verglichen mit herkömmlichen Zeitreihenmodellen große Leistungsverbesserungen aufzeigen.

Auch wenn der \ac{TFT} ein großes Modell ist und am besten für große Datenmengen geeignet ist, erzielt er auch für kleine Datensätze mit ca. 20 000 Samples gute Ergebnisse~\cite{Beitner.2020}.
Er verwendet als Input verschiedenste Daten, wie in Abbildung~\ref{FIG:tft-forecasting} dargestellt~\cite{Lim.19.12.2019}:
\begin{itemize}
 \item Werte der Zielvariablen aus der Vergangenheit (Past Targets)
 \item Zeitlich abhängige bekannte Informationen sowohl für die Vergangenheit, als auch für die Zukunft (Known Inputs)
 \item Zeitlich abhängige jedoch nur in der Vergangenheit bekannte Eingabewerte (Observed Inputs)
 \item Feststehende, zeitunabhängige Daten, die einen kontextuellen Zusammenhang bieten (statische Kovariaten)
\end{itemize}
\begin{figure}
 \caption{Multi-horizon Forecasting mit dem Temporal Fusion Transformer~\cite{Lim.19.12.2019}.}
 {\includegraphics[width=0.7\textwidth]{\figdir/TFT_multi-horizon-forecasting}}
 \label{FIG:tft-forecasting}
\end{figure}
Die Ausgabe des Modells ist nicht ein einziger Wert, sondern ein Werteintervall (Prediction Interval).
Die Multi-Horizon Vorhersage ist deshalb so komplex, weil trotz weniger Informationen darüber, das Zusammenspiel und die Abhängigkeit der Daten untereinander erlernt werden müssen~\cite{Lim.19.12.2019}.

Jeder Wert der Zielvariablen $i$ ist an eine oder mehrere statische Kovariaten $s_i$, an zeitabhängige Variablen $\chi_{i,t}$ und skalare Zielvariablen $y_{i,t}$ zu jedem Zeitschritt $t$ gebunden.
Wie in vorheriger Auflistung deutlich wurde, können die zeitabhängigen Variablen weiter unterteilt werden in Known Inputs ($x_{i,t}$) und Observed Inputs ($z_{i,t}$).
Wichtig sind außerdem sogenannte Quantile, denn sie ermöglichen die Vorhersage von Ergebnisintervallen, anstatt nur einzelner Werte und bieten auf diese Weise die Aussage über Best- und Worst-Case Werte (z.B. Ausgabe des 10., 50. und 90. Perzentil jedes Zeitschrittes)~\cite{Lim.19.12.2019}.

Formel~\ref{eq:tft} beschreibt die Prognose des $q$-ten Quantils des $\tau$-ten Vorhersageschrittes zum Zeitpunkt $t$.
Dabei werden alle Informationen innerhalb des endlichen Look-Back-Windows $k$ mit einbezogen, inklusive der Zielvariablen $y$ bis einschließlich des Startpunkts der Vorhersage $t$ ($y_{i,t-k:t} = \{y_{j,t-k}, \ldots, y_{j,t}\}$) und bekannter Variablen $x$ für den gesamten Zeitraum ($x_{i,t-k:t+\tau} = \{x_{i,t-k}, \ldots, x_{i,t}, \ldots, x_{i,t+\tau}\}$)~\cite{Lim.19.12.2019}.
\begin{equation}
 \label{eq:tft}
 \hat{y}_i(q, \tau, T) = f_q(\tau, y_{j,t-k:t}, z_{i,t-k:t}, x_{j,t-k:t+T}, s_i)
\end{equation}

Abbildung~\ref{FIG:tft-architecture} zeigt den Aufbau eines \ac{TFT}s, wovon die wichtigsten Bestandteile im Folgenden kurz erklärt werden.
\begin{figure}
 \caption{Vereinfachte Darstellung des Aufbaus eines Temporal Fusion Transformers~\cite{Labiadh.2023}.}
 {\includegraphics[width=0.90\textwidth]{\figdir/TFT-architecture}}
 \label{FIG:tft-architecture}
\end{figure}

Gating-Mechanismen in Form von \ac{GRN}s verfolgen das Ziel, nicht-lineare Verarbeitung nur dort anzuwenden, wo sie nötig ist.
Sie dienen dazu, ungenutzte Komponenten zu überspringen, sodass die Tiefe und Komplexität des Netzwerkes anpassungsfähig ist, um ein breites Spektrum an Szenarien und Datensätzen abdecken zu können~\cite{Lim.19.12.2019}.

Ein Variable Selection Network dient dazu, die relevanten Input-Variablen für jeden Zeitschritt auszuwählen.
Dadurch, dass eine Vielzahl von Variablen verfügbar ist, ist es essenziell, die Relevanz und den Beitrag der einzelnen Variablen zu untersuchen.
Die Variablen Auswahl, in der Architektur-Abbildung unten zu finden, wird auf statische und zeitabhängige Kovariaten angewendet.
Sie hilft, die auffälligsten Merkmale herauszufinden und diejenigen, die sich negativ auf das Ergebnis auswirken, herauszufiltern.
Dadurch kann sie die Leistung des Modells erheblich verbessern~\cite{Lim.19.12.2019}.

Ein wichtiger Bestandteil sind außerdem die statischen Kovariaten-Kodierer (Static Covariate Encoders), die in Abbildung~\ref{FIG:tft-architecture} links zu finden sind.
Mit Hilfe von diesen werden verschiedene Kontextvektoren erstellt, nämlich für die Auswahl zeitlicher Variablen, die lokale Verarbeitung zeitlicher Variablen und das Anreichern von zeitlichen Merkmale mit statischen Variablen.
Diese werden an den verschiedenen Stellen im Decoder verknüpft, um statische Merkmale in das Netzwerk zu integrieren.
Das unterscheidet das \ac{TFT}-Netzwerk von anderen Architekturen für die Zeitreihenprognose, die für statische Metadaten keine separaten Kodierer verwenden~\cite{Lim.19.12.2019, Labiadh.2023}.

Sowohl kurzfristige als auch langfristige zeitliche Zusammenhänge zu erfassen ist eine der Hauptaufgaben bei der Zeitreihenvorhersage.
Um beides ausreichend abdecken zu können, werden bei einem \ac{TFT} Sequence-to-Sequence-Schichten (kurzfristige Prognosen) und ein Multi-Head-Attention-Block (langfristige Prognosen) eingesetzt.
Die Sequence-to-Sequence-Schichten sind die Alternative zur Positionskodierung, die in Transformern üblicherweise verwendet wird.
Solche Schichten überzeugen durch ihre Fähigkeit, zeitliche Muster durch rekurrente Verbindungen festzuhalten~\cite{Labiadh.2023}.
Der in anderen Transformer-Architekturen verwendete Multi-Attention-Block wurde leicht abgewandelt, um die zeitlichen Beziehungen über verschiedene Zeitschritte zu erlernen.
Dafür wird ein sogenannter Self-Attention-Mechanismus eingesetzt, der es ermöglicht, Beziehungen über mehrere Zeitschritte hinweg zu erlernen.
Bei diesem wurde im Vergleich zu anderen Transformer-basierten Architekturen die Erklärbarkeit verbessert~\cite{Lim.19.12.2019}.
Er bewertet die Wichtigkeit einzelner Werte anhand von Beziehungen zwischen Schlüsseln und Abfragen~\cite{Labiadh.2023}.

Besonders ist zudem die verwendete Loss-Funktion.
Ein \ac{TFT} wird dadurch trainiert, dass die Quantilverluste über alle Quantilausgaben, d.h. Aussagen über die Verteilung des Zielwerts, minimiert werden.
Die spezielle Quantile Loss-Funktion, auch Pinball Regression genannt, ist durch Formel~\ref{eq:tft-quantile-loss} beschreibbar.
Bei Unterschätzungen wird der erste Summand der Funktion für obere Quantile hoch gewichtet und bei Überschätzungen bekommt der zweite Summan mehr Gewicht~\cite{Labiadh.2023}.
\begin{equation}
 \label{eq:tft-quantile-loss}
 QL(y, \hat{y}, q) = q \max(0, y - \hat{y}) + (1 - q) \max(0, \hat{y} - y)
\end{equation}

Die neue Form der Interpretierbarkeit eines \ac{TFT} ermöglicht ein einfaches Zurückverfolgen der relevantesten Zeitschritte für jede Prognose, was traditionelle Saisonalitäts- und Autokorrelationsanalysen ersetzen kann.
Außerdem hilft sie, signifikante Veränderungen zu erkennen, indem ein durchschnittlicher Aufmerksamkeitswert pro Vorhersagebereich berechnet wird und zu jedem Zeitpunkt als Vergleich dient~\cite{Labiadh.2023}.

Die Vorteile der Architektur eines \ac{TFT} sind unter anderem die Möglichkeit zur Verwendung der Kovariaten, was auch dazu führt, dass es auch für kurze Zeitreihen gute Ergebnisse erzielen kann.
Weitere Vorteile sind, dass mehrere Ziele gleichzeitig und sogar heterogene Ziele mit kontinuierlichen und kategorischen Variablen, also Regression und Klassifikation zur selben Zeit, unterstützt werden.
Die Anwendung von sowohl Regression als auch Klassifikation unterscheidet ihn zum Beispiel von DeepAR, einem Modell zur Zeitreihenprognose von Amazon, welches ausschließlich die Regression anwendet.
Der \ac{TFT} berücksichtigt zudem die Unsicherheit in Form von Quantilen, was ihn von RecurrentNetwork mit einfacher \ac{LSTM}- oder \ac{GRU}-Schicht und Ausgabeschicht oder von NBeats und NHiTS unterscheidet.
Durch die Nutzung statischer Kovariaten kann der \ac{TFT} auch mit sogenannten \glqq Cold-Start-Problemen \grqq{}, also Prognosen ohne oder mit nur sehr wenigen historischen Daten, umgehen.
Die komplexe Architektur kann mit Blick auf Anforderungen an die Rechenleistung zum Nachteil werden.
Im Vergleich zu anderen Aufgaben zum Beispiel im Bereich Computer Vision oder Sprachverarbeitung sind Zeitreihendaten meist kleiner, wodurch eine höhere Komplexität des verwendeten Modells den positiven Effekt haben kann, dass die verwendete Hardware ausgereizt wird~\cite{PytorchForecastingDocumentation.20230410T20:05:43.000Z}.
Dies ist auch im Hinblick auf Nachhaltigkeit ein guter Ansatz, weil die Hardware effizient genutzt wird.
%!
%!
\chapter{Prognose der Kohlenstoffintensität}
\section{Auswahl, Vorverarbeitung und Analyse der Daten}\label{CAP:data-preparation-analysis}
Die Kohlenstoffintensität von Strom hängt stark von der Verfügbarkeit erneuerbarer Energien ab und diese sind wiederum auf bestimmte Wetterverhältnisse angewiesen.
Um präzise Vorhersagen zu treffen, ist es daher wichtig, sowohl historische Kohlenstoffintensitätswerte zu analysieren, als auch den Einfluss des Wetters zu berücksichtigen.
Auf dieser Grundlage kann dann eine möglichst genaue Prognose für zukünftige Zeitpunkte erstellt werden.

Die \ac{MOER}, also die Grenzbetriebsemissionsrate wie sie auch im SCI-Index verwendet wird, ist dafür das geeignete Maß.
Diese gibt die Emissionsintensität der Grenzkraftwerke an, die auf die veränderte Stromnachfrage reagieren~\cite{Buchanan.2023}.
Sie eignet sich für den vorliegenden Anwendungsfall besser, als die durchschnittliche Kohlenstoffintensität, da sie die Emissionen pro Lastverschiebung angibt und somit die Beziehung zwischen Ursache und Wirkung der Lastverschiebung erfasst\cite{Wiesner.2021}.

Als Datengrundlage dient das Datenmodell zur \ac{MOER} von WattTime.
Die \ac{MOER} wird für einen bestimmten Zeitpunkt und Ort angegeben und kann dadurch gut verglichen werden.
Die Schwierigkeit bei der Datenerfassung zur \ac{MOER} besteht darin, dass zwei Fälle miteinander verglichen werden müssen.
Zum Einen, dass die Lastverschiebung stattfindet und zum Anderen dass sie nicht stattfindet, von denen nicht beide gleichzeitig umgesetzt werden können.
Durch schwankende Bedingungen im Stromnetz ändert sich die \ac{MOER} ständig, weshalb man den Fall der Lastverschiebung und den Fall, in dem keine Last verschoben wird, nicht einfach hintereinander ausprobieren kann.
Eine Lösung dieses Problems ist, auch die Netzbedingungen zu betrachten und davon auszugehen, dass bei gleichen Bedingungen auch die \ac{MOER} gleich ist.
Diesen Ansatz nutzt WattTime und überprüft dessen Genauigkeit durch verschiedenste Experimente~\cite{WattTime.2022}.
% pro kWh wird soviel mehr g CO2 ausgestossen
% Electricity Maps Beitrag zu MOER !

Die beiden Länder wurden aufgrund folgender Kriterien ausgewählt:
\begin{itemize}
 \item Repräsentativer Charakter: In die Entscheidung fließt die Verfügbarkeit von Rechenzentren der drei größten öffentlichen Cloud-Anbieter - \ac{AWS}, Microsoft Azure und Google Cloud - in diesen Ländern ein.
 Alle drei Anbieter haben Verfügbarkeiten in Deutschland.
 In Norwegen sind zum jetzigen Stand keine Datencenter von \ac{AWS} stationiert.
 Google Cloud kündigt an, demnächst in dieser Region verfügbar zu sein und Microsoft Azure haben dort bereits Datencenter~\cite{AmazonWebServices.20240318T18:30:19.000Z}\cite{GoogleCloud.20240311T07:04:17.000Z}\cite{Microsoft.20240307T01:34:37.000Z}.
 \item Verfügbarkeit: Für beide Länder bietet WattTime eine ausreichende Datenverfügbarkeit.
 Ziel war es, eine Datenbasis von mindestens einem kompletten Jahr für das Training zu haben.
 Für Deutschland ist dieses Ziel mit Beginn der Datenerfassung im Oktober 2022 gerade so gegeben, für Norwegen sind mehr als ausreichend Daten vorhanden (Beginn der Datenerfassung ist Januar 2021).
 \item Diversität: Durch die Auswahl von Deutschland und Norwegen können zwei Stromnetze mit unterschiedlicher Zusammensetzung der Energiequellen (mehr dazu im Verlauf dieses Kapitels) analysiert werden.
 Neben Deutschland mit einer im internationalen Vergleich Kohlenstoffintensität, wird mit Norwegen eines der Länder mit der niedrigsten Kohlenstoffintensität für eine gute Vergleichbarkeit gewählt.
\end{itemize}
Die \ac{MOER}-Werte werden über die \ac{API} von WattTime pro Monat für die Länder Deutschland und Norwegen abgefragt.
Um auf die Daten zugreifen zu können, registrieren sich berechtigte Nutzer mit ihrer E-Mail-Adresse über einen Registrierungsendpunkt und loggen sich anschließend über einen Login-Endpunkt ein, um einen Token zu erhalten.
Dieser Token ist eine halbe Stunde lang gültig und ermöglicht durch Setzen im Header die Abfrage der Daten über die \ac{API}~\cite{.20240220T17:59:19.000Z}.
Der Vorgang der Datenabfrage und Speicherung ist in Code-Ausschnitt~\ref{CODE:watttime_get_moer} zu sehen.
Die Daten werden ab dem frühst möglichen Zeitpunkt, also dem Zeitpunkt ab dem WattTime mit der Datenerfassung begonnen hat, abgefragt.
Für Deutschland ist das ab Oktober 2022 und für Norwegen ab Januar 2021.

Zur Verwendung der Daten für die Vorhersage sind ein paar Vorverarbeitungsschritte nötig.
Die Daten stehen in einem Intervall von fünf Minuten zur Verfügung, was insgesamt zu einer sehr großen Datenmenge führen würde, z.B. für Deutschland $12 * 24 * 457 = 131 616$ Datenpunkte.
Außerdem wird ein stündliches Intervall für die Vorhersage als ausreichend angesehen.
Die \ac{MOER}-Werte werden von der Einheit lbs/\ac{kWh} in g/MWh umgerechnet und die Informationen über die Zeitzone werden entfernt, da sich die Daten bereits in \ac{UTC} befinden.
Diese Zeitzone wird als Standard für alle Daten verwendet, sodass später ein einfaches Zusammenfügen aus einzelnen Datenquellen und Vergleichen der Daten untereinander ohne Konvertierung möglich ist.
Die \ac{MOER}-Werte der beiden Länder werden separat in eine CSV-Datei eingelesen.

Die Daten zur \ac{MOER} sollen zunächst für die beiden Länder analysiert werden, um ein Bild über deren Beschaffenheit zu erhalten.
Zwischen den beiden Ländern ist ein deutlicher Unterschied für die MOER-Werte zu erkennen.
In Abbildung~\ref{FIG:moer_distribution} wird ersichtlich, dass Norwegens Werte weitaus niedriger sind, als die Deutschlands.
\begin{figure}
 \caption{Die Verteilung der MOER-Werte von Deutschland und Norwegen (eigene Darstellung)}
 {\includegraphics[width=0.6\textwidth]{\figdir/moer_distribution}}
 \label{FIG:moer_distribution}
\end{figure}
Der Mittelwert für Deutschland ist mit rund 762g/kWh mehr als dreimal so hoch wie der Mittelwert Norwegens (rund 236g/kWh).
Bei Betrachtung der Häufigkeit der Werte muss berücksichtigt werden, dass für die beiden Länder nicht die gleiche Anzahl an Werten vorliegt.
Für Deutschland liegen insgesamt 10986 Werte vor und für Norwegen 26280 Werte vor.

Eine Zerlegung in Trend, Saisonalität und Residual, wie in Kapitel~\ref{CAP:intor-time-series-forecasting} beschrieben, soll mehr Aufschluss über die Daten geben.
Die Zerlegung für die Werte von Deutschland ist in Abbildung~\ref{FIG:moer_decomposition_DE} zu sehen.
\begin{figure}
 \caption{Die Dokomposition der MOER-Werte von Deutschland (eigene Darstellung)}
 {\includegraphics[width=0.99\textwidth]{\figdir/moer_decomposition_DE}}
 \label{FIG:moer_decomposition_DE}
\end{figure}
Bei der Trend-Komponente ist zu erkennen, dass die Werte zwar teils starke, teils weniger starke Schwankungen haben, jedoch über die Zeit kein klarer Auf- oder Abwärtstrend erkennbar ist.
Für die Saisonalität wird lediglich ein Ausschnitt der Daten (Oktober 2022 bis Dezember 2022) abgebildet, weil sonst die Menge der Daten zu groß wäre, um die Saisonalität zu erkennen.
Die Daten weisen deutlich eine tägliche Saisonalität auf, weil sich je Tag immer der gleiche Zyklus wiederholt.
Zuerst fällt der Wert stark ab und steigt anschließend wieder stark an.
Die Residuen zeigen deutliche, durch Trend und Saisonalität nicht abgedeckte Schwankungen, doch insgesamt, dass die Daten stationär sind.
Für Norwegen zeigt die Dekomposition (siehe Abbildung~\ref{FIG:moer_decomposition_NO}) ähnliche Muster auf.
Die \ac{MOER}-Werte haben keinen klaren Auf- oder Abwärtstrend.
Sie verfügen ebenfalls über ein sich täglich wiederholendes Muster, dessen Verlauf sich jedoch von dem der deutschen Werte unterscheidet.
Das Rauschen ist erneut sehr stark ausgeprägt und zeigt, dass viele Schwankungen nicht durch die Trend- und Saisonalitätskomponente abgedeckt werden können.

Für Vorhersagen der \ac{MOER} soll die Zusammensetzung der Stromproduktion betrachtet werden, um wichtige Faktoren ausfindig machen zu können.
Die Energiequellen der Stromproduktion in Deutschland und Norwegen im Jahr 2023 sind in Tabelle~\ref{TAB:energyproduction_de_no} aufgelistet.
\begin{table}[t]
 \centering\small
 \caption{Der Anteil der Energiequellen an der Stromproduktion in Deutschland und Norwegen im Jahr 2023 (\cite{ElectricityMaps.20240305T20:54:29.000Z})}
 \label{TAB:energyproduction_de_no}
 \input{\tabledir/energyproduction_de_no.tex}
\end{table}
Ersichtlich wird, dass Deutschland eine Vielfalt an Energiequellen einschließlich Kohle, Gas, Kernenergie und erneuerbaren Energien hat.
Norwegen ist durch einen sehr großen Anteil fast vollständig auf Wasserkraft als erneuerbare Energiequelle angewiesen.

Die Verfügbarkeit erneuerbarer Energien hängt maßgeblich vom Wetter ab.
Dieser Zusammenhang kann für die Vorhersage genutzt werden, weshalb Wetterdaten zum Datensatz hinzugefügt werden.
Dafür wird auf den Copernicus Climate Change Service~\cite{Copernicus.20231212T14:09:40.000Z} zurückgegriffen, der umfangreiche Datensätze rund um das weltweite Klima und den Klimawandel zur Verfügung stellt.
Konkret wurden vier mögliche Einflussfaktoren für die Kohlenstoffintensität auf Basis von Tablle~\ref{TAB:energyproduction_de_no} als wichtig identifiziert und ausgewählt:
die Lufttemperatur in 2 m Höhe, die Windgeschwindigkeit in 100 m Höhe, der Niederschlag und die globale horizontale Bestrahlungsstärke (im Englischen als \ac{GHI} bezeichnet).
Letztere wird durch die atmosphärische Zusammensetzung mit Aerosolen, Wasserdampf und Ozon, hauptsächliche jedoch von Wolken beeinflusst~\cite{KallioMyers.2020}.
Für die Stromerzeugung durch Photovoltaik ist in erster Linie die Sonneneinstrahlung, die auf das Solarmodul einfällt, entscheidend~\cite{James.}.
Die Erzeugung durch Windkraft hängt hauptsächlich von der Windgeschwindigkeit ab.
Für die Wasserkraft ist unter anderem der Niederschlag ausschlaggebend~\cite{Copernicus.20231212T14:09:40.000Z}.

Bei den Werten handelt es sich um Daten des ERA5 Reanalysis Datensatzes.
Dies sind von verschiedenen Quellen, wie Satelliten oder Wetterstationen, gemessene Daten, bei denen fehlende Werte mit durch physikalische Gesetze oder historische Verlaufsmuster generierten Daten ergänzt wurden.
Grundsätzlich sind die Daten also echt gemessene Daten, die zur Vollständigkeit mit generierten Daten ergänzt wurden~\cite{CopernicusClimateChangeService.2020}~\cite{CopernicusKnowledgeBase.20231009}.

Außerdem werden zeitliche Variablen hinzugefügt, die den MOER-Wert ebenfalls beeinflussen können: Wochentag, Jahreszeit und, ob es sich um einen Feiertag handelt.
Die jeweiligen Werte für die Variablen werden kodiert, sodass sie verarbeitet werden können.

In einer Correlation Matrix soll analysiert werden, ob und wie die einzelnen Variablen zusammenhängen.
Für Deutschland und Norwegen wird je eine dieser Matrizen erstellt, zu sehen in Abbildung~\ref{FIG:correlation_matrix}.
\begin{figure}
 \caption{Der Zusammenhang zwischen den verwendeten Variablen für Deutschland und Norwegen dargestellt in je einer Correlation Matrix (eigene Darstellung)}
 {\includegraphics[width=0.99\textwidth]{\figdir/correlation_matrix}}
 \label{FIG:correlation_matrix}
\end{figure}
Insgesamt geht hervor, dass der lineare Zusammenhang zwischen den \ac{MOER}-Werten und den anderen Variablen eher gering ist.
Der größte Zusammenhang besteht mit der Windgeschwindigkeit (-0,31) und der Sonneneinstrahlung (-0,24) in Deutschland.
Das bedeutet, dass sich bei höheren Werten für diese beiden Variablen, niedrigere Werte für die \ac{MOER} verzeichnen lassen.
Vor dem Hintergrund der Energiequellen des deutschen Stromnetzes scheint die Interpretation sinnvoll, dass bei steigender Windstärke und Sonneneinstrahlung mehr Strom durch Solarstrom und Strom aus Windkraftwerken gewonnen werden kann und somit die Emissionsgröße sinkt.
Für Norwegen zeigt die Windgeschwindigkeit die höchste Abhängigkeit aller analysierten Werte, die mit eineme Wert von -0,13 jedoch eher zu vernachlässigen ist.

Der zuvor beschriebene Datensatz wird aufgeteilt in einen Trainingsdatensatz, einen Validierungsdatensatz und einen Testdatensatz, wie in Abbildung~\ref{FIG:moer_train_validation_test} sichtbar wird.
Als Unterteilung wird das Datum 30.09.2023, bzw. 30.11.2023 verwendet.
Auf diese Weise wird sichergestellt, dass die Saisonalität vom Modell erkannt werden kann, denn es bekommt im Training genau ein Jahr an Daten für Deutschland zu sehen.
Für Deutschland ergibt sich daraus ein Trainingszeitraum von einem Jahre, ein Validierungszeitraum von zwei Monaten und ein Testzeitraum von einem Monat.
Für Norwegen stehen für das Training 33 Monate zur Verfügung, die anderen beiden Zeiträume sind identisch.

\section{Modellentwicklung und Training mit SARIMAX}
Für die Verwendung von auf ARIMA basierenden Modellen ist es zu aller Erst nötig, die Daten auf Stationarität zu untersuchen, weil diese eine Voraussetzung für die Modelle ist.
Stationär ist eine Zeitreihe dann, wenn ihr Mittelwert, ihre Varianz und ihre Autokorrelation konstant und zeitunabhängig sind.
Die vorherige Dekomposition der Daten weist darauf hin, dass diese stationär sind, jedoch soll diese Vermutung nochmals bestätigt werden.
Mit Hilfe eines \ac{ADF} Tests werden die Daten auf eine Einheitswurzel getestet (Nullhypothese).
Gibt es keine Einheitswurzel in der Zeitreihe (Alternativhypothese), so ist sie stationär.
Dafür wird die Implementierung von statsmodels verwendet, bei der die Daten lediglich in die \lstinline[columns=fixed]{adfuller} Funktion eingesetzt werden müssen.
Der Test liefert die ADF-Statistik, eine negative Zahl, die je kleiner sie ist desto stärker die Nullhypothese ablehnt und den p-Wert, der der Nullhypothese widerspricht, wenn er kleiner als 0,05 ist.
Code-Ausschnitt~\ref{CODE:adf-test_moer-de} zeigt den \ac{ADF} Test für die \ac{MOER}-Zeitreihe für Deutschland.
\lstinputlisting[language=Python, caption=Python-Code zur Überprüfung der MOER-Werte für Deutschland auf Stationarität mithilfe des ADF-Tests, label=CODE:adf-test_moer-de]{\codedir/adf-test_moer-de.m}
Das Ergebnis zeigt, dass die Nullhypothese aufgrund des p-Wertes abgelehnt wird und die Daten somit stationär sind.
Die \ac{MOER}-Werte für Norwegen sind ebenfalls stationär, jedoch ist die Ablehnung der Nullhypothese mit einer ADF-Statisik von rund -4 weniger stark.
Wären die Daten nicht stationär, müssten sie solange differenziert werden, bis sie Stationarität aufweisen~\cite{Peixeiro.2022}~\cite{Rahmadhan.8.5.2023}.

Zur Anwendung des SARIMAX-Modells wird die Implementierung von \glqq statsmodels\grqq{} verwendet.
Ein SARIMAX-Modell muss, anders als beim \ac{TFT} mit der Möglichkeit separater Gruppen-Ids, für jedes der beiden Länder separat erstellt werden
Definiert wird das Modell durch Angabe der Zielvariable, der exogenen Variablen, der Parameter für den nicht-saisonalen Teil des Modells (\lstinline[columns=fixed]{p,d,q}) und derjenigen für den saisonalen Teil des Modells (\lstinline[columns=fixed]{P,D,Q,s}).

Die Festlegung der Parameter für SARIMAX kann durch Analyse der \ac{ACF} und der \ac{PACF} erfolgen.
Eine Alternativmethode ist, verschiedene Werte für die Parameter automatisch überprüfen zu lassen.
Dafür kann die \lstinline[columns=fixed]{auto_arima} Funktion der pmdarima Bibliothek verwendet werden.
Sie hilft, die optimalen Parameterwerte zu finden, indem sie die Kombination ausfindig macht, bei der das \ac{AIC} am niedrigsten ist~\cite{Rahmadhan.8.5.2023}.
Das \ac{AIC} bewertet die Qualität eines Modells im Vergleich zu anderen Modellen durch Messen der durch das jeweilige Modell verlorenen Informationen.
Ein niedrigerer \ac{AIC}-Wert bedeutet demnach, dass weniger Informationen verloren gingen und das Modell deshalb besser abschneidet.
Für die Funktion kann unter anderem die Testmethode angegeben werden, hier wird der \ac{ADF} Test verwendet.
Außerdem werden für \lstinline[columns=fixed]{p} und \lstinline[columns=fixed]{q} Start- und Maximalwerte definiert.
Als Startwert wird für beide Parameter 0 gewählt und als Maximalwert 3~\cite{Peixeiro.2022}.
Die Frequenz wird in dieser Implementierung als Parameter m angegeben, entspricht aber dem sonst verwendet s und wird der täglichen Saisonalität wegen auf 24 gesetzt.
Der Test ergibt, dass das Modell am besten als \lstinline[columns=fixed]{ARIMA(2,0,0)(0,0,0)[24] intercept} funktioniert.
Zu beachten ist, dass bei Setzen von \lstinline[columns=fixed]{P, D} und \lstinline[columns=fixed]{Q} auf 0 das Modell keine Saisonalität berücksichtigt und somit einem ARIMA-Modell entspricht.

Basierend auf diesem Ergebnis, wird das Modell mit diesen Parametern erstellt und trainiert, zu sehen in Code-Ausschnitt~\ref{CODE:sarimax_model-train}
\lstinputlisting[language=Python, caption=Python-Code zum Erstellen und Trainieren des SARIMAX-Modells, label=CODE:sarimax_model-train]{\codedir/sarimax_model-train.m}

Nach automatischer Wahl der Parameter soll an dieser Stelle zusätzlich die \ac{ACF} und \ac{PACF} der Zeitreihe analysiert werden.
Korrelation ist ein Maß, um die lineare Beziehung zweier Variablen zu messen.
Die \ac{ACF} misst für eine Zeitreihe diese Beziehung zwischen zwei um eine bestimmte Anzahl an Zeitschritten versetzten Werten.
Sie misst dadurch die Korrelation der Zeitreihe mit sich selbst und gibt Hinweise auf die Moving Average Komponente eines ARIMA Modells~\cite{Peixeiro.2022}.
Das \ac{ACF}-Diagramm für die \ac{MOER}-Werte für Deutschland ist in Abbildung~\ref{FIG:acf_pacf_moer_de}~(a) zu sehen.
\begin{figure}
 \centering
 \subfloat[\centering ACF]{{\includegraphics[width=.4\textwidth]{\figdir/acf_moer_de} }}%
 \qquad
 \subfloat[\centering PACF]{{\includegraphics[width=.4\textwidth]{\figdir/pacf_moer_de} }}%
 \caption{Die ACF und PACF der MOER-Zeitreihe für Deutschland (eigene Darstellung)}%
 \label{FIG:acf_pacf_moer_de}%
\end{figure}
Die x-Achse des Diagramms repräsentiert die Anzahl der zeitlichen Verzögerungen, während die y-Achse das Ausmaß der Korrelation zwischen den Zeitpunkten darstellt.
Zu sehen ist eine Abnahme der Korrelation bei den ersten Zeitversetzungen, was auf eine starke Korrelation der zeitlich nahe beieinander liegenden Werte hinweist.
Das bedeutet, dass die Messwerte aus den vergangenen Stunden einen signifikanten Einfluss auf die der folgenden Stunden ausüben.
Die \ac{ACF} zeigt außerdem, dass die Daten trotz ihrer Stationarität saisonale Muster aufweisen.
Nach 24 Zeitschritten, also einem Tag, ist eine wiederkehrende Spitze in den Daten erkennbar und das schwingende Muster scheint sich mit einem täglichen Rhythmus zu wiederholen.
Dies weist auf einen täglichen Zyklus der Zeitreihe hin.
Gegen Ende der aufgezeigten Zeitverzögerungen nähern sich die Korrelationswerte der Nulllinie an und liegen innerhalb des Konfidenzintervalls (als hellblauer Bereich gekennzeichnet), was bestätigt, dass die Daten stationär sind.
Statistisch signifikante Werte außerhalb des Konfidenzintervalls deuten darauf hin, dass die Daten mithilfe vergangener Werte vorhersagbar sind.

Die \ac{PACF} wird genutzt, um die autoregressive Komponente eines ARIMA-Modells zu identifizieren.
Sie misst die Korrelation zwischen der Zeitreihe und zeitlich verschobenen Werten der Zeitreihe, jedoch unter Kontrolle der Zwischenzeitpunkte.
Dies bedeutet, dass die \ac{PACF} die direkte Korrelation zwischen Beobachtungen darstellt, die um eine bestimmte Anzahl von Zeitschritten verschoben sind, unabhängig von den Korrelationen kürzerer Verzögerungen~\cite{Peixeiro.2022}.
In der \ac{PACF} der \ac{MOER}-Werte für Deutschland (siehe Abbildung~\ref{FIG:acf_pacf_moer_de}~(b)) ist eine starke partielle Auto-Korrelation bei Zeitverzögerung 1 zu beobachten.
Nach dieser ersten Zeitverzögerung sinken die Korrelationswerte sehr stark ab und nähern sich der Nulllinie, was darauf hindeutet, dass eine autoregressive Komponente erster Ordnung für das ARIMA-Modell geeignet sein könnte.
Die näherungsweise Null-Korrelation bei höheren Zeitverzögerungen stützt diese Annahme, da keine weiteren signifikanten Korrelationen ersichtlich sind, die eine höhere Ordnung der AR-Komponente rechtfertigen würden.

Die Parameter für das SARIMAX-Modell können anhand der beiden Funktionen nicht immer eindeutig bestimmt werden.
Für \lstinline[columns=fixed]{p} wird nach Analyse der \ac{PACF} wie erwähnt der Wert 1 verwendet.
Nachdem der \ac{ADF}-Test bestätigt hat, dass die Daten stationär sind, werden \lstinline[columns=fixed]{d} und \lstinline[columns=fixed]{D} auf 0 gesetzt.
Die tägliche Saisonalität wurde durch die \ac{ACF} bestätigt, sodass \lstinline[columns=fixed]{s} gleich 24 gesetzt wird.
Bei den restlichen Parameter ist eine eindeutige Entscheidung nicht gegeben, sodass die optimalen Werte zum Beispiel durch Ausprobieren ausfindig gemacht werden können.
\lstinline[columns=fixed]{q} wäre durch einen starken Abfall der Korrelation im \ac{ACF}-Diagramm definiert.

\section{Modellentwicklung und Training mit Temporal Fusion Transformer}
Für die Implementierung des \ac{TFT} wird das Modell von \glqq pytorch forecasting\grqq{}, einem Package für Zeitreihenvorhersage mit neuronalen Netzen, verwendet~\cite{PytorchForecastingDocumentation.20230410T20:05:46.000Z}.
Zunächst muss dem Zeitreihen-Datensatz ein Zeitindex hinzugefügt werden, der sich bei jedem Zeitschritt (hier eine Stunde) um eins erhöht.
Die Daten werden in einen Trainings-, einen Validierungs- und einen Testdatensatz eingeteilt und je ein dataloader dafür erstellt.
Code-Ausschnitt~\ref{CODE:tft_training-dataset} zeigt das Vorgehen.
\lstinputlisting[language=Python, caption=Python-Code zur Erstellung des Trainingsdatensatzes für den TFT, label=CODE:tft_training-dataset]{\codedir/tft_training-dataset.m}
Dem Trainingsdatensatz werden Parameter mitgegeben, um den Aufbau und die Struktur der Eingabedaten für das Modell zu definieren.
Bei Zeitreihenprognosen mit mehreren Zeitreihen, die einen unterschiedlichen Datenbereich haben, ist es entscheidend, den Trainingsdatensatz für jede der Reihen anzupassen.
Durch die Berechnung eines spezifischen Cut-off-Werts auf Grundlage der maximalen Vorhersagelänge, wird sichergestellt, dass künftige Daten den Lernprozess des Modells nicht beeinflussen.
Dadurch kann ein realistischer Trainingsaufbau und eine faire Bewertung der Vorhersagefähigkeit des Modells gewährleistet werden.
Eine Abfrage des maximalen Zeitindex des Trainingsdatensatzes und des minimalen Zeitindex des Validierungsdatensatzes gruppiert nach Land garantiert, dass sich die Daten nicht überschneiden und kein Leck enhalten.
Wichtig ist darüber hinaus eine korrekte Zuordnung der Variablen zu den verschiedenen Variablenarten~\cite{GitHub.20240307T20:56:16.000Z}.
Im betrachteten Anwendungsfall der Vorhersage von \ac{MOER}-Werten, werden diese als Zielvariable festgelegt.
Das Land wird als Gruppen-Id gesetzt, um zu definieren, dass ausgehend vom Land zwei verschiedene Zeitreihen existieren.
Zwei für die Vorhersage wichtige Angaben sind die Anzahl der zu verwendenden Vergangenheitswerte (\lstinline[columns=fixed]{max_encoder_length}) und wie weit in die Zukunft vorhergesagt wird (\lstinline[columns=fixed]{max_predicition_length}).
Da die vorherige Analyse eine deutliche tägliche Saisonalität in den Daten ersichtlich machte, wird für beide größen die Länge eines Tages (24 Stunden) gesetzt.
Das Land wird außerdem auch als statische, kategorische Variable \lstinline[columns=fixed]{static_categoricals} angegeben, da sich die Werte im Lauf der Zeit nicht verändern.
Kategorische Werte, die sich über die Zeit ändern, aber im Voraus bekannt sind (Jahreszeit, Wochentag und, ob es sich um einen Feiertag handelt), werden als \lstinline[columns=fixed]{time_varying_known_categoricals} gesetzt.
Sich über die Zeit ändernde, im Voraus bekannte, aber kontinuierliche Werte, sind die Wetterdaten (Temperatur, Windgeschwindigkeit, \ac{GHI} und Niederschlag).
Sie werden als \lstinline[columns=fixed]{time_varying_known_reals} gesetzt, nachdem sie durch Normalisierung mithilfe des \lstinline[columns=fixed]{StandardScaler} vom Package \glqq sktlearn\grqq auf eine ähnliche Werteskala gebracht wurden.
Gibt es für diese Art von Werten Variablen, die im Voraus nicht bekannt sind, werden sie als \lstinline[columns=fixed]{time_varying_unknown_reals} gesetzt.
Welche Variable zu dieser Kategorie auf jeden Fall dazu gehört ist die die Zielvariable.
Es wird außerdem eine Normalisierung anhand der Gruppen durch die Softplus-Funktion durchgeführt, um positive Ausgabewerte zu garantieren.
Durch die Angabe sogenannter \lstinline[columns=fixed]{lags} wird die Eingabe um vergangene Werte der Zielvariablen, verschoben um den angegebenen Zeitraum, erweitert.
Ausgehend von der Saisonalität der Daten werden hier die Werte verschoben um einen Tag und um eine Woche verwendet.
Zuletzt werden dem Modell zusätzliche Merkmale und Kontext hinzugefügt, wie ein relativer Zeitindex um das Verhältnis jeden Datenpunkts zum Vorhersagepunkt zu erfassen, das Ausmaß der Zielvariable in Form von Mittelwert und Standardabweichung, und die Größe des Vorhersagefensters.
Außerdem wird festgelegt, dass fehlende Zeitschritte geduldet werden, obwohl der Datensatz zuvor auf fehlende Werte untersucht wurde.
Die Datensätze zur Validierung und zum Testen werden dann ausgehend vom Trainingsdatensatz erstellt, wobei für diese angegeben wird, dass eine Vorhersage ausgeführt werden soll (\lstinline[columns=fixed]{predict=True})~\cite{Labiadh.2023}~\cite{GitHub.20240307T20:56:16.000Z}.

Zunächst soll der \ac{MAE} des Baseline-Modells errechnet werden, um eine Ausgangslage zu haben~\cite{PytorchForecastingDocumentation.20230410T20:05:46.000Zb}.
Dieses verwendet den letzten bekannten Wert der Zielvariable, um eine Vorhersage zu machen.
Der erzielte Wert für den \ac{MAE} liegt bei rund 32,4.

Zum Vorschlagen einer passenden Learning-Rate wird der Tuner von pytorch lightning verwendet.
Dieser schlägt den geeignetsten Wert für die Learning-Rate unter Angabe des \ac{TFT}-Modells, der Trainings- und Validierungsdaten und eines Mindest- und Höchstwerts, vor.
Für die optimierte Einstellung weiterer Parameter des Modells wird das Hyperparamter-Tuning von pytorch forecasting getestet~\cite{PytorchForecastingDocumentation.20230410T20:05:46.000Z}.
Dafür werden die Trainings- und Validierungsdaten benötigt und Werteräume für die zu analysierenden Parameter definiert.
Die am besten funktionierenden Parameter sind laut erstellter Studie folgende:
\lstinline[columns=fixed]{gradient_clip_val: 0.7704201821245052, hidden_size: 101, dropout: 0.16435100042063222, hidden_continuous_size: 14, attention_head_size: 2, learning_rate: 0.00560374967}.


\section{Evaluierung und Vergleich der Modelle}
%!
%!
\chapter{Anwendung der Prognose}
\section{Anwendungsmöglichkeiten}
Die Analyse der \ac{MOER} hat eine deutliche tägliche Saisonalität der Daten gezeigt (siehe Kapitel~\ref{CAP:data-preparation-analysis}).
Für Deutschland zeigen die Daten täglich folgendes Muster auf:
Zu Beginn des Tages ist die \ac{MOER} relativ hoch, bevor sie zuerst langsam, dann schneller absinkt.
Kurz vor dem Mittag erreicht sie ihren Tiefstwert und steigt danach erneut stark an bis zum Höchstwert kurz vor Mitternacht.
Das Wissen über diese Muster in den Daten kann für die in Kapitel~\ref{CAP:strategies} beschriebenen Strategien angewandt werden.
Demand Shifting nach Zeit ist dafür der geeignete Prozess.

Ein praktisches Beispiel für die Anwendung von Demand Shifting ist die Optimierung von Cron Jobs.
Ein Cron Job dient dazu, Jobs gemäß einem wiederkehrenden Zeitplan zu erstellen.
Regelmäßig anfallende Aufgaben, wie das Erstellen von Backups oder das Generieren von Berichten, können dadurch automatisiert zu bestimmten Zeiten abgearbeitet werden.
Durch die Definition eines Zeitplans kann ein Job in festgelegten Intervallen ausgeführt werden~\cite{TheKubernetesAuthors.20240119T14:53:20+01:00}.
Diesen Zeitplan nach der Kohlenstoffintensität des Stromnetzes auszurichten, kann eine Reduzierung der Emissionen begünstigen.

Eine weitere Möglichkeit besteht darin, Auto Scaling Tools von Cloud-Plattformen auf die Kohlenstoffintensität auszurichten.
Auto-Scaling zielt eigentlich auf eine automatische Skalierung der Anwendungsressourcen je nach Lastnachfrage ab.
Auf diese Weise soll sichergestellt werden, dass zwar immer ausreichend viele Instanzen verfügbar sind, aber nicht mehr als nötig.
Die Optimierung ist dabei auf eine Reduzierung der Kosten ausgelegt, denn im Vordergrund steht, dass bei geringer Nachfrage die Kapazitäten reduziert werden, um Kosten zu sparen.
Zudem können zum Beispiel bei der Auto-Scaling Funktion von Amazon EC2-Instanzen mehrere Regionen für die Verteilung der Ressourcen angegeben werden, um eine hohe Ausfallsicherheit zu garantieren~\cite{AmazonWebServices.20240229}.

ist ein Umdenken erforderlich.
Ziel ist es nicht, die Kapazität auf die Nachfrage anzupassen, sondern die Kapazität auf die Kohlenstoffintensität.
Das hat Auswirkungen auf die Nachfrage, weil diese durch eine eingeschränkte Kapazität möglicherweise nicht mehr gedeckt werden kann.
Es gilt also, teils Kompromisse bei der Nachfrage zu machen, um eine Skalierung entlang der Kohlenstoffintensität zu ermöglichen.

Bei Amazon könnte die Umsetzung durch Einführung der Kohlenstoffintensität als benutzerdefinierte Metrik für das Auto-Scaling umgesetzt werden.
Standardmäßig wird Amazon CloudWatch, ein Tool zur Beobachtung bestimmter Metriken einer Instanz, mit Metriken wie z.B. der CPU-Auslastung zur Überwachung der Last und entsprechender Ressourcenanpassung verwendet.
Für CloudWatch können aber auch eigene Metriken konfiguriert werden.
Auf diese Weise könnten für die prognostizierte Kohlenstoffintensität bestimmte Schwellwerte festgelegt werden, anhand dieser der Auto-Scaler die Anzahl der Instanzen erhöht oder verringert.
Die Funktionsweise von CloudWatch, inklusive Bestandteile und dem Ablauf, ist in Abbildung~\ref{FIG:aws-cloudwatch} dargestellt.
\begin{figure}
 \caption{Die Funktionsweise von AWS Cloudwatch zur automatischen Skalierung von Cloud-Instanzen anhand spezifischer Metriken~\cite{amazonwebservices}}
 {\includegraphics[width=0.8\textwidth]{\figdir/aws-cloudwatch}}
 \label{FIG:aws-cloudwatch}
\end{figure}
AWS-Ressourcen, z.B. EC2-Instanzen, generieren und senden Metriken an CloudWatch.
CloudWatch verwaltet die standard oder benutzerdefinierten Metriken und generiert Statistiken auf Basis dieser Daten.
Der CloudWatch Alarm wird ausgelöst, wenn ein bestimmtes Ereignis eintritt, wie dass ein Messwert einen definierten Schwellwert über- oder untertrifft.
Auf Basis dieses Alarms können spezifische Aktionen ausgeführt werden, wie das Senden einer Benachrichtigung per E-Mail oder vor allem die automatische Skalierung der Ressourcen.
Beispielsweise könnte ein niedriger \ac{MOER}-Wert das Scaling-Out bewirken, um vom grünen Strom zu profitieren.
Die AWS Management Konsole dient zur Beobachtung und Analyse der Metriken und Statistiken, sowie dem Konfigurieren des Alarms.
Außerdem können die von CloudWatch generierten Statistiken weiter verarbeitet werden durch einen Statistics Consumer.
Das Hinzufügen einer eigenen Metrik zu CloudWatch erfolgt durch die Angabe verschiedener Parameter.
Code-Ausschnitt~\ref{CODE:aws-cloudwatch} zeigt, wie ein Befehl in der Kommandozeile von AWS zur Definition eines vorhergesagten \ac{MOER}-Werts aussehen könnte.
\lstinputlisting[language=sh, caption=Kommandozeilenbefehl zur Definition eines prognostizierten \ac{MOER}-Werts für CloudWatch, label=CODE:aws-cloudwatch]{\codedir/aws-cloudwatch_put-custom-metric.m}
Angegeben wird der Name der benutzerdefinierten Metrik, gefolgt vom Name der Metrik-Sammlung.
\lstinline[columns=fixed]{--value} ist der prognostizierte Wert, der für einen bestimmten Zeitpunkt und eine bestimmte Region angegeben wird~\cite{amazonwebservices}.

Ein mögliches Umsetzungsfeld ist Cron Jobs nach diesen Zeiten auszurichten.
Cron Jobs führen Jobs zu bestimmten geplanten Zeiten aus.
Gängige Cloud-Plattformen oder das Orchestrierungs Kubernetes bieten dafür Lösungen an.
Eine Möglichkeit, dies umzusetzen, ist mit dem Cloud Scheduler von Google Cloud.
Mit diesem können Cronjobs erstellt werden, die Jobs regelmäßig zu bestimmten Zeiten ausführen.
Auf diese Weise kann die Ausführung regelmäßig anstehender Aufgaben, wie zum Beispiel Batch-Jobs, durch einen Zeitplan konfiguriert werden
\section{API zur Abfrage der Prognose}
Die Prognose soll dazu dienen, für einen fiktiven Workload die Startzeit auszuwählen, bei der die Summe der \ac{MOER}-Werte über die Dauer des Workloads am geringsten sind.
Der Ansatz entspricht dem in Kapitel~\ref{CAP:dynamic-scheduling-models} beschriebenen Scheduling-Verfahrens \glqq Flexibler Start \grqq{}.
Dazu sollen die prognostizierten Werte über eine \ac{API} abgefragt werden können.
Um dies mit so wenig Overhead wie möglich zu machen, wird Flask, ein Mikro-Frontend zur einfachen Erstellung von Webanwendungen und ~\ac{API}s mit Python, verwendet~\cite{.20240203T21:13:11.000Z}.
Zur Konfiguration der Abfrage sollen folgende Parameter angegeben werden: Welches Prognose-Modell verwendet werden soll (SARIMAX oder \ac{TFT}, default ist \ac{TFT}), die geschätzte Dauer des Workloads in Minuten, ein Fälligkeitsdatum und für welche Länder die \ac{MOER}-Wert abgefragt werden sollen.
Die Erstellung des Prognose-Endpunkts mit Parametern ist in Code-Ausschnitt~\ref{CODE:prediction_endpoint} zu sehen.
\lstinputlisting[language=Python, caption=Python-Code zur Erstellung der Prognose API mit Flask, label=CODE:prediction_endpoint]{\codedir/flask_prediction_endpoint.m}
\section{Grenzen}
%Für welche Architektur geeignet
Echtzeit

Es gilt zu beachten, dass die analysierten Optimierungen ihre Grenzen haben.
Es handelt sich nicht um allgemein gültige, immer anwendbare Lösungen, sondern vielmehr um Vorschläge oder Richtlinien, die unter bestimmten Umständen durchaus positive Auswirkungen haben können.
Eine wichtige Einschränkung kann ein mögliches Fälligkeitsdatum oder eine hohe Dringlichkeit sein~\cite{Dodge.06212022}.
Ist bekannt, dass eine Aufgabe zu einem bestimmten Zeitpunkt angefangen oder sogar bereits abgeschlossen sein muss, hat es keinen Nutzen, wenn der nächste kohlenstoffarme Zeitraum nach diesem Fälligkeitsdatum liegt.
Allgemein können die zeitlichen Verschiebungen zu Verspätungen führen und diese können wiederum einen Anstieg von Emissionen durch andere Teile des Projekts verursachen.
Es ist deshalb stets wichtig, die Vorteile der eingesetzten Maßnahmen gegenüber ihrem Aufwand abzuwägen~\cite{Dodge.06212022}.

Die örtliche Verlagerung ist nicht immer sinnvoll.
Bei Workloads, die eine große Datenmenge erfordern überwiegt der Energieaufwand für die Verlagerung der Daten in ein anderes Rechenzentrum möglicherweise den Einsparungen durch dieses umweltfreundlichere Rechenzentrum~\cite{Norlander}.

Mit größerer Vorhersagelänge steigen auch die Fehleranfälligkeit der Vorhersagen.
Sowohl eine fehlerhafte Vorhersage des Wetters, als auch Fehler der Zeitreihenvorhersage selbst sind mögliche Fehlerquellen~\cite{Wiesner.2021}.
%!
%!
\chapter{Ergebnisse und Diskussion}
Eine große Herausforderung besteht darin, das Thema Nachhaltigkeit in der \ac{IT} anzubringen.
Lange wurde diesem keine Aufmerksamkeit geschenkt und Optimierungen fanden hauptsächlich für anfallende Kosten statt.
Wenn der Umweltaspekt nicht Motivation genug ist, können noch zwei weitere Aspekte hilfreich sein.
Zum einen gehen mit der Optimierung des Energieverbrauchs und der Umstellung auf grüne Energiequellen oft verminderte Stromkosten einher, was als Anreiz dienen kann.
Zum anderen werden immer mehr Vorschriften und Berichtsanforderungen in Bezug auf verursachte Emissionen eingeführt~\cite{GreenSoftwareFoundation.2023}.
Dies sind weitere Gründe, sich mit der Energienutzung zu beschäftigen.

Der primäre Antrieb für den Übergang zu erneuerbaren Energien mag ökonomischer, nicht ökologischer Natur sein

Inwiefern ist die Vorhersage für Länder sinnvoll auf die Regionen der Datencenter übertragbar?
%!
%!
\chapter{Fazit und Ausblick}
Diese Arbeit untersucht das Potenzial der Nachhaltigkeitsoptimierung von Software anhand der Zeitreihenvorhersage der marginalen Kohlenstoffintensität zwei spezifischer Stromnetze.
Neben der Vorhersage durch ein statistisches Modell wurde mit dem \ac{TFT} auch ein Neuronales Netz trainiert und die Ergebnisse der beiden Modelle verglichen.
Für die Optimierungsmöglichkeiten wurden unterschiedliche Strategien untersucht und die Anwendbarkeit der zeitlichen Verschiebung für verschiedene Workloads bewertet.
Die Ergebnisse zeigen sowohl für Deutschland, als auch für Norwegen, ein sich täglich wiederholendes Muster im Verlauf der marginalen Kohlenstoffintensität, wobei sich die Muster je Land geringfügig unterscheiden.
Die Minimalwerte zur Mittagszeit können für eine generelle Verschiebung in diese Zeiten genutzt werden.
Darüber hinaus kann die Vorhersage der Intensität zwei Wochen im Voraus jederzeit unter Angabe von Dauer und Fälligkeitsdatum des betrachteten Workloads abgerufen werden.

Cloud- und Service-Anbieter sollten Nutzer ermutigen, ihre Workloads möglichst zeitlich flexibel und unterbrechbar zu gestalten und sie dementsprechend zu deklarieren.
Spot-Instanzen sind bereits bei vielen Anbietern verfügbar, sodass die Cloud besser ausgelastet und zugleich niedrige Kosten angeboten werden können.
Nicht zuletzt zwingt die Bepreisung von Kohlendioxid, die laut World Bank 2023 ein Rekordhoch erreicht hat~\cite{WorldBank.2023}, Anbieter und Nutzer dazu, sich mit diesem Thema zu beschäftigen.
Neben finanziellen Anreizen, sollten Anbieter vermehrt Wissen über Kohlenstoffintensität und die adäquate Gestaltung von Workloads zur Verfügung stellen.
Eine einfache Änderung wäre die Angabe von Zeitfenstern (z.B. nachts) anstelle konkreter Zeitpunkte (z.B. um 1 Uhr) für Workloads, sodass das Potenzial für Kohlenstoffeinsparungen genutzt werden kann~\cite{Wiesner.2021}.

Qualitativ hochwertige Prognosen sowohl der Kohlenstoffintensität als auch von Workloads stehen im Mittelpunkt erfolgreicher kohlenstoffbewusster Verlagerungen.
Wichtig sind dabei vor allem auch Informationen über die Workloads, wie zeitliche Beschränkungen, erwartete Dauer und, ob eine Unterbrechung möglich ist.
Zur Unterstützung könnten Tools zur Bereitstellung dieser Informationen eingesetzt werden.
So könnten Schnittstellen bereitgestellt werden, um zeitliche Beschränkungen und andere Eigenschaften programmatisch zu deklarieren oder diese automatisch zu erkennen.
Die benötigte Zeit für das Anhalten und Wiederaufnehmen eines Workloads könnte erfasst und dieser anhand dessen automatisch als unterbrechbar oder nicht unterbrechbar gekennzeichnet werden
Zeitliche Beschränkungen könnten zum Beispiel vom Abhängigkeitsgraphen eines Workloads abgeleitet werden~\cite{Wiesner.2021}.
%Geplante Skalierung
%Dynamische Skalierung
%Dynamische Ressourcenanpassung
%Optimierung des Datenzugriffs


