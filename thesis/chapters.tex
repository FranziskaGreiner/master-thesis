%! EINLEITUNG
%!
\chapter{Einleitung}

\section{Motivation}
Der Klimawandel stellt eine der größten Herausforderungen unserer Zeit dar.
Seine Realität und die dringende Notwendigkeit, seine Hauptverursacher und Treibfaktoren zu bremsen, sind längst unbestreitbar.
Dies wird unter anderem durch den im März 2023 veröffentlichten Bericht des \ac{IPCC} deutlich bestätigt.
Der Bericht zeigt auf, dass die globale Oberflächentemperatur im Zeitraum von 2011 bis 2020 um 1,09 Grad Celsius höher lag als im Zeitraum von 1850 bis 1900 und identifiziert die nicht nachhaltige Energienutzung als einen der Haupttreiber dieser Veränderungen~\cite{IPCC.2023}.
Auch die neuesten Zahlen des EU-Erdbeobachtungsdienstes Copernicus bekräftigen den Trend.
Dieser gab bekannt, dass das Jahr 2023 das wärmste Jahr seit Beginn der globalen Temperaturaufzeichnungen im Jahr 1850 war~\cite{CopernicusClimateChangeService.09.01.2024}.
Die neuesten Veröffentlichungen des Beobachtungsdienstes von Anfang Februar diesen Jahres beschreiben außerdem, dass die 1,5-Grad-Marke des Pariser Abkommens in den vergangenen zwölf Monaten erstmals gebrochen wurde.
Die globale Durchschnittstemperatur der Monate Februar 2023 bis Januar 2024 lag 1,52 Grad über dem vorindustriellen Niveau~\cite{Eichhorn.8.2.2024}.
%!Gleichzeitig war es lange Zeit üblich, quasi unendlich Rechenkapazität etc. zur Verfügung zu haben und auch zu nutzen

%In diesem Zusammenhang stellt sich natürlich die Frage nach der Ursache für diese Entwicklung.
In diesem Kontext spielt die Technologiebranche eine zweischneidige Rolle.
Der rasante technologische Fortschritt hat zu einem exponentiellen Wachstum der Rechenkapazitäten geführt.
%Besonders im Bereich der Softwareentwicklung ist ein stetiges Wachstum von Technologien und Anwendungen zu verzeichnen.
%!Kommentar Thomas: Übergang zu Technologiebranche zu groß. Technologie kann alles mögliche sein.
Einerseits geht dieses Wachstum mit einem erheblichen Anstieg des Energieverbrauchs einher.
Schätzungen zufolge trägt die Technologiebranche mit mehr als 10\% zu den jährlichen globalen Kohlenstoffemissionen bei~\cite{Buchanan.2023}.
Andererseits steckt in dieser Branche auch das Potenzial, erheblich zur Verringerung der Auswirkungen beizutragen.

Angesichts dieser Entwicklungen hat die Bedeutung nachhaltiger Technologien und Praktiken signifikant zugenommen.
Es ist von großer Wichtigkeit, dass die \ac{IT}-Branche ihre Verantwortung wahrnimmt und aktiv dazu beiträgt, den Klimawandel durch innovative und nachhaltige Lösungen zu bekämpfen.
Die Entwicklung und Implementierung von energieeffizienten und kohlenstoffarmen Technologien und Methoden in der Softwareentwicklung ist daher nicht nur eine technologische Herausforderung, sondern auch eine moralische Verpflichtung gegenüber zukünftigen Generationen.

Zwar werden die Stromnetze weltweit auf kohlenstoffärmere Quellen umgestellt, doch dieser Wandel kann nicht unmittelbar geschehen, sondern benötigt Zeit und ist nur möglich, wenn die Stromnachfrage trotz der Umstrukturierung nach wie vor gedeckt werden kann.
Um dieser Schwierigkeit Abhilfe zu schaffen, soll betrachtet werden, wie der Übergang beschleunigt werden kann und fossile Brennstoffe verzichtbarer gemacht werden können, indem die Nutzung erneuerbarer Energiequellen priorisiert wird~\cite{GreenSoftwareFoundation.2022}.
%effizienter Code oft sowohl grüner als auch schneller ist als ineffizienter Code
%aber: keine Selbstverständlichkeit, "grün", "effizient" und "performant" sind keine Synonyme
%offensichtlichste Zweck der Code-Effizienz: Anzahl und Größe der Maschinen zu reduzieren, die für das Hosting Ihrer Dienste erforderlich sind --> Anzahl der Nutzer, Zuverlässigkeit und Leistungsniveau; = Maximierung der Hardwareproduktivität, sehr umweltfreundliches Konzept
%weniger Server --> weniger Strom für den Betrieb --> das System enthält weniger Kohlenstoff (jede Maschine enthält Kohlenstoff, der bei ihrer Herstellung und Entsorgung ausgestoßen wird)
%früher nicht viel Hardwarekapazität (Maschinen waren langsam, Anschaffung teuer, Unterbringung in Rechenzentren kostspielig) --> Verwendung von hocheffizienten Sprachen wie C, sodass ausführbare Dateien klein und minimale Anzahl der CPU-Zyklen pro Vorgang; Services haben nicht zu viele Nachrichten mit anderen Maschinen ausgetauscht, Daten auf der Festplatte wurden nicht ständig abgefragt
%Warum nicht einfach dahin zurückkehren?
%Maschinen und Netzwerke verdoppelten ihre Kapazität alle achtzehn Monate (Moore´s law), sind heute mindestens drei Größenordnungen schneller als in den 90er Jahren
%Dienste von Drittanbietern wurden innovativer und wertvoller
%Nutzerzahlen stiegen, erwarteten mehr Funktionen und schnellere Entwicklung
%Sicherheitsbedrohungen wurden immer beängstigender und häufiger
%Hauptziel Maschinenproduktivität hat sich in letzten zwei Jahrzenten hin zur Entwicklerproduktivität geändert
%!siehe "Nutzen Code-Effizienz" von Building Green Software

%Der Klimawandel ist eine der größten Herausforderungen der heutigen Zeit.
%Längst ist er nicht mehr abzustreiten und macht es notwendig, die Auslöser und Beschleuniger zu bremsen.
%Der vom Menschen gemachte Klimawandel ist nicht mehr abzustreiten.
%%Es wurde mehrfach bewiesen, dass der Klimawandel Realität ist.
%Er wurde mehrfach bewiesen, zuletzt zum Beispiel durch den im März 2023 veröffentlichten Bericht des IPCC (Intergovernmental Panel on Climate Change).
%Dieser hält fest, dass die globale Oberflächentemperatur 2011 bis 2020 um 1,09 Grad Celsius wärmer war, als in den Jahren 1850 bis 1900 und nennt als einen wichtigen Auslöser eine nicht nachhaltige Energienutzung~\cite{IPCC.2023}.
%
%Die Technologiebranche spielt eine wichtige Rolle beim Klimawandel.
%Der technologische Fortschritt hat einen scheinbar unendlichen Zugang zu Rechenleistung verschafft.
%Im Bereich der Softwareentwicklung wird seit längerer Zeit eine rasche Entwicklung von Technologien verzeichnet.
%Damit einher kommt ein steigender Energieverbrauch~\cite{Buchanan.2023}.
%Schätzungsweise beläuft sich der Anteil der Technologiebranche an den jährlichen Kohlenstoffemissionen auf mehr als 10\%.
%Angesichts dessen hat der Entwurf von nachhaltigen Technologien und Praktiken an Wichtigkeit gewonnen.
%Zustande kommen diese Emissionen hauptsächlich durch zwei Faktoren:
%Zum einen durch die Herstellung von Benutzergeräten für Anwendungen, sogenannter verkörperter (englisch embodied) Kohlenstoff, und zum anderen durch den Betrieb der Anwendungen in Rechenzentren~\cite{Currie.2024}.

%Das rasche Wachstum von Softwaretechnologien und die steigende Popularität von Cloud-Diensten haben einen scheinbar unendlichen Zugang zu Rechenleistung verschafft.
%Damit einher kommt ein steigender Energieverbrauch

\section{Forschungsziele}\label{CAP:goals}
Das primäre Ziel dieser Arbeit liegt darin, die Nachhaltigkeit von Softwareanwendungen durch die Entwicklung eines Prognosemodells zur Vorhersage der Kohlenstoffintensität zu verbessern.
Insbesondere soll untersucht werden, wie die Vorhersagen genutzt werden können, um den Aufbau der Softwarearchitektur und den Betrieb der Anwendung möglichst nachhaltig zu gestalten.
Die Arbeit wird durch folgende spezifische Forschungsfragen definiert:

\begin{enumerate}
 \item \textbf{Entwicklung eines Vorhersagemodells}: Wie kann ein effektives Modell zur Vorhersage der Kohlenstoffintensität des Stromnetzes entwickelt werden?
 Ist ein statistisches Modell dafür ausreichend oder erzielt ein auf \ac{KI} basierendes Modell bessere Ergebnisse?
 Welche Rolle spielen dabei verschiedene Eingabedaten, wie zum Beispiel bestimmte Wetterdaten?
 \item \textbf{Nachhaltigkeitsoptimierung von Softwareanwendungen}: Welche Strategien zur Nachhaltigkeitsoptimierung von Software gibt es?
 Inwiefern können die Vorhersagen der Kohlenstoffintensität dazu verwendet werden, um die Architektur und den Betrieb von Software nachhaltiger zu gestalten?
 % \item Simulation und Bewertung: Wie kann die Wirksamkeit der entwickelten Strategien durch Simulationen getestet und bewertet werden?
 \item \textbf{Beitrag zu nachhaltigen Softwaresystemen}: Wie trägt diese Arbeit zur Förderung der Nachhaltigkeit von Softwarelösungen bei?
 Welche praktischen Erkenntnisse und Empfehlungen können abgeleitet werden?
 Stehen die gewonnenen Verbesserungen der Nachhaltigkeit im Verhältnis zum Aufwand der Implementierung?
\end{enumerate}
%Das Ziel dieser Arbeit ist die Entwicklung eines KI-basierten Prognosemodells, das mithilfe von Zeitreihenanalysen die Kohlenstoffintensität des Stromnetzes von Ländern in Europa vorhersagt.
%Durch die Integration dieses Modells in eine \ac{API} soll eine Schnittstelle geschaffen werden, die es Softwareanwendungen ermöglicht, ihre Betriebszeiten an die Kohlenstoffintensität des Stromnetzes anzupassen.
%Die zentralen Forschungsfragen umfassen:
%
%Wie kann ein KI-basiertes Prognosemodell effektiv die Kohlenstoffintensität des Stromnetzes vorhersagen?
%Wie können die Vorhersagen genutzt werden, um nachhaltige Scheduling-Strategien in Softwarearchitekturen zu implementieren?
%Ist die dadurch gewonnene Verbesserung der Nachhaltigkeit von Softwareanwendungen lohnenswert?
\section{Struktur der Arbeit}
Die vorliegende Arbeit verfolgt eine bestimmte Struktur, die im folgenden Kapitel erklärt werden soll.
Dies ermöglicht einen vereinfachten Überblick über den Inhalt und den Aufbau.
Gegliedert ist die Arbeit in sieben Hauptkapitel, was zu einer klaren Strukturierung der Forschungsinhalte und -ergebnisse dient.

Dieses erste Kapitel erklärt die Motivation, die Ziele, die Abgrenzung und die Einordnung im Hinblick auf vorhandene Forschungen

Nach der Einleitung werden in Kapitel~\ref{CAP:sustainable-software-basics} die theoretischen Grundlagen nachhaltiger Softwarearchitektur und die Relevanz der Kohlenstoffintensität als Maßstab für Nachhaltigkeit dargelegt.

Kapitel~\ref{CAP:strategies} widmet sich verschiedenen Strategien zur Optimierung der Nachhaltigkeit von Softwaresystemen.
Hier werden mögliche Ansätze wie Scheduling, Scaling, Demand Shifting und Demand Shaping erörtert, sowie die Voraussetzungen, die für die Anwendung dieser Strategien erforderlich sind.

Kapitel~\ref{CAP:tsf_basics} bietet eine Einführung in die Grundlagen der Zeitreihenprognose, gefolgt von detaillierten Beschreibungen des SARIMAX-Modells sowie des Temporal Fusion Transformers als fortschrittliches KI-basiertes Modell.
Es dient als Grundstein für das darauffolgende Kapitel.

Kapitel~\ref{CAP:prediction} fokussiert sich auf die Prognose der Kohlenstoffintensität.
Hierbei wird zunächst auf die Auswahl und Vorverarbeitung der Daten eingegangen, gefolgt von einer tiefgreifenden Analyse der Daten.
Die zuvor theoretisch erklärten Verfahren zur Zeitreihenprognose finden nun ihre praktische Anwendung.
Abgeschlossen wird das Kapitel durch eine umfangreiche Evaluierung der Prognose und einem Vergleich zwischen den beiden Ansätzen.

Anwendungsmöglichkeiten der in Kapitel~\ref{CAP:prediction} entwickelten Prognosemodelle werden in Kapitel~\ref{CAP:prediction-application} aufgezeigt.
Es werden konkrete Anwendungsszenarien, verfügbare Tools, eine API zur Abfrage der Prognose sowie die damit verbundenen Herausforderungen und Grenzen behandelt.

Die Ergebnisse der gesamten Arbeit sowie eine umfassende Diskussion werden in Kapitel~\ref{CAP:results} präsentiert.

Das abschließende Kapitel~\ref{CAP:resumee} bietet ein Resümee der gesamten Arbeit und einen Ausblick auf zukünftige Forschungsmöglichkeiten in diesem Bereich.
Es fasst die wichtigsten Erkenntnisse zusammen und beschreibt potenzielle Entwicklungen der Untersuchungen.

\section{Themenabgrenzung}
Im Folgenden werden gezielte Abgrenzungen vorgenommen, um den Umfang und Fokus der Arbeit zu definieren und gleichzeitig relevante, jedoch nicht zentrale Themen auszugrenzen.
Diese Abgrenzungen schaffen einen klaren und strukturierten Rahmen für die Forschung und legen den Grundstein für mögliche zukünftige Arbeiten.

Nachhaltigkeit ist ein vielschichtiges Konzept, jedoch liegt der Schwerpunkt dieser Arbeit auf der ökologischen Dimension, insbesondere auf der Reduktion der Kohlenstoffintensität.
Soziale und ökonomische Nachhaltigkeitsaspekte werden nur am Rande behandelt.

Neben den Einflüssen der Software auf die Umwelt ist die Hardware ein wichtiger Faktor.
Bei ihrer Herstellung und Entsorgung freigesetzte, sogenannte verkörperte, Emissionen können einen großen Anteil ausmachen.
%!Kommentar Thomas: Software hat auch Einfluss auf verkörperte Emissionen, weil effizienter die Software ist und je weniger Hardware sie benötigt, desto weniger verkörptere Emissionen, weil Hardware wird besser genutzt.
Dieser Aspekt und daraus resultierenden Maßnahmen sind zwar signifikant, werden jedoch in dieser Arbeit nicht direkt behandelt, sondern bieten einen Ansatzpunkt für weiterführende Forschung.
Indirekt werden jedoch auch die verkörperten Emissionen thematisiert, denn Maßnahmen wie der effiziente Betrieb von Software gehen beispielsweise oft mit einer Reduzierung der verkörperten Emissionen einher.
% Indirekt sind die jedoch auch die verkörperten Emissionen betroffen, denn je effizienter die Software gestaltet ist, desto weniger Hardware benötigt sie zur Ausführung und desto weniger verkörperte Emissionen sind damit verbunden.

Die Software betreffend werden mögliche Bereiche, die hinsichtlich Nachhaltigkeit betrachtet werden können, eingeschränkt.
Nicht Teil der Arbeit sind zum Beispiel Maßnahmen auf Code-Ebene, wie die Verwendung nachhaltiger Programmiersprachen und Frameworks, effiziente Datenverwaltung und -speicherung oder nachhaltig gestaltete Benutzeroberflächen.
Der Fokus liegt allein auf Nachhaltigkeits-Verbesserungen durch die Nutzung kohlestoffarmen Stroms.
%Wichtig ist auch die Betrachtung von Hardwareeinflüssen, wie den bei Herstellung und Entsorgung freigesetzten verkörperten Kohlenstoffemissionen.
%Diese sind zwar signifikant, werden jedoch in dieser Arbeit nicht behandelt, bieten jedoch interessante Ansatzpunkte für weiterführende Forschung.

%Obwohl Nachhaltigkeit ein vielschichtiges Konzept ist, fokussiert sich diese Arbeit primär auf die ökologische Dimension der Nachhaltigkeit, insbesondere auf die Reduktion der Kohlenstoffintensität.
%Soziale und ökonomische Aspekte der Nachhaltigkeit werden nur am Rande behandelt.

Des Weiteren ist sowohl der technologische, als auch der geografische Rahmen des Vorhersagemodells begrenzt.
Die Umsetzung erfolgt durch ein \ac{SARIMAX}- und ein \ac{TFT}-Modell.
Weitere möglicherweise passende Ansätze zur Umsetzung werden nicht untersucht.
Die Analyse beschränkt sich auf die Stromnetze Deutschlands und Norwegens.
Diese beiden Länder eignen sich gut für den Vergleich, wie in Kapitel~\ref{CAP:data-preparation-analysis} erklärt wird.
Außerdem würde ein größerer Umfang eine enorm größer Datenmenge bedeuten, was dem Rahmen dieser Arbeit nicht gerecht werden würde.

\section{Verwandte Arbeiten}
Nachfolgend soll der Stand der Forschung betrachtet und verwandte Arbeiten identifiziert werden, um die Relevanz und den Beitrag dieser Arbeit aufzuzeigen.

Zur Vorhersage der Kohlenstoffintensität gibt es einige Arbeiten mit unterschiedlichem Umfang.
Electricity Maps und WattTime sind die wohl am häufigsten verwendeten und gängigsten Anbieter für Daten zur Kohlenstoffintensität.
Sie verfügen beide nicht nur über ein umfangreiches Datenportfolio historischer Daten und Übersichten, sondern auch über Vorhersagen.
Electricity Maps bietet eine \ac{API} an, über die verschiedene Daten wie u.a. die Kohlenstoffintensität oder die Zusammensetzung des Strommixes abgefragt werden können.
Die Abdeckung umfasst über 160 Regionen weltweit und es sind historische, aktuelle und zukünftige (24 Stunden im Voraus) Werte verfügbar~\cite{ElectricityMaps.20231220T09:16:49.000Z}.
WattTime stellt ebenfalls eine \ac{API} zur Verfügung.
Über diese kann z.B.\ die marginale und die durchschnittliche Kohlenstoffintensität in der Vergangenheit, in Echtzeit oder 24 Stunden im Voraus abgefragt werden.
Die Daten sind in einem Intervall von fünf Minuten erhältlich~\cite{WattTime.20231130T19:28:06+00:00}.

Längerfristige Vorhersagen bietet nach jetzigem Stand lediglich die Carbon Intensity \ac{API}~\cite{LyndonRuff.20220420T15:34:17.000Z}.
Diese \ac{API} ist ein Projekt aus und für Großbritannien.
Es ist entstanden aus einem Zusammenschluss zwischen dem nationalen Stromnetz, mehreren Nichtregierungsorganisationen und akademischen Einrichtungen, um die Kohlenstoffintensität für verschiedene Regionen Großbritanniens vorherzusagen.
Die Schnittstelle basiert auf maschinellem Lernen und ermöglicht Anwendern, ihren Stromverbrauch so zu planen, dass die Kohlenstoffemissionen auf regionaler Ebene minimiert werden~\cite{Currie.2024}.
Die Vorhersagen sind 96 Stunden im Voraus verfügbar.
% Dafür wurde der Berechnungsansatz, der im vorherigen Kapitel (Formel~\ref{eq:ci}) erklärt wurde, verwendet.
% Der Intensitätsfaktor der verschiedenen Energiequellen ist dabei spezifisch für die jeweilige Region oder das jeweilige Land.

Darüber hinaus existieren auch Arbeiten zur Anwendung der Vorhersagen.
Ein Projekt, dass sich konkret mit der Anwendung der Vorhersagen zur Kohlenstoffintensität befasst, wurde von der Green Software Foundation ins Leben gerufen.
Es bietet eine erleichterte Integration von Vorhersagen in bestehende Anwendungen.
Dafür wurde ein \ac{SDK} entwickelt, mit Hilfe dessen herausgefunden werden kann, wann und wo ein Workload mit der geringsten Menge an Kohlenstoff ausgeführt werden kann.
Carbon Aware \ac{SDK} kann als Web\ac{API} und als Command Line Interface verwendet werden und steht als Open-Source-Projekt zur Verfügung.
Es kann dabei helfen, die Ausführung nicht zeitkritischer Workloads auf Zeiten mit geringeren Emissionen zu verschieben und in andere Rechenzentren zu verlagern~\cite{GreenSoftwareFoundation.20231212T09:58:27.000Z}.
Angeboten werden unterschiedliche Abfragen, zum Beispiel die Ausgabe der besten Startzeit und des besten Ortes unter Angabe eines Zeitintervalls und der Workload-Dauer oder die Ausgabe der aktuellsten Vorhersagedaten inklusive Berechnung des Zeitfensters mit der geringsten \ac{MOER}~\cite{GreenSoftwareFoundation.20240316T16:54:58.000Z}.
Carbon Aware \ac{SDK} dient also dazu, Daten zur Kohlenstoffintensität für Software anwendbar zu machen.
Die vorliegende Arbeit grenzt sich dadurch ab, dass sie die Kohlenstoffintensität weiter in die Zukunft vorherzusagen, diese Vorhersagen verfügbar zu machen und deren Anwendungsmöglichkeiten aufzuzeigen.

Ein weiteres relevantes Projekt ist der Carbon Aware Scheduler~\cite{Siemers.20240319T14:09:25.000Z}.
Es handelt sich dabei um ein Command Line Tool für das Scheduling von Unix Jobs.
Basierend auf der Kohlenstoffintensität des Stromnetzes wird die beste Zeit für das Ausführen eines Jobs ausfindig gemacht.
Zum jetzigen Stand ist der Carbon Aware Scheduler jedoch nur für die Niederlande verfügbar.

Microsoft, UBS, WattTime und die Green Software Foundation haben zusammen ein Paper über kohlenstoffbewusste Software veröffentlicht~\cite{Buchanan.2023}.
Darin beschreiben sie, wie Kohlenstoffintensität gemessen und Software nach dieser ausgerichtet werden kann.
Sie verwenden das Carbon Aware \ac{SDK}, um einen Prototypen zur zeitlichen Verschiebung von Workloads zu entwerfen.
Das Projekt zeigt, wie das Tool im laufenden Betrieb eingesetzt werden kann.

Die örtliche Verlagerung von Workloads kann mit einem Kostenmehraufwand verbunden sein, der nicht nur die Verlagerung an sich sondern möglicherweise auch den höheren Preis der ausgewählten Region umfasst.
Diese zusätzlichen Kosten können von der Umsetzung der Verlagerung abhalten, weshalb Microsoft für Azure ein Werkzeug zusätzlich zur Carbon Aware \ac{SDK} entwickelt hat, das auf dieser aufbaut.
Die Carbon Economy \ac{SDK} errechnet für die vom Carbon Aware \ac{SDK} ausgegebene optimale Zeit und Region die Kosten, um einen Workload in diese andere Region zu verlagern.
Unter Verwendung einer normalisierten Gewichtung werden geringere Kohlenstoffemissionen gegenüber zusätzlichen Kosten priorisiert.
Anhand diesem Gewicht kann entschieden werden, wie viel einem die geringeren Emissionen Wert sind~\cite{Norlander.2023}.

Als bedeutend wurde vor allem das Paper von Wiesner et al. \cite{Wiesner.2021} identifiziert, welches das Potenzial zeitlicher Verlagerung von Workloads in Rechenzentren, um emissionsarme Energie zu verwenden, untersucht.
Die Autoren konzentrierten sich dabei insbesondere darauf, über welche Eigenschaften Workloads verfügen müssen, sodass sie gut für die Verschiebung geeignet sind und auf die Kohlenstoffintensität vom Stromnetz vier verschiedener Länder.
Sie kamen zu dem Ergebnis, dass das Verlagerungspotenzial in Ländern mit einem hohen Anteil von Solarenergie vor Sonnenaufgang und abends hoch ist und dass eine Verlagerung auf die Wochenenden zu Einsparungen von mehr als 20\% führen können.
Außerdem stellte es sich als besonders hilfreich heraus, wenn für Workloads die zeitlichen Beschränkungen gelockert und die Möglichkeit für Unterbrechungen genutzt werden~\cite{Wiesner.2021}.

Eine jüngst veröffentlichte Forschungsarbeit beschäftigt sich mit den Einschränkungen zeitlicher und örtlicher Verlagerungen von Workloads in der Cloud anhand der Kohlenstoffintensität~\cite{Sukprasert.2023}.
Sie zeigt, dass eine räumlich-zeitliche Verlagerung von Workloads deren Kohlenstoffemissionen zwar reduzieren kann, das Potenzial aber dennoch durch eine praktische Obergrenze reguliert und deshalb weit von Idealwerten entfernt ist.
Die größte Wirkung liegt dabei in einfachen Scheduling-Strategien.
Wenig überraschend ist die Erkenntnis, dass ein Wandel der Energieversorgung hin zu \glqq grüneren\grqq{} Stromquellen den Nutzen der kohlenstoffbewussten Planung senken~\cite{Sukprasert.2023}.
%!
%!
\chapter{Grundlagen nachhaltiger Software}\label{CAP:sustainable-software-basics}
Das folgende Kapitel erklärt die Grundlagen nachhaltiger Software und beschreibt, wie die Nachhaltigkeit gemessen werden kann.
Es bildet den Grundstein für die im nachfolgenden Kapitel behandelten Strategien und für das Verständnis, welchen Einfluss die Kohlenstoffintensität hat.
\section{Definition Nachhaltigkeit}
Das Ziel der vorliegenden Arbeit besteht darin, eine Möglichkeit zu finden, die Nachhaltigkeit von Software zu optimieren.
An dieser Stelle wird deshalb als Grundlage der Begriff der Nachhaltigkeit definiert und erklärt, welche Rolle Nachhaltigkeit in der Softwareentwicklung spielt.

Vom Bundesministerium für wirtschaftliche Zusammenarbeit und Entwicklung \cite{BundesministeriumWirtschaftlicheZusammenarbeitundEntwicklung} wird Nachhaltigkeit wie folgt definiert:
\glqq Nachhaltigkeit oder nachhaltige Entwicklung bedeutet, die Bedürfnisse der Gegenwart so zu befriedigen, dass die Möglichkeiten zukünftiger Generationen nicht eingeschränkt werden.\grqq{}
Dabei erstrecke sich Nachhaltigkeit gleichermaßen auf drei verschiedene Dimensionen, nämlich wirtschaftlich effizient, sozial gerecht und ökologisch tragfähig.
Laut Duden ist Nachhaltigkeit in Bezug auf Ökologie das \glqq Prinzip, nach dem nicht mehr verbraucht werden darf, als jeweils nachwachsen, sich regenerieren, künftig wieder bereitgestellt werden kann\grqq{}~\cite{Dudenredaktion.27.04.2018}.

Die im Jahr 1992 auf der Konferenz für Umwelt und Entwicklung der Vereinten Nationen verabschiedeten Agenda 21 legte Nachhaltigkeit als übergreifendes Ziel der Politik fest.
Außerdem wurden in der Agenda 2030, die 2015 von der Weltgemeinschaft beschlossen wurde, 17 Nachhaltigkeitsziele definiert.
Davon sieht Ziel zwölf Nachhaltigkeit in Produktion und Konsum vor.
Die deutsche Ausarbeitung dieses Ziels besagt, dass dazu unter anderem der Verzicht auf fossile Energieträger, wie Kohle, Gas und Öl und stattdessen die Nutzung von erneuerbaren Energien gehört~\cite{Bundesregierunginformiert}.

Treibhausgase wie \ac{CO2} kommen zwar natürlich in der Erdatmosphäre vor, um Wärme zurückzuhalten.
Jedoch sind sie durch menschliche und industrielle Aktivitäten im Überfluss vorhanden, was einen globalen Temperaturanstieg und Klimawandel zur Folge hat~\cite{Currie.2024}.
Eine Möglichkeit zur Erfassung und Berechnung von Treibhausgasemissionen bietet das \ac{GHG} Protokoll als Grundlage zur transparenten Kommunikation von Nachhaltigkeitsbestrebungen~\cite{WorldBusinessCouncilforSustainableDevelopment.2004}.
Dieses Protokoll wurde vom World Resource Institute und vom World Business Council for Sustainable Development zusammen mit Unternehmen entwickelt.
Es umfasst mehrere Standards, die Unternehmen und Organisationen eine Bilanz der Treibhausgas-Emissionen ermöglichen.
Dabei werden die Emissionen in drei verschiedene Bereiche unterteilt, siehe Tabelle~\ref{tab:GhgScopes}, die zusammen ein Gesamtbild der verursachten Emissionen ergeben.
\begin{table}[t]
 \centering\small
 \caption[GHG Protokoll Scopes]{Die drei verschiedenen Dimensionen von Emissionen laut dem GHG Protokoll~\cite{WorldBusinessCouncilforSustainableDevelopment.2004}}
 \label{tab:GhgScopes}
 \input{\tabledir/GhgScopes.tex}
\end{table}

\section{Nachhaltige Software}\label{CAP:sustainable-software}
Betrachtet man den Begriff der Nachhaltigkeit vor dem Aspekt nachhaltiger Softwaresysteme, so lassen sich die Prinzipien wie folgt zusammenfassen~\cite{Calero.2015}:
\begin{itemize}
 \item Überwachung, Messung, Bewertung, sowie Optimierung des Ressourcen- und Energieverbrauchs während Herstellung und Nutzung der Software
 %direkten (während Herstellung und Nutzung) und indirekten () Verbrauchs natürlicher Ressourcen, der durch den Einsatz und die Nutzung entsteht, bereits im Entwicklungsprozess;
 %wobei sich direkt auf den Verbrauch während der Herstellung und Nutzung bezieht und indirekt auf die Verwendung des Softwareprodukts zusammen mit anderen Prozessen und langfristigen systemischen Auswirkungen
\item Möglichkeit zur kontinuierlichen Auswertung und Optimierung des Einsatzes und der Nutzungsfolgen der Software
\item Zyklische Bewertung und Minimierung des Verbrauchs von natürlichen Ressourcen und Energie während der Entwicklungs- und Produktionsprozesse
\end{itemize}
Nachhaltige Software zeichnet sich also dadurch aus, dass die negativen Auswirkungen auf die drei Bereiche der Nachhaltigkeit (Wirtschaft, Gesellschaft und Umwelt) während Entwicklung, Einsatz und Nutzung der Software gering gehalten werden oder sich sogar positiv auf diese auswirken.

Die \ac{IT} spielt für das Thema Nachhaltigkeit eine wichtige Rolle und das sowohl als Teil des Problems, als auch als Teil der Lösung.
Diese Arbeit spricht von den drei genannten Ausprägungen der Nachhaltigkeit hauptsächlich die ökologische Tragfähigkeit an, die auch als \glqq grüne\grqq{} oder \glqq umweltfreundliche\grqq{} Dimension bezeichnet wird.
Betreffend dieser Dimension kann die \ac{IT} zwei verschiedene Rollen annehmen, bezeichnet als Green in \ac{IT} und Green by \ac{IT}\@.
Tabelle~\ref{tab:GreenInByIT} grenzt die beiden Begriffe voneinander ab.
Beschrieben wird jeweils die Rolle der \ac{IT}, das Ziel und das Potenzial der beiden Bereiche, untermauert durch je ein Beispiel~\cite{Calero.2015}.

\begin{table}[t]
 \centering\small
 \caption{Green in IT vs. Green by IT}
 \label{tab:GreenInByIT}
 \input{\tabledir/GreenInByIT.tex}
\end{table}

Die Unterteilung ist sowohl für Software als auch für Hardware relevant.
Demnach bestehen vier Hauptkategorien:
Green in Software, Green in Hardware, Green by Software, Green by Hardware.
Diese Arbeit konzentriert sich nicht auf die Hardware-bezogenen Aspekte.
Sie thematisiert die Kohlenstoffintensität und damit die Emissionen, die Software verursacht und ist deshalb der Kategorie Green in Software zuzuordnen.
Betrachtet man die Prognose der Kohlenstoffintensität mithilfe von Künstlicher Intelligenz, so könnte man dabei die \ac{IT} auch als Hilfsmittel zur Verbesserung der Nachhaltigkeit ansehen und somit Green by Software zuordnen~\cite{Calero.2015}.

Grüne Software bezieht sich auf Anwendungen, die weniger Kohlenstoffemissionen verursachen.
Laut der Green Software Foundation gilt es dabei folgende Aspekte zu berücksichtigen~\cite{GreenSoftwareFoundation.20240316T16:54:58.000Z}:

\begin{itemize}
 \item \textbf{Kohlenstoffeffizienz:}
 Änderungen in der Software oder Architektur einer Anwendung, sodass sie verantwortungsvoll weniger Kohlenstoff emittiert. Unter diesem Aspekt fallen weitere Unterziele:
 \begin{itemize}
  \item \textbf{Energieeffizienz:}
  Die Nutzung der geringstmöglichen Menge an Energie für die gleiche Arbeit.
  \item \textbf{Hardwareeffizienz:}
  Die Nutzung der geringstmöglichen Menge an Hardware für die gleiche Arbeit.
 \end{itemize}
 \item \textbf{Kohlenstoffbewusst:}
 Änderungen im Verhalten der Anwendung, sodass sie verantwortungsvoll weniger Kohlenstoff emittiert.
\end{itemize}

Energieeffiziente Software zeichnet sich also dadurch aus, dass sie so wenig Energie wie möglich verbraucht.
Hardware-Effizienz wird erreicht, wenn weniger Hardware für die gleiche Aufgabe benötigt wird und dadurch die sogenannten verkörperten (embodied) Emissionen, die durch Herstellung und Entsorgung von Hardware entstehen, gering gehalten werden.
Diese beiden Faktoren sind zwar wichtig, aber werden in der vorliegenden Arbeit nicht weiter berücksichtigt, denn der Fokus liegt auf dem bewussten Umgang mit Kohlenstoff.
Kohlenstoffbewusstsein meint eine Modifizierung von Berechnungen, um die Umwelt so wenig wie möglich zu belasten.
Ein bewusster Umgang mit Kohlenstoff setzt voraus zu verstehen, dass die gleiche Menge an verbrauchter Energie nicht immer die gleiche Kohlenstoffintensität hat, da diese je nach Zeitpunkt und Ort des Verbrauchs variiert~\cite{GreenSoftwareFoundation.2022}.

Im vorherigen Abschnitt wurden die verschiedenen Bereiche, in denen Emissionen laut dem \ac{GHG}-Protokoll anfallen, aufgezeigt (vgl. Tabelle~\ref{tab:GhgScopes}).
An dieser Stelle wurde jedoch nur auf die allgemeine Anwendung der Scopes für Unternehmen, nicht aber speziell für Software eingegangen.
Die Einteilung für Software hängt von der Art der Anwendung und der Umgebung, auf der sie betrieben wird, ab.
Betreiben Unternehmen Cloud-Anwendungen auf eigenen Server, so gehört die dafür benötigte Energie in Scope 2 und der verkörperte Kohlenstoff in Scope 3.
Werden Public Cloud-Anbieter genutzt, so fallen die Emissionen aus beiden Kategorien in Scope 3.
Für ein hybrides Szenario aus privaten und öffentlichen Cloud-Anwendungen fällt dementsprechend ein Teil der Emissionen in Scope 2 und ein Teil in Scope 3.
Entwickelt ein Unternehmen kundenorientierte Front-End-Anwendungen, so sind diese in Scope 3 des Unternehmens einzuordnen, da der Kunde selbst die Energie für das verwendete Gerät kauft~\cite{GreenSoftwareFoundation.2022}.

Die wachsende Notwendigkeit für Unternehmen, ihre Nachhaltigkeitsbemühungen zu intensivieren, wird zunehmend durch regulatorische Vorgaben getrieben.
Insbesondere verlangt die EU-Gesetzgebung von Großunternehmen sowie börsennotierten Gesellschaften, umfassend über soziale und ökologische Risiken, Chancen sowie die Auswirkungen ihrer Geschäftstätigkeiten auf Umwelt und Gesellschaft Rechenschaft abzulegen.
Ziel dieser Richtlinien ist es, eine konsistente und vergleichbare Nachhaltigkeitsbewertung von Unternehmen innerhalb der EU zu gewährleisten.
Mit der Verschärfung dieser Richtlinien im Jahr 2023, die ab 2025 Wirkung zeigen wird, verstärkt sich der Anreiz für Unternehmen, ihre Umweltauswirkungen offenzulegen und Maßnahmen zu deren Verbesserung einzuleiten~\cite{DirectorateGeneralforFinancialStability.20240411T13:37:55.000Z}.
Transparenz in der Nachhaltigkeitskommunikation bietet Unternehmen darüber hinaus eine Chance, ihr Image zu verbessern und sich auf dem Markt als Vorreiter in Sachen Umweltbewusstsein zu positionieren.
In diesem Kontext spielt die zuvor beschriebene Dokumentation anhand der GHG-Protokoll-Scopes eine wesentliche Rolle.

Angesichts dieser Entwicklungen hat die Randstad Digital Germany AG als Unternehmen für individuelle Softwaresysteme und kundenspezifische Beratung ihr Angebot im Bereich der nachhaltigen Softwarelösungen ausgebaut.
Angeregt durch die strengeren EU-Richtlinien und das steigende Bewusstsein für Klimaschutz, sieht die Randstad Digital Germany AG eine zunehmende Nachfrage nach ökologisch nachhaltiger Entwicklung und Beratung.
Kunden unterschiedlichster Branchen suchen gezielt nach Unterstützung, um ihre Anwendungen und Prozesse klimafreundlicher zu gestalten und somit ihren Beitrag zur Verringerung der ökologischen Fußabdrücke zu leisten.

%Die Motivation für Unternehmen, ihre Nachhaltigkeit zu dokumentieren und zu verbessern ist hauptsächlich durch Regulatorien getrieben.
%Das EU-Recht verpflichtet zum Beispiel alle Großunternehmen und alle börsennotierten Unternehmen, soziale und ökologische Risiken und Chancen, sowie Informationen der Auswirkungen ihrer Tätigkeiten auf Mensch und Natur offenzulegen.
%Auf diese Weise soll eine einheitliche Bewertung der Nachhaltigkeit von europäischen Unternehmen garantiert werden.
%Die Richtlinien wurden zuletzt 2023 mit Auswirkung auf 2025 verschärft, sodass nicht zuletzt deswegen Unternehmen zur Offenlegung ihrer Auswirkungen auf die Umwelt angeregt werden.
%Darüber hinaus bieten solche transparente Kommunikationen und damit verbundene Optimierungen für Unternehmen eine Möglichkeit, ihr Image zu verbessern und sich gegenüber der Konkurrenz am Markt attraktiv darzustellen.
%Zur Dokumentation werden unter anderem die zuvor genannten \ac{GHG}-Scopes verwendet.
%
%In Reaktion unter anderem auf die verschärften EU-Richtlinien und aufgrund des positiven Beitrags zum Klimaschutz, hat die Randstad Digital Germany AG als Unternehmen für individuelle Softwarelösungen und kundenspezifische Beratung ihre Expertise im Bereich nachhaltiger Softwarelösungen verstärkt.
%Randstad Digital Germany AG verzeichnet eine steigende Anzahl von Anfragen, in denen explizit nachhaltige Softwareentwicklung und Beratung gewünscht wird.
% Diese beiden Aspekte sind die Hauptfaktoren, die Randstad Digital bei seinen Kunden für die Motivation für nachhaltige Software sieht.

\section{Kohlenstoffintensität als Maß für Nachhaltigkeit}\label{CAP:ci}
Im Folgenden gilt es, Kohlenstoffintensität zu definieren und die Frage zu klären, wie diese mit Nachhaltigkeit zusammenhängt.
Der Begriff Kohlenstoff wird häufig als Synonym für alle Treibhausgase und als Oberbegriff für die Auswirkungen aller Arten von Emissionen und Aktivitäten auf die globale Erwärmung verwendet~\cite{GreenSoftwareFoundation.2022}.
Die Kohlenstoffintensität wird in Gramm \ac{CO2} pro bestimmter Einheit gemessen und gibt an, wie viel Kohlendioxid für diese Einheit emittiert wird~\cite{LyndonRuff.20220420T15:34:17.000Z}.

Das Konzept der Kohlenstoffintensität kann auf verschiedenste Bereich angewand werden, um die Auswirkungen auf die Umwelt zu messen.
Die vorliegende Arbeit beschäftigt sich damit, wann Strom verbraucht werden soll, sodass möglichst wenig Emissionen damit verbunden sind.
Sie betrachtet also die Kohlenstoffintensität von Stromnetzen, die in diesem Fall die Kohlenstoffintensität in Gramm \ac{CO2} pro \ac{kWh} angibt und aussagt, wie viel Kohlendioxid pro verbrauchter \ac{kWh} Strom emittiert wird.

Kohlenstoffintensität ist hier also ein Maß der Nachhaltigkeit des erzeugten oder verbrauchten Stroms.
Je geringer der Wert ist, desto nachhaltiger ist der Strom.
Genauer betrachtet führt ein verringerter Anteil von Energiequellen mit hoher \ac{CO2}-Emission (z.B.\ Braun- oder Steinkohle) am erzeugten Strom zu einem geringeren Emissionsfaktor des Stromnetzes.
Eine weitere Größe, die den Emissionsfaktor beeinflusst, ist der Wirkungsgrad des Energieträgers.
Ein Anstieg des Wirkungsgrads von Energiequellen mit hoher \ac{CO2}-Emission führt letztlich ebenfalls zu einem geringeren Emissionsfaktor des Stromnetzes, weil dann weniger dieser Quellen benötigt werden, um die gleiche Menge an Strom zu erzeugen~\cite{Icha.2020}.

Um Strom zu erzeugen, muss Energie einer anderen Form in elektrische Energie umgewandelt werden.
In Kohlekraftwerken wird zum Beispiel Kohle in einem Kessel verbrannt um Dampf zu erzeugen, der chemische Energie in elektrische Energie umwandelt.
Als Nebenprodukt wird viel Kohlendioxid freigesetzt, was die Stromerzeugung durch fossile Brennstoffe zu einer kohlenstoffintensiven Variante macht~\cite{Currie.2024}.
Stellt man sich vor, dass ein Gerät seinen Strom direkt aus einem Windkraftwerk beziehen würde, hätte diese Energie eine Kohlenstoffintensität von 0 g \ac{CO2}/\ac{kWh}, weil bei der Energieerzeugung durch Wind kein Kohlenstoff freigesetzt wird~\cite{GreenSoftwareFoundation.2022}.
Das Stromnetz ist jedoch ein Mix verschiedener Energiequellen (deshalb auch als Strommix bezeichnet), die sich unterschiedlich auf die Kohlenstoffintensität auswirken.
Bei der Verwendung von Strom kann nicht zwischen verschiedenen Erzeugungsarten ausgewählt werden, weshalb nur die Intensität des Gesamtnetzes betrachtet werden kann.
Durch erneuerbare Energien, wie Wind-, Solar-, oder Wasserenergie, erzeugter Strom ist weniger kohlenstoffintensiv, jedoch variiert seine Verfügbarkeit je nach Zeit und Ort.
Die Zeit ist ein wichtiger Faktor, weil ein direkter Zusammenhang zwischen Zeit und Wetter besteht und vom Wetter wiederum die erneuerbaren Energien abhängig sind.
Außerdem verfügen verschiedene Länder und Regionen über eine unterschiedliche Zusammensetzung des Stromnetzes.
Um die \ac{CO2}-Bilanz des verwendeten Stroms so gering wie möglich zu halten, ist es deshalb wichtig zu wissen, wann und wo die Kohlenstoffintensität des Stromnetzes gering ist.
%So kann zum Beispiel, wenn es die Anforderungen erlauben, die Nutzung des Stroms auf kohlenstoffarme Zeiten verschoben werden.
Hinzu kommt, dass erneuerbare Energien in Betrieb und Wartung verhältnismäßig günstig sind, was dazu führt, dass deren erzeugter Strom nicht nur umweltfreundlicher, sondern meist auch kostengünstiger ist~\cite{NationalGrid.20231106T13:28:05.000Z}.

Ein weiterer wichtiger Faktor ist das Zusammenspiel aus Energieangebot und -nachfrage.
Die Nachfrage kann stark schwanken, doch muss das Stromnetz trotzdem immer in der Lage sein, diese zu decken.
Optimal wäre es natürlich, das Angebot unverzüglich an die Nachfrage anpassen zu können, jedoch variiert die Flexibilität je nach Energiequelle stark.
Betrachtet man zum Beispiel Windenergie, liegt auf der Hand, dass diese über eine sehr geringe Flexibilität verfügt, weil die Windstärke nicht kontrolliert werden kann.
Mit Kohleenergie ist es hingegen weitaus einfacher, auf ein Angebotsdefizit zu reagieren.
Eine wichtige Größe ist dabei die marginale Kohlenstoffintensität~\cite{GreenSoftwareFoundation.2022}.
Übersteigt der Strombedarf plötzlich die vorhandene Energie, wird die benötigte Energie aus dem sogenannten Grenzkraftwerk bezogen.
%Steigt der Strombedarf plötzlich an,
Dieses zeichnet sich dadurch aus, dass es auf solche Änderungen schnell reagieren kann.
Um den Strompreis so gering wie möglich zu halten, werden die verfügbaren Energiequellen aufsteigend ihres Preises angeordnet~\cite{Corradi.20231207T10:48:51.000Z}.
Diese sogenannte Merit-Order (im Deutschen als Reihenfolge der Vorteilhaftigkeit bezeichnet) ist das Zusammenspiel aus Stromangebot, -preis und -bedarf.
Eine beispielhafte Merit-Order des deutschen Stromnetzes ist in Abbildung~\ref{FIG:merit-order} dargestellt.
\begin{figure}
 \caption[Merit-Order des deutschen Stromnetzes]{Die Merit-Order des deutschen Stromnetzes~\cite{Gro.5.10.2022}}
 {\includegraphics[width=0.99\textwidth]{\figdir/merit-order}}
 \label{FIG:merit-order}
\end{figure}
Die verschiedenen Energiequellen sind nach Preis aufsteigend angeordnet und durch den eingezeichneten Strombedarf wird ersichtlich, welche dieser Quellen je nach Bedarf herangezogen werden.
Eine ausreichende Stromversorgung wird durch Hinzuziehen einer geeigneten Menge an Kraftwerken garantiert, wobei die Quellen mit niedrigstem Preis die höchste Priorität haben.
Daraus lässt sich schließen, dass der Strom sowohl günstiger als auch grüner wird, je mehr erneuerbare Energien eingespeist werden~\cite{Gro.5.10.2022}.

Es bestehen verschiedene Ansätze für die Berechnung der Kohlenstoffintensität.
Allgemein stellt sich die Berechnung als anspruchsvoll heraus, vor allem weil jedes Land seine eigene Zusammenstellung verschiedener Kraftwerke hat~\cite{Currie.2024}.
Für eine Berechnungsweise (Formel~\ref{eq:ci}), die Forscher aus Großbritannien für die Carbon Intensity \ac{API} verwendet haben, wird der Anteil der verschiedenen Energiequellen, ein Intensitätsfaktor je Energiequelle, der spezifisch für eine Region ist, und die Stromnachfrage benötigt.
\begin{equation}
 \label{eq:ci}
 C_t = \frac{\sum_{g=1}^{G} P_{g,t} \times c_g}{D_t}
\end{equation}
Die Kohlenstoffintensität Ct zur Zeit t ist das Produkt aus Energiequelle und Intensitätsfaktor summiert für alle verwendeten Energiequellen und anschließender Teilung durch die Nachfrage~\cite{LyndonRuff.20220420T15:34:17.000Z}.
Für \ac{CO2}-Emissionen durch erneuerbare Energien sowie durch Kernkraft wird der Intensitätsfaktor mit 0 berechnet, sodass für diese Energiequellen das Gesamtergebnis der Kohlenstoffintensität gleich 0 ist.

Für den vorliegenden Anwendungsfall ist die marginale Kohlenstoffintensität, auch als \ac{MOER} bezeichnet, das geeignete Maß.
Diese gibt die Emissionsintensität der Grenzkraftwerke an, die auf die veränderte Stromnachfrage reagieren~\cite{Buchanan.2023}.

Nach neuesten Erkenntnissen ist die \ac{MOER} die geeignete Größe für die Einsparung von Emissionen durch zeitliche oder örtliche Lastverschiebung~\cite{Schram.03.05.201905.05.2019}\cite{WattTime.12.3.2024}.
Über lange Zeit hinweg bestand die Überzeugung, dass eine Verlagerung der Energielast in Phasen mit geringeren Großhandelsstrompreisen unweigerlich zu einer Reduktion der Emissionen führen müsse.
Dennoch belegten verschiedene Untersuchungen, dass häufig genau das Gegenteil eintritt.
Ferner wurde über Jahre angenommen, dass das Verschieben der Last in Zeiträume mit niedrigen durchschnittlichen Emissionsraten zwangsläufig zu einer Emissionsreduktion führen müsse.
Doch auch diese Annahme wurde von Studien widerlegt.
Im Kontext der Lastverschiebung haben über eineinhalb Jahrzehnte hinweg peer-reviewte Studien deutlich gemacht, dass die entscheidenden Faktoren die Emissionen und der Überschuss an erneuerbaren Energien sind, abhängig davon, welche Energieerzeuger als Grenzanbieter fungieren.
Es geht darum, zu identifizieren, welche Energieerzeuger als Reaktion auf die durch Lastverschiebung verursachten Nachfrageänderungen ihre Produktion erhöhen oder reduzieren.
Dies ermöglicht eine präzise Bewertung der realen Auswirkungen auf das Energienetz und dessen Emissionen.
Die Idee, Lastverschiebung in Datenzentren zur Minimierung von Emissionen einzusetzen, indem man sich auf Zeiten oder Orte konzentriert, wo reichlich Sonnenlicht für die Solarenergieerzeugung vorhanden ist, mag zunächst sinnvoll erscheinen.
Wenn jedoch die gesamte von Solaranlagen erzeugte Energie bereits durch die Netzlast aufgebraucht wird, könnte jede zusätzliche Nachfrage durch die Lastverschiebung das Hochfahren eines umweltschädlichen, fossil betriebenen Spitzenlastkraftwerks erforderlich machen.
Um überschüssige erneuerbare Energien effektiv zu nutzen, die ansonsten reduziert würden, ist es entscheidend, die Grenzemissionsraten und die Grenzerzeugungsquellen zu analysieren.
Es stellt sich die Frage, zu welchen Zeiten und an welchen Orten Anlagen erneuerbarer Energiequellen am Rande der Kapazitätsbeschränkungen operieren.
Zu ermitteln, wann und wo diese Anlagen ihre Produktion drosseln müssen und wie durch gezielte Lastverschiebung zusätzlicher sauberer Strom genutzt werden kann, ohne dass es zu einem Anstieg der Gesamtnetzemissionen kommt, ist zentral für umweltfreundliche Lastverschiebungen~\cite{WattTime.12.3.2024}.

Der Zusammenhang zwischen Stromerzeugung, Stromnachfrage und marginalen Emissionen ist in Abbildung~\ref{FIG:grid-moer} beispielhaft dargestellt.
\begin{figure}
 \caption[Nachfrage, Erzeugung und Emissionen eines Stromnetzes]{Nachfrage, Erzeugung und Emissionen eines Stromnetzes~\cite{DiStefano.2019}}
 {\includegraphics[width=0.8\textwidth]{\figdir/grid-moer}}
 \label{FIG:grid-moer}
\end{figure}
Entsteht an einem bestimmten Ort und zu einer bestimmten Zeit eine zusätzliche Stromnachfrage, ändert sich die Belastung der Kraftwerke, die zu diesem Zeitpunkt Strom produzieren.
Diese Grenzkraftwerke unterscheiden sich in Wirkungsgrad und Brennstoff stark und bestimmten die Emissionen der zusätzlichen Last auf das Stromnetz.
Übersteigt die Erzeugung aus erneuerbaren Energien die Gesamtlast des Netzes, wird diese in das Netz eingespeist und zusätzlich anfallende Last würde zu diesem Zeitpunkt nur wenig oder gar keine Emissionen verursachen.
Im Zeitraum bei dem erneuerbare Energie nur in einem geringen Maß verfügbar sind, ist die fossile Erzeugung hoch, um die Nachfrage zu decken.
Es werden entweder zusätzliche Kraftwerke eingeschaltet oder Strom aus benachbarten Netzen importiert.
In dieser Abbildung wird sichtbar, dass die höchste \ac{MOER} nicht unbedingt zum gleichen Zeitpunkt wie ein Nachfragehoch auftritt~\cite{DiStefano}.

Die \ac{MOER} eignet sich für den vorliegenden Anwendungsfall demnach besser, als die durchschnittliche Kohlenstoffintensität, da sie die Emissionen pro Lastverschiebung angibt und somit die Beziehung zwischen Ursache und Wirkung der Lastverschiebung erfasst\cite{Wiesner.2021}.


Kohlenstoffintensität kann auch verwendet werden, um die Nachhaltigkeit von Software zu berechnen, nicht nur für die Nachhaltigkeit der von der Software verwendeten Energie.
Ein Berechnungsansatz, der von der Green Software Foundation speziell für Software entwickelt wurde, ist der sogenannte \ac{SCI} Wert.
Er gibt die gesamten Kohlenstoffemissionen einer funktionalen Einheit R in g\ac{CO2}eq/\ac{kWh} an~\cite{GreenSoftwareFoundation.2022}.
Die entsprechende Formel ist Formel~\ref{eq:sci}.
\begin{equation}
 \label{eq:sci}
 SCI = ((E \times I) + M) pro R
\end{equation}
In die Berechnung fließen folgende Werte ein~\cite{Buchanan.2023}~\cite{GreenSoftwareFoundation.2022}:
\begin{itemize}
 \item die von der Software benötigte Energie E (Strom für \ac{VM}, Kühlung, etc.)
 \item die standortbezogene marginale Kohlenstoffemissionen I (emittierter Kohlenstoff pro \ac{kWh})
 \item der verkörperte Kohlenstoff M der zugrunde liegenden Hardware (emittierter Kohlenstoff während der Herstellung und der Entsorgung der Hardware)
 \item die Skaliereinheit R, die für die zu messende Software relevant ist (z.B. pro zusätzlichem Benutzer, Gerät, ~\ac{API}-Aufruf, etc.)
\end{itemize}
Anhand dieser Formel ergeben sich die Optimierungspunkte des \ac{CO2}-Fußabdrucks eines Workloads, nämlich Energie, Kohlenstoffintensität, verkörperter Kohlenstoff und Skaliereinheit.
Die Green Software Foundation empfiehlt demnach auch, die marginale Kohlenstoffintensität als Messwert für die Emissionen des Stromnetzes zu verwenden~\cite{GreenSoftwareFoundation.2022b}.

Für die Berechnung der Summe an Kohlenstoffemissionen einer Software werden demnach detaillierte Daten zu den einzelnen Faktoren der Formel benötigt.
Wichtig ist dabei, die Berechnung stets pro einer Einheit R zu machen, da nur dann aussagekräftige Werte erzielt werden können.
Würden stattdessen absolute Werte verwendet werden, könnte ein Anstieg der Nutzer zum Beispiel fälschlicherweise dazu führen, dass höhere Emissionswerte als schlechte Nachhaltigkeitsentwicklung interpretiert werden~\cite{GreenSoftwareFoundation.2022}.

Bergman stellt im Microsoft-Blog einen Ansatz vor, um die Energie E in kWh zu berechnen, die eine bestimmte \ac{VM} verbraucht (vgl. Formel~\ref{eq:e})~\cite{Bergman.15.2.2021}.
\begin{equation}
 \label{eq:e}
 E = \frac{c \times E_c + E_r + g \times E_g}{1000}
\end{equation}
Diese ergibt sich aus der Anzahl der \acp{CPU} und der \acp{GPU} multipliziert mit dem jeweiligen Energieverbrauch zuzüglich der Energie für den Speicher.
Um das Ergebnis in Kilowattstunden zu erhalten, wird der Wert durch 1000 geteilt.
Im Beispiel von Microsoft wird die Thermal Design Power der Hardware-Komponenten als Schätzung für den Stromverbrauch hergenommen.
Die \ac{TDP}, auch Prozessor-Basisleistung genannt, gibt den Stromverbrauch unter maximaler theoretischer Last an.
Bei Lasten geringer der maximalen Last ist der Stromverbrauch geringer als die \ac{TDP}~\cite{Intel.20240317}.
Es wird allerdings darauf hingewiesen, dass die Verwendung des tatsächlichen Energieverbrauchs zu einem genaueren Ergebnis führen würde.
Diesen zu messen ist jedoch nicht immer möglich.
Bei einer Azure D2sv3, einer Standard-\ac{VM} geeignet zum Beispiel für kleine bis mittlere Business-Anwendungen, ergibt sich bei 80\% \ac{CPU}-Auslastung über eine Stunde nach Formel~\ref{eq:e-example} ein Energieverbrauch von 0,264kWh~\cite{Bergman.15.2.2021}.
\begin{equation}
 \label{eq:e-example}
 E = \frac{2 \times 0,8 \times 165W}{1000}
\end{equation}
$E_c$ wird hier durch die Auslastung multipliziert mit der \ac{TDP} berechnet.
Die Berechnungsfaktoren für \acp{GPU} fallen in diesem Beispiel weg, weil ausschließlich \acp{CPU} verwendet werden.

Bislang wurde ein wesentlicher Aspekt noch nicht berücksichtigt:
Von der gesamten Energie, die aufgebracht werden muss, kann nicht 100\% tatsächlich von Servern verwendet werden.
Die \ac{PUE} ist ein Maß für die Energieeffizienz von Rechenzentren und wird wie in Formel~\ref{eq:pue} beschrieben berechnet~\cite{Walsh.22.4.2022}.
\begin{equation}
 \label{eq:pue}
 PUE = \frac{\text{Gesamtstromverbrauch des Rechenzentrums}}{\text{Stromverbrauch der IT-Komponenten}}
\end{equation}
Sie gibt das Verhältnis zwischen der Energie an, die in einem Rechenzentrum verwendet werden kann, verglichen mit der insgesamt aufgewendeten Energie.
Verbraucht wird die Differenz unter anderem für Kühlung und Stromverteilung.
Ein optimaler \ac{PUE}-Wert würde bei 1 liegen, was bedeuten würde, dass die gesamte zugeführte Energie effektiv für den Betrieb der Hardware genutzt wird~\cite{GreenSoftwareFoundation.2022}.
Die durchschnittliche \ac{PUE} der Rechenzentren liegt für Azure bei 1,235 und für \ac{GCP} bei 1,10~\cite{Walsh.22.4.2022}\cite{Google.20240312}.
Amazon veröffentlicht für \ac{AWS} keine Informationen über die \ac{PUE}~\cite{Cockcroft.18.7.2023}.

Berechnet man den Energieverbrauch wie in Formel~\ref{eq:e}, muss dieser schlussendlich noch mit der \ac{PUE} verrechnet werden.
Insgesamt ergibt sich dadurch ein Energieverbrauch von $0,264kWh \times 1,235 = 0,32604kWh$.
%!
%!
\chapter{Strategien zur Nachhaltigkeitsoptimierung}\label{CAP:strategies}
Für viele Unternehmen bietet die Verlagerung in die Cloud einen Ansatz der Verbesserung der Nachhaltigkeit durch zentrale Verwaltung, Konsolidierung von Ressourcen und Effizienzsteigerungen.
Durch zunehmendes Migrieren in die Cloud wird es umso wichtiger, dass die entsprechenden Betreiber sich um den ökologischen Fußabdruck ihres Services kümmern~\cite{Buchanan.2023}.
Verwaltete Cloud-Dienste bieten aufgrund ihrer hohen Rechendichte und ihrer enormen Hardware- und Energienutzung ein großes Potenzial zur Effizienzverbesserung~\cite{Currie.2024}.
Die drei großen Cloud-Anbieter \ac{AWS}, Azure und \ac{GCP} haben sich zur Kohlenstoffneutralität verpflichtet.
Dies bedeutet jedoch nicht, dass es allein ausreichend ist, eine Anwendung auf einer dieser Plattformen bereitzustellen und sich nicht weiter mit dem Thema Nachhaltigkeit auseinander setzten zu müssen.
Auch die Anwendung selbst muss für Nachhaltigkeit optimiert sein und die vorhanden Möglichkeiten müssen genutzt werden~\cite{Newman.2023}.
Der deutsche Bitkom hat in einer Metastudie auf Grundlage von mehreren Studien aus den Jahren 2012 bis 2019 zwei Hauptkategorien für Energieensparungen definiert: Anwendungen und Infrastruktur.
In Abbildung~\ref{FIG:sustainability-infrastructure} werden die Facetten der Nachhaltigkeit auf Anwendungs- und Infrastrukturebene dargestellt.
Von Amazon~\cite{Mokhtari.2023} wird dieses Modell als Modell der geteilten Verantwortungen vorgestellt.
Der Cloud-Anbieter ist für die Nachhaltigkeit der Cloud verantwortlich, dies beinhaltet unter anderem die Bereitstellung einer effizienten, gemeinsam genutzten Infrastruktur, die Wasserverantwortung und die Beschaffung erneuerbarer Energien.
Der Cloud-Nutzer ist für die Nachhaltigkeit in der Cloud verantwortlich, also zum Beispiel das Optimieren von Workloads und Ressourcennutzung und das Minimieren der Gesamtressourcen, die für die Workloads bereitgestellt werden müssen~\cite{Mokhtari.2023}.
\begin{figure}
 \caption[Nachhaltigkeit auf Anwendungs- und Infrastrukturebene]{Nachhaltigkeit auf Anwendungs- und Infrastrukturebene, eigene Darstellung nach ~\cite{Mokhtari.2023}}
 {\includegraphics[width=0.99\textwidth]{\figdir/SustainabilityOfAndOnInfrastructure}}
 \label{FIG:sustainability-infrastructure}
\end{figure}
Im folgenden soll deshalb analysiert werden, welche Optimierungsstrategien für den Cloud-Nutzer zur Verfügung stehen.

%Eine Strategie im Cloud-Umfeld ist die Verwendung sogenannter Spot Virtual Machines.
%Der Gedanke dahinter ist, nicht verwendete Instanzen von anderen virtuellen Maschinen herzunehmen, wenn diese nicht benötigt werden, anstatt eigene anzufragen.
%Das wirkt sich nicht nur positiv auf die Kosten (bei Azure laut Microsoft bis zu 90\% Preisnachlass), sondern auch auf den Energieverbrauch aus, weil die Kapazität der VMs genutzt wird, anstatt dass sie sich im Leerlauf befinden.
%Dieser Ansatz hat jedoch den Nachteil, dass der Workload bei fehlenden Kapazitäten nicht gestartet oder möglicherweise abgebrochen und nicht fortgesetzt werden kann~\cite{Norlander}
\section{Scheduling}\label{CAP:scheduling}
% \subsection{Grundlagen des Scheduling}
Unter Scheduling versteht man im Allgemeinen die optimierte Zuteilung von Ressourcen zu Aufgaben in bestimmten Zeiträumen.
Dabei kann die zum Ziel gesetzte Optimierung vielerlei Ausführungen haben, zum Beispiel die Fertigstellungszeit so gering wie möglich zu halten oder die Anzahl der zu spät vollendeten Aufgaben zu minimieren~\cite{Gawiejnowicz.2020}.

Ein Scheduling-Problem wird durch vier verschiedene Größen definiert~\cite{Gawiejnowicz.2020}:
\begin{itemize}
 \item Die Menge an auszuführenden Arbeiten
 \item Die Menge an ausführenden Einheiten
 \item Die Menge an zusätzlichen für die Ausführung benötigten Einheiten
 \item Ein Maß für die Qualität der Lösung
\end {itemize}
Außerdem hat ein Scheduling-Problem verschiedene Eigenschaften, die im Folgenden kurz erklärt werden~\cite{Pinedo.2022}:
\begin{itemize}
 \item Bearbeitungszeit: Die Bearbeitungszeit eines Auftrags auf einer Infrastruktur. Diese kann abhängig von der jeweiligen Maschine sein.
 \item Freigabedatum: Der Zeitpunkt, an dem der Auftrag im System eintritt und somit der frühestmögliche Zeitpunkt für den Beginn des Auftrags
 \item Fälligkeitsdatum: Der Zeitpunkt der zugesagten Fertigstellung. Muss das Fälligkeitsdatum eingehalten werden, wird es als Frist bezeichnet.
 \item Gewichtung: Ein Prioritätsfaktor, der die Bedeutung des Auftrags im Verhältnis zu den anderen Aufgaben im System angibt
\end{itemize}

% \subsection{Möglicher Einfluss von Scheduling auf Kohlenstoffemissionen}
% Die Green Software Foundation definiert ~\cite{GreenSoftwareFoundation.2022}.
Zur Veranschaulichung, welchen Einfluss die zeitliche Verschiebung von Workloads haben kann, soll zunächst ein beispielhaftes Verhältnis von Stromangebot und -nachfrage betrachtet werden (Abbildung~\ref{FIG:grid-supply-demand}).
\begin{figure}
 \caption[Beispielhafte Darstellung von Angebot und Nachfrage eines Stromnetzes]{Eine beispielfhafte Darstellung von Angebot und Nachfrage eines Stromnetzes~\cite{Buchanan.2023}.}
 {\includegraphics[width=0.9\textwidth]{\figdir/grid-supply-demand}}
 \label{FIG:grid-supply-demand}
\end{figure}
In dieser vereinfachten Darstellung wird davon ausgegangen, dass zur Mittagszeit die Verfügbarkeit von Solarstrom besonders hoch ist.
Aus diesem Grund übersteigt das Angebot in diesem Zeitraum die Nachfrage.
Dieses überschüssige Angebot könnte genutzt werden und der höhere Stromverbrauch würde keine zusätzlichen Emissionen verursachen.
Andererseits muss zu den Zeiten, zu denen kein oder nur wenig Solarstrom zur Verfügung steht, auf Gas zurückgegriffen werden, um die Nachfrage decken zu können~\cite{Buchanan.2023}.
Eine Studie von Microsoft~\cite{Dodge.06212022} aus dem Jahr 2022 hat aufgezeigt, dass durch Zeitverschiebung durchschnittlich 15\% weniger Kohlenstoffintensität (gemessen am \ac{SCI}) erreicht werden kann.
Die Studie hat sich zwar auf Aufgaben von Künstlicher Intelligenz in der Cloud konzentriert, sie zeigt jedoch, dass die zeitliche Verlagerung von rechenintensiven Workloads ein großes Verbesserungspotenzial birgt.
Das größte Potenzial der Zeitverschiebung liegt demnach in der Verschiebung von Workloads mit kurzer Dauer und großem Volumen in Regionen mit großer Unstetigkeit~\cite{Buchanan.2023}.

% \subsection{Dynamische Scheduling Modelle}\label{CAP:dynamic-scheduling-models}
Es sollen zwei zeitliche Optimierungsmethoden untersucht werden, die im folgenden kurz beschrieben werden~\cite{Dodge.06212022}:
\begin{itemize}
 \item Flexibler Start: Ein flexibler Workload wird zu einem Zeitpunkt mit minimaler marginalen Kohlenstoffintensität in den nächsten N Stunden gestartet.
 Dabei wird der Workload nicht unterbrochen, sondern wird bis zur Fertigstellung ausgeführt.
 Es werden alle möglichen Startzeiten im Zeitfenster N mit einer Granularität von fünf Minuten betrachtet.
 \item Anhalten und Fortsetzen: Ein Workload wird (möglicherweise mehrmals) angehalten und wieder neu gestartet, um sicherzustellen, dass er ausschließlich zu kohlenstoffarmen Zeiten läuft.
 Der Workload wird dadurch in den nächsten (N + Dauer des Auftrags) Stunden durchgeführt.
 Wichtige Voraussetzungen für diese Methode sind ebenfalls die zeitliche Flexibilität und zudem funktioniert sie nur für Workloads, die unterbrochen und wieder neu aufgenommen werden können.
 Zur Umsetzung werden genügend viele fünfminütige Intervalle mit geringsten Grenzemissionen während des Zeitfensters (N + Dauer des Auftrags) gesucht.
 Es wird davon ausgegangen, dass die Unterbrechung und der Neustart des Auftrags sofort erfolgen und keine zusätzliche Energie verbraucht wird.
 Dies ähnelt den Spot-Instanzen auf bestehenden Cloud-Plattformen, die einen vom Nutzer festgelegten Preis als Schwellenwert für automatisches Pausieren verwenden.
\end {itemize}
Zu beachten ist dabei, dass unterschiedliche Regionen sich durch verschieden starke Varianzen auszeichnen.
In manchen Regionen schwankt die Emissionsgröße innerhalb eines Tages stark, wodurch sich diese sehr gut für die Methode des Anhaltens und Fortsetzens eignen.
Andere Regionen haben dagegen relativ gleichbleibende Emissionen, sodass zeitliche Optimierungen nur einen geringen Einfluss haben können.
In einer vorherigen Studie (\cite{Dodge.06212022}) wurde z.B. die Emissionsverringerung durch einen flexiblen Start anhand des DenseNet 201 untersucht.
DenseNet201 ist ein Convolutional Neural Network mit wenigen Schichten, das in der Studie weniger als eine halbe Stunde benötigte.
Bei vielen Regionen ist eine Emissionsreduzierung um mehr als 20\% möglich, das beste Ergebnis liegt bei 80\% für die Region West US\@.
Die Wirksamkeit der eingesetzten Methode hängt demnach auch stark von der betrachteten Region ab~\cite{Dodge.06212022}.

Betrachtet man ein zeitintensiveres Experiment, wie das Training eines Language Modells mit sechs Millionen Parametern, ist das Gegenteil zu beobachten.
Beim flexiblen Start ist das Optimierungspotential sehr gering (meist weniger als 1\%), jedoch bietet das Anhalten und Fortsetzen gute Optimierungsergebnisse.
In vielen Regionen sind Emissionsreduzierungen um mehr als 10\% und in der Region West US erneut am meisten mit ungefähr 28\% möglich~\cite{Dodge.06212022}.

\section{Scaling}
Skalierung bedeutet im Kontext des Softwarebetriebs eine Erhöhung oder Verringerung der Rechen-, Speicher- oder Netzwerkressourcen, die für einen bestimmten Workload genutzt werden.
Wohingegen man in herkömmlichen, dedizierten Hosting-Umgebungen durch die verfügbaren Hardwareressourcen begrenzt ist, bietet eine Cloud-basierte Anwendung diese Möglichkeit.
Es kann zudem zwischen vertikaler Skalierung (Scaling Up) und horizontaler Skalierung (Scaling Out) unterschieden werden.
Bei der vertikalen Skalierung wird die Rechenleistung erhöht (oder auch verringert) und die Infrastruktur beibehalten, die horizontale Skalierung vergrößert oder verkleinert die Ressourcen bei gleichbleibender Rechenleistung~\cite{AlibabaCloudCommunity.20240118T09:19:15.000Z}.
%Ein Vergleich dieser beiden Methoden ist in Abbildung~\ref{FIG:scaling} dargestellt.
%\begin{figure}
% \centering
% \subfloat[\centering Horizontale Skalierung]{{\includegraphics[width=.4\textwidth]{\figdir/horizontal-scaling-2} }}%
% \qquad
% \subfloat[\centering Vertikale Skalierung]{{\includegraphics[width=.5\textwidth]{\figdir/vertical-scaling-2} }}%
% \caption{Skalierung \cite{Slingerland.2023}}%
% \label{FIG:scaling}%
%\end{figure}
Durch vertikale Skalierung wird zum Beispiel die Anzahl der \acp{VM} in einem Cluster erhöht oder verringert, um das System auf eine steigende oder sinkende Nachfrage anzupassen.
Durch Load Balancing wird die Rechenlast auf die verschiedenen Maschinen verteilt.
Ein Vorteil dieser Maßnahme ist unter anderem die höhere Ausfallsicherheit durch Verteilung auf mehrere Einheiten.
Bei der horizontalen Skalierung wird beispielsweise bei einer vorhandenen \ac{VM} die \ac{CPU} oder die Speicherkapazität erhöht oder verringert.
Der Workload wird also nach wie vor von einer Infrastruktur-Einheit bewerkstelligt, diese kann aber mehr Last verarbeiten.
Das hat den Vorteil, dass nicht zwischen unterschiedlichen Komponenten kommuniziert und synchronisiert werden muss und die zusätzlich anfallenden Kosten meist geringer sind als bei der horizontalen Skalierung~\cite{Slingerland.2023}.
Cloud-Anbieter verfügen meist über automatische Skalierungsmöglichkeiten, sodass eine Anpassung der Ressourcen nicht manuell stattfinden muss.
Dadurch ist ein effizienteres und Kosten-effektiveres Skalieren möglich.
Auto-Scaling wird detaillierter in Kapitel~\ref{CAP:scenarios} behandelt, wenn es um die Anwendung der hier vorgestellten Strategien geht.
% Load shifting
\section{Demand Shifting}
Eine Software-Anwendung kann nachhaltiger betrieben werden, indem der Strombedarf auf die Kohlenstoffintensität ausgerichtet wird.
Als Lösung sollen Software-Workloads, die viel Rechenleistung benötigen, auf Zeiten und Orte verlagert werden, bei denen der Strommix zu den geringst möglichen Kohlenstoffemissionen führt.
Durch sogenanntes Demand-Shifting, also der Verschiebung der Nachfrage je nach Kohlenstoffintensität, kann Studien zufolge zwischen 45\% und 99\% Kohlenstoff eingespart werden.
Bei der örtlichen Verlagerung wird ein Ort, bei dem der Strom im Moment \glqq kohlenstoffärmer\grqq{} ist, gewählt.
Für die örtliche Verlagerung kann man sich die unterschiedlichen Wetterverhältnisse weltweit zu nutzen machen, denn wenn es z.B. in Deutschland im Moment wolkig und windstill ist, können an einem anderen Ort bei wolkenfreiem Himmel und Sonnenschein die perfekten Verhältnisse für die Solarstrom-Erzeugung herrschen.
Für die zeitliche Verlagerung ist ein umfangreiches Wissen über die Kohlenstoffintensität bzw. die Wetterverhältnisse im Verlauf der Zeit nötig.
Auf Grundlage dessen können Workloads in Zeiten mit geringer Kohlenstoffintensität verlagert werden~\cite{GreenSoftwareFoundation.2022}.
Eine beispielhafte Darstellung der zeitlichen Verlagerung ist in Abbildung~\ref{FIG:time-shifting} dargestellt.
\begin{figure}
 \caption[Zeitliche Verschiebung eines Workloads]{Zeitliche Verschiebung eines Workloads, um die Variabilität der Kohlenstoffintensität zu nutzen~\cite{Currie.2024}}
 {\includegraphics[width=0.7\textwidth]{\figdir/time-shifting}}
 \label{FIG:time-shifting}
\end{figure}
Die Kohlenstoffintensität (blaue Linie) schwankt im Laufe der Zeit stark.
Der Beispiel-Workload dauert eine Stunde lang und wird lediglich um eine Stunde verschoben, verbraucht aber dadurch deutlich weniger Kohlenstoff.
Ein Szenario, das von der zeitlichen Verschiebung profitieren kann ist zum Beispiel das Batch Processing.
Ein Batch-Job eignet sich besonders gut dafür, da mehrere Aufgaben gruppiert und nacheinander ohne Benutzereingabe ausgeführt werden.
Außerdem sind sie oft zu einer beliebigen Zeit innerhalb eines bestimmten Zeitraums ausführbar.
Dazu zählen unter anderem Software-Updates und System-Backups.
Microsoft nutzt diesen Ansatz für Windows-Updates und erzielt damit eine Verringerung der Kohlenstoffemissionen um bis zu 99\%~\cite{Currie.2024}.
Die örtliche Verlagerung kann laut einer Studie jedoch einen weitaus größeren Einfluss haben, als die Verschiebung auf zeitlicher Ebene.
Die Kohlenstoffintensität kann dadurch um bis zu 75\% reduziert werden~\cite{Dodge.06212022}.
% Das Demand Shifting auf örtlicher Ebene kann als eine Form des Scheduling betrachtet werden, denn beide verfolgen das Grundprinzip der optimalen zeitlichen Planung der Ausführung eines Workloads.

\section{Demand Shaping}
Eine ähnliche Methode, die ebenfalls das Ziel verfolgt, die Kohlenstoffemissionen zu senken, ist das Demand-Shaping.
Dabei wird bei geringer Kohlenstoffintensität die volle Funktionalität geboten und bei hoher Kohlenstoffintensität Einschränkungen getroffen oder die Funktionalität reduziert~\cite{Currie.2024}.

Man kann Demand Shifting und Demand Shaping also voneinander abgrenzen, indem man beim Demand Shifting einen bestimmten Workload betrachtet und die Verhältnisse dafür entweder auf örtlicher oder auf zeitlicher Ebene anpasst.
Beim Demand Shaping werden die aktuellen Verhältnisse betrachtet (Ort und Zeit sind unveränderlich) und der Workload, genauer gesagt die Höhe der Stromnachfrage, wird an diese angepasst.

Das Demand Shaping kann als eine Form des Scalings betrachtet werden, weil beide Prinzipien die Betriebsgröße bzw. Kapazität unter bestimmten Voraussetzungen anpassen.

Ein Beispiel für die Anwendung von Demand Shaping zeigt Google mit der optimierten Lastverteilung seiner Datencenter.
Täglich vergleicht Google die Vorhersage der erwarteten durchschnittlichen Kohlenstoffintensität im Verlauf des nächsten Tages (verwendet wird die Vorhersage von Electricity Maps) mit der Vorhersage der auf dem System zu erwartenden Last und passt diese beiden Größen aufeinander an.
Die Compute Tasks werden Stunde für Stunde an die Kohlenstoffwerte angepasst, um möglichst viel Last auf Zeiten mit geringen Emissionen zu verschieben.
Rundum verfügbare Services, wie die Google-Suche oder der Kartendienst Google-Maps sollen davon nicht beeinträchtigt werden, doch für nicht zeitkritische Updates, wie das Hinzufügen neuer Wörter zum Google-Übersetzer ist die Verschiebung geeignet.
Laut Google zeigt die Lastverteilung Erfolge und die Verwendung kohlenstoffarmer Energiequellen kann durch diesen Ansatz erfolgreich erhöht werden~\cite{Radovanovic.22.4.2020}.

Ein allgegenwärtiges Beispiel für Demand Shaping ist der bei Geräten wie Autos, Waschmaschinen oder Spülmaschinen verfügbare Eco-Modus.
Das Prinzip dahinter ist, dass Leistung verzögert wird für weniger Ressourcenverbrauch.
Wichtig ist dabei, dass der Eco-Modus lediglich eine Option ist, aber der Leistungsverzicht nicht dauerhaft oder verpflichtend ist~\cite{GreenSoftwareFoundation.2022}.
Das Konzept eines Eco-Modus könnte auch für Software-Anwendungen basierend auf der Kohlenstoffintensität angewandt werden.
Dieser könnte die Funktionalitäten der Software an die Kohlenstoffintensität anpassen, indem lediglich eingeschränkte Funktionalitäten geboten werden, wenn der Strommix \glqq kohlenstoffreich\grqq{} ist, und ansonsten die gesamte Funktonalität.

\section{Voraussetzungen zur Anwendung der Strategien}\label{CAP:requirements}
% Geeignete Workloads
Im Folgenden sollen die Eigenschaften für Workloads, die für zeitliche Verschiebungen geeignet sind, gefolgt von Voraussetzungen für die örtliche Verschiebung analysiert werden.
Für die zeitliche Verschiebung werden die Bearbeitungszeit, der Zeitpunkt der Ausführung und die Möglichkeit für Unterbrechungen untersucht.
Bezüglich der Bearbeitungszeit lassen sich Workloads grob in drei Kategorien unterteilen: kurze, lange und kontinuierliche Workloads.
Für die vorliegende Arbeit werden folgende Definitionen verwendet:
Als kurz wird ein Workload dann definiert, wenn er eine Laufzeit von wenigen Minuten bis hin zu ein paar Stunden hat.
Lang dauernde Workloads benötigen bis zu ihrer Fertigstellung mehrere Stunden bis hin zu einigen Tagen.
Bei kontinuierlichen Workloads ist die Ausführungszeit mehrere Wochen oder Monate lang oder es gibt kein definiertes Ende~\cite{Wiesner.2021}.

Die kurzen Workloads machen den Großteil aus, wie Untersuchungen für Google oder Alibaba gezeigt haben.
Dabei hat sich gezeigt, dass bei Google die meisten Workloads nur wenige Minuten dauern und bei Alibaba 90\% der Batch-Jobs weniger als 15 Minuten benötigen.
Wichtig für die Verschiebung sind vor allem zeitliche Beschränkungen.
Haben die Workloads eine baldige Frist, wie zum Beispiel CI-/CD-Läufe, so eignen sie sich nicht zur späteren Ausführung.
Jedoch können einige Batch-Jobs, darunter zum Beispiel nächtliche Backups, an bestimmte Vereinbarungen gekoppelt sein und deshalb eine höhere Flexibilität hinsichtlich des Ausführungszeitpunkts bieten.
Für länger laufende Workloads wie zum Beispiel das Trainieren von Machine-Learning-Modellen besteht das Problem, dass sie oft mehr Ressourcen anfragen, als eigentlich nötig.
Wegen ihrer oft Energie-intensiven Arbeit und der Tatsache, dass auf ihre Fertigstellung oft ein menschlicher Eingriff folgt, sind sie gut für die zeitliche Verschiebung geeignet.
Ist es dafür beispielsweise nicht entscheidend, ob der Workload direkt gestartet wird und dann nachts fertig ist, oder später gestartet wird und bis zum nächsten Morgen vollendet ist, ist das die perfekte Vorassetzung für eine mögliche spätere Ausführung.
Kontinuierliche Workloads umfassen zum Beispiel Blockchain-Mining, langandauernde wissenschaftliche Simulationen oder durchgehend verfügbare Benutzerschnittstellen.
Sie bieten zwar den Vorteil, dass in ihnen möglicherweise ein großes Einsparungspotenzial steckt, jedoch macht die lange oder unendliche Laufzeit eine zeitliche Verschiebung nur schwer oder gar nicht möglich~\cite{Wiesner.2021}.

Betrachtet man den Zeitpunkt der Ausführung, können im wesentlichen zwei Unterscheidungen getroffen werden:
Ad-hoc-, also sofort auszuführende, und geplante Workloads.
Der Ausführungszeitpunkt und wie wichtig dessen Einhaltung ist, hat eine große Auswirkung auf die Eignung für zeitliche Verschiebung.
Ein Großteil der kurzen und langen Workloads sind Ad-hoc-Workloads.
Die Schwierigkeit besteht darin, die Ausstellung eines solchen Workloads im Voraus zu wissen.
Erst wenn der Workload erstellt wurde, kann anhand seiner zeitlichen Beschränkungen die Ausführung geplant werden.
Beispiel sind erneut CI-/CD-Läufe und Machine-Learning Trainings und zudem von bestimmten Ereignissen ausgelöste Workloads oder von Nutzern direkt für die Ausführung erstellte Workloads.
Geplante Workloads sind solche, deren Ausführung für einen Zeitpunkt in der Zukunft geplant ist, wie bereits in Kapitel~\ref{CAP:scheduling} beschrieben.
Typisch dafür sind periodisch auftretende Workloads, wie beispielsweise nachts ausgeführte Integrationstests und Builds oder regelmäßige Backups, Aktualisierung von Suchindizes in Datenbanken und automatisch erstellte Berichte.
Der große Vorteil ist, dass sie oft zeitlich sowohl nach hinten, als auch nach vorne verschoben werden können, abhängig vom Fälligkeitsdatum.
Bei einem regelmäßig in der Nacht ausgeführten Integrationstests mag es zum Beispiel egal sein, ob er statt um 1 Uhr schon um 23 Uhr oder erst um 3 Uhr ausgeführt wird.
Diese Art von Workload eignet sich also meist hervorragend für eine zeitliche Verschiebung und kommt darüber hinaus auch oft vor.
Bei Microsoft sind bis zu 60\% der auf großen Klustern laufenden Workloads wiederkehrende Batch-Jobs, wovon mehr als 40\% täglich ausgeführt werden.
Andere gängige Ausführungsperioden sind 15 Minuten, eine Stunde und zwölf Stunden.
Bei Google zeigte sich zwischen 2011 und 2019 eine Veränderung hin zu mehr geplanten Workloads und einer deutlichen Erhöhung der Ausführungsrate~\cite{Wiesner.2021}.
\begin{figure}
 \centering
 \subfloat[\centering Zeitliche Verschiebung von Ad-hoc- und geplanten Workloads]{{\includegraphics[width=.45\textwidth]{\figdir/ScheduledVsAdhocWorkloads} }}%
 \qquad
 \subfloat[\centering Aufteilung von unterbrechbaren Workloads]{{\includegraphics[width=.45\textwidth]{\figdir/InterruptibleWorkloads} }}%
 \caption[Zeitpunkt der Ausführung und Unterbrechbarkeit von Workloads]{Zeitpunkt der Ausführung und Unterbrechbarkeit von Workloads}%
 \label{FIG:workloads-execution-time-interruptibility}%
\end{figure}

Eine Unterscheidung kann außerdem zwischen unterbrechbaren und nicht unterbrechbaren Workloads gemacht werden.
Unterbrechbare Workloads verfügen über einen Checkpoint-Mechanismus oder speichern Zwischenergebnisse, sodass sie pausiert und später fortgeführt werden können.
Ein Beispiel dafür ist ein Training eines Machine-Learning-Modells, bei dem in regelmäßigen Abständen Checkpoints gespeichert werden, die später zur Analyse, zur Fortführung oder zur Fehlerbehebung hergenommen werden können.
Dabei handelt es sich vermehrt um lange Workloads.
Workloads mit Möglichkeit zur Unterbrechung eignen sich gut für die im vorherigen Kapitel beschriebene Methode des Anhaltens und Fortsetzens.
Ein weiteres Beispiel könnte die Erstellung von monatlichen Geschäftsberichten für verschiedene Kunden sein, wobei die Workloads hier aus vielen kleineren Aufgaben bestehen und somit gut aufgeteilt werden können.
Daneben kann für andere Workloads die Unterbrechung nicht möglich oder nicht sinnvoll sein.
Bei bestimmten CI-/CD-Läufen oder Kompilierungsprozessen überwiegt die benötigte Energie zum Starten und Stoppen der Workloads der erzielbaren Einsparung, weil sie oft eine lange Setup- und Tear-Down-Zeit haben.
Bei Datenbankmigrationen oder -backups gilt es, Dateninkonsistenzen zu vermeiden, weshalb ein Abbruch nicht möglich ist.
Aufgrund der Notwendigkeit, solche Workloads am Stück auszuführen, sind diese weniger flexibel, wenn es um die Vermeidung von Emissionshöchstwerten des Stromnetzes geht~\cite{Wiesner.2021}.

Als nächstes sollen die Voraussetzungen für örtliche Verschiebungen analysiert werden.
Hürden für diese Art von Verschiebung können Regulierungen, wie zum Beispiel Datenschutzgesetze sein~\cite{Sukprasert.2023}~\cite{Koningstein.18.5.2021}.
Darüber hinaus kann die Möglichkeit der örtlichen Verschiebung durch die Verfügbarkeit der Datencenter des verwendeten Cloud-Anbieters limitiert sein.
Um einen Workload in eine wegen niedriger Kohlenstoffintensität ausgewählte Region verschieben zu können, muss der Cloud-Anbieter Rechenzentren in dieser Region stationiert haben.
Zusätzlich müssen diese Rechenzentren ausreichend Ressourcen verfügbar haben.
Eine wichtige Rolle neben der Kapazität spielen Latenzbeschränkungen.
Bestimmte Latenzziele, wie Anforderungen an die Antwortzeit führen zu einer geringen Toleranz gegenüber Latenzzeiten und schränken das Potenzial der räumlichen Verlagerung stark ein.
Ideale Voraussetzungen bieten beispielsweise interaktive Anfragen von webbasierten Diensten oder Inferenz-liefernde Machine-Learning Systeme.
Diese Workloads sind für gewöhnlich nicht datenabhängig, wodurch ihre Anfragen an jedem beliebigen Ort weltweit bearbeitet werden kann, vorausgesetzt die zeitlichen Beschränkungen des Nutzers werden nicht verletzt.
Vor dem Hintergrund, dass die Kohlenstoffintensität verschiedener Länder täglich, wöchentlich oder je nach Saisonalität schwankt, könnte statt einer einmaligen Verlagerung eines Workloads auch die mehrfache Verlagerung in verschiedene Rechenzentren zu Emissionsreduzierungen führen.
Für diesen Anwendungsfall ist vor allem die Dauer und die zusätzlich benötigte Energie für die Verlagerung entscheidend~\cite{Sukprasert.2023}.
Einer ausführlichen Studie zur räumlichen Verlagerung von Workloads zur Folge~\cite{Sukprasert.2023} bringt diese Strategie, selbst unter Vernachlässigung des zusätzlichen Aufwands durch die Mehrzahl der Verschiebungen, keine besseren Einsparungen als die einmalige Verlagerung in ein grüneres Rechenzentrum.
%!
%!
\chapter{Grundlagen zur Zeitreihenprognose}\label{CAP:tsf_basics}
In diesem Kapitel sollen die Grundlagen für die Zeitreihenprognose definiert werden.
Zuerst geht es darum, festzulegen, was Zeitreihen sind, gefolgt von der Beschreibung zweier ausgewählter Methoden zur Vorhersage von Zeitreihen.
Diese Grundlagen dienen als Basis für das darauffolgende Kapitel, in dem die beiden Methoden Anwendung finden.
\section{Einführung in die Zeitreihenprognose}\label{CAP:intor-time-series-forecasting}
Eine Zeitreihe ist eine Sequenz von Datenpunkten in zeitlicher Abfolge~\cite{Peixeiro.2022}.
Diese spezielle Art von Datensatz dokumentiert, wie sich ein bestimmter Sachverhalt im Laufe der Zeit verhält.
Die wichtigste Spalte eines Zeitreihendatensatzes ist die Zeitspalte, welche die Grundlage für die Sortierung der Daten bildet.
Meist liegt zwischen den einzelnen Datenpunkten der gleiche Zeitabstand, sodass die Daten zum Beispiel in einem stündlichen, monatlichen oder jährlichen Intervall vorliegen.
Um Zeitreihendaten verarbeiten zu können, müssen oft spezielle Techniken zur Vorverarbeitung und zum Feature-Engineering angewandt werden~\cite{Lazzeri.2021}.

Eine Prognose ist eine Vorhersage zukünftiger Zustände und Entwicklungen, basierend auf Daten aus der Vergangenheit und mithilfe von Wissen über mögliche künftige Ereignisse, von denen die Entwicklung beeinflusst werden kann~\cite{Peixeiro.2022}.

Obwohl die Methodik der Zeitreihenprognose oberflächlich einer klassischen Regressionsanalyse ähnelt, indem sie vergangene Daten heranzieht, um zukünftige Werte als Funktion der Vergangenheit zu modellieren, weist sie doch wesentliche Unterscheidungsmerkmale auf.
Insbesondere die sequentielle Reihenfolge und die Möglichkeit, nicht zwingend über andere Merkmale als die Zielvariable verfügen zu müssen, unterscheidet sie von anderen Regressionsaufgaben.
Es ist durchaus üblich, dass eine Zeitreihe lediglich aus einer Zeitkomponente und einer dazugehörigen Wertspalte besteht, da die Zeitdimension alleine die Zielvariable definieren kann.
Die zeitliche Reihenfolge einzuhalten ist entscheidend, denn sie zeichnet die Beziehung der Werte untereinander aus und ist damit essentiell für die Gültigkeit der prognostizierten Ergebnisse~\cite{Peixeiro.2022}.

Eine Zeitreihe lässt sich über den Prozess der Dekomposition in drei Komponenten untergliedern:
Trend, Saisonalität und Residuen.
Der Trend beschreibt die langfristigen Veränderungen und Tendenzen in einer Zeitreihe, während die Saisonalität sich wiederholende Zyklen über einen bestimmten Zeitraum darstellt.
Residuen, auch als Rauschen bezeichnet, repräsentieren nicht durch den Trend oder die Saisonalität abzubildende, zufällige Schwankungen.
Die Analyse von Zeitreihen auf diese Weise eignet sich besonders gut, um Muster und Entwicklungen von Daten über die Zeit hinweg zu untersuchen und zu interpretieren. ~\cite{Peixeiro.2022}.
Näheres dazu wird in Kapitel~\ref{CAP:ci-analysis} beschrieben.

\section{SARIMAX als statistisches Basismodell}
Für die Zeitreihenprognose soll ein statistisches Modell namens SARIMAX als Basismodell dienen.
SARIMAX (Seasonal Auto-Regressive Integrated Moving Average with exogenous variables) kommt folgendermaßen zustande:
Für das ARMA-Modell werden die autoregressive (AR) und die Moving-Average- (MA) Komponente zusammengefügt.
Ein autoregressiver Prozess geht davon aus, dass die Zielvariable linear von ihren eigenen Werten in der Vergangenheit abhängt.
Beim Moving-Average-Prozess (zu Deutsch gleitender Durchschnittsprozess) wird der aktuelle Wert als lineare Kombination des Mittelwerts der aktuellen und vergangenen Fehlerterme dargestellt.
Das \ac{ARIMA}-Modell enthält zusätzlich noch einen Schritt (I), für die Modellierung nicht-stationärer Zeitreihen.
Das ARIMA-Modell in Abhängigkeit von der Ordnung des AR-Prozesses $p$ (linear Regression auf die letzten $p$ Werte der Zeitreihe), der Ordnung der Integration $d$ und der Ordnung des MV-Prozesses $q$ (lineare Regression auf die letzten $q$ Fehlerwerte), kann durch Formel~\ref{eq:arima} ausgedrückt werden~\cite{Peixeiro.2022}.
$p$ gibt an, wie viele zeitversetzte Werte hinzugegeben werden, wohingegen $q$ die Anzahl der zeitversetzten Fehlerterme definiert.
\begin{equation}
 \label{eq:arima}
 y'_t = C + \phi_1 y'_{t-1} + \ldots + \phi_p y'_{t-p} + \theta_1 \varepsilon_{t-1} + \ldots + \theta_q \varepsilon_{t-q} + \varepsilon_t
\end{equation}
Der aktuelle Wert der differenzierten Reihe $y'_t$ ist somit die Summe aus einer Konstanten $C$, den Vergangenheitswerten dieser differenzierten Reihe $\phi_p y'_{t-p}$, vergangenen Fehlertermen $\theta_1 \varepsilon_{t-1}$ und dem aktuellen Fehlerterm $\varepsilon_t$.
$d$ ist hier nicht explizit angegeben, definiert jedoch die Anzahl der Differenzierungen, bis eine Reihe stationär wird.

Um saisonale Muster berücksichtigen zu können, wird aus dem \ac{ARIMA}-Modell das SARIMA-Modell, welches schließlich um exogene Variablen (X), also die Zielvariable zusätzlich beeinflussende Variablen, erweitert wird.
Basierend auf Formel~\ref{eq:arima} ergibt sich Formel~\ref{eq:sarimax} für das SARIMAX-Modell.
\begin{equation}
 \label{eq:sarimax}
 d_t = C + \sum_{n=1}^{p} \alpha_n d_{t-n} + \sum_{n=1}^{q} \theta_n e_{t-n} + \sum_{n=1}^{P} \phi_n d_{t-sn} + \sum_{n=1}^{Q} \eta_n e_{t-sn} + \sum_{n=1}^{r} \beta_n x_{t-n} + \varepsilon_t
\end{equation}
Ergänzt wurde die Formel durch die Saisonalität in Form einer zusätzlichen AR- ($\phi_n d_{t-sn}$) und MA- ($\eta_n e_{t-sn}$) Komponente verschoben um die Frequenz der Saisonalität $s$ (z.B.\ 12: monatlich oder 24: stündlich) und durch die exogenen Variablen ($\sum_{n=1}^{r} \beta_n x_{t-n}$)~\cite{Artley.26.4.2022}.

Wie Daten zur Verwendung des Modells vorbereitet werden müssen und wie die beschriebenen Parameter konfiguriert werden, wird im Abschnitt~\ref{CAP:SARIMAX-training} thematisiert.

Wohingegen \ac{SARIMAX} auf einer vordefinierten Struktur basiert, ist das folgende Modell weitaus flexibler und nutzt maschinelles Lernen, um komplexe Muster und Beziehungen in den Daten zu erfassen~\cite{Lim.19.12.2019}.

\section{Temporal Fusion Transformer als KI-basiertes Modell}
Neben dem Einsatz von SARIMAX als Basismodell, soll ein sogenannter Temporal Fusion Transformer angewandt werden.

Machine-Learning- und Deep-Learning-Ansätze haben in den vergangenen Jahren aufgrund ihrer breiten Einsatzmöglichkeit bei realen Herausforderungen und ihrer Fähigkeit, praktikable Lösungen zu generieren, signifikant an Bedeutung gewonnen.
Für Zeitreihenvorhersagen sind überwiegend Deep-Learning-Verfahren für die Vorhersage in Szenarien bestehend aus einer Sequenz von einzelnen Datenpunkten konzipiert worden.
Dadurch, dass die Zeitreihen nur aus einem Attribut bestanden, tendierten einfachere und traditionellere Methoden, wie \ac{ARIMA}, in einigen Fällen besser abzuschneiden.
Die damit verbundene Fehlannahme, dass Deep-Learning-Modelle bei der Vorhersage von Zeitreihen nicht effektiv seien, rechtfertigte ihren Mehraufwand gegenüber einfacheren ansätzen nicht.
Mittlerweile hat sich jedoch gezeigt, dass \acp{RNN} in vielen Anwendungen mit Zeitreihen oder sequenziellen Daten, einschließlich Übersetzung und Spracherkennung die besten Ergebnisse erzielen~\cite{Lazzeri.2021}.

Wissenschaftlern von Google gelang 2017 eine Forschungsarbeit~\cite{Vaswani.2017}, die die Forschung zur künstlichen Intelligenz vor allem im Bereich der natürlichen Sprachverarbeitung maßgeblich beeinflusste.
Sie stellten eine neue Architektur für neuronale Netze vor, sogenannte Transformer, die zunächst nur für die Sprachverarbeitung angewendet und getestet wurden, aber als grundlegendes Konzept auch andere Bereiche revolutionierten.
Anstelle von Feedback-Schleifen durch \acp{RNN}, die üblicherweise in Encoder-Decoder-Architekturen verwendet wurden, setzten sie auf eine Methode namens \glqq Multi-headed Self-Attention\grqq{}.
Es stellte sich heraus, dass sich dieser neue Architekturansatz für verschiedenste Zwecke eignet und große Vorteile bietet, wie zum Beispiel die Parallelisierbarkeit, die Vielseitigkeit und nicht zuletzt die hohe Qualität der erzielten Ergebnisse~\cite{Vaswani.2017}.
Heute basieren bedeutende Modelle wie ChatGPT unter anderem auf Transformern.

Ein \ac{TFT} ist ein \ac{DNN} mit besonderer Architektur für das Multi-Horizon Forecasting, also die Vorhersage für mehrere zukünftige Zeitschritte.
Zunehmend wurden für diesen Anwendungsfall \acp{DNN} eingesetzt, da sie die Leistung herkömmlicher Modelle übertreffen~\cite{Lim.19.12.2019}.
Die Idee des Attention-Mechanismus, der den Kern der Transformer-Architektur bildet, wurde für \ac{TFT} übernommen.
Um die Architektur auf die Varietät der Eingabedaten und deren zeitliche Beziehungen auszurichten, werden Codierer für die statischen Kovariaten, also Variablen zusätzlich zur Zielvariable, die Informationen über diese enthalten, eingesetzt, Gating-Mechanismen und stichprobenabhängige Variablenauswahl zur Minimierung irrelevanter Eingaben, eine Sequence-to-Sequence-Schicht zur Verarbeitung der Known und Observed Inputs und ein Self-Attention Decodierer zur Analyse der Eingabedaten auf langfristige Abhängigkeiten.
Bei anderen aufmerksamkeitsbasierten Methoden, die auch Transformer-basierte Modelle mit einschließen, besteht oft das Problem, dass die verschiedenen Datenquellen (z.B. statische Kovariaten) nicht ausreichend berücksichtigt werden.
Außerdem können sie oft nicht die Bedeutung verschiedener Daten in einem bestimmten Zeitabschnitt unterscheiden.
Andere \acp{DNN} neben dem \ac{TFT} haben zum Beispiel den Nachteil, dass sie die zeitliche Anordnung der Eingangsmerkmale nicht berücksichtigen (konventionelle Formen von LIME und SHAP), obwohl diese Abhängigkeit in Zeitreihen signifikant sind~\cite{Lim.19.12.2019}.
%\ac{DNN}s werden zunehmen für diese Aufgabe herangezogen, weil sie verglichen mit herkömmlichen Zeitreihenmodellen große Leistungsverbesserungen aufzeigen.

Auch wenn der \ac{TFT} ein großes Modell ist und am besten für große Datenmengen geeignet ist, erzielt er auch für kleine Datensätze mit ca. 20 000 Samples gute Ergebnisse~\cite{Beitner.2020}.
Er verwendet als Input verschiedenste Daten, wie in Abbildung~\ref{FIG:tft-forecasting} dargestellt~\cite{Lim.19.12.2019}:
\begin{itemize}
 \item Werte der Zielvariablen aus der Vergangenheit (Past Targets)
 \item Zeitlich abhängige bekannte Informationen sowohl für die Vergangenheit, als auch für die Zukunft (Known Inputs)
 \item Zeitlich abhängige jedoch nur in der Vergangenheit bekannte Eingabewerte (Observed Inputs)
 \item Feststehende, zeitunabhängige Daten, die einen kontextuellen Zusammenhang bieten (statische Kovariaten)
\end{itemize}
\begin{figure}
 \caption[Multi-horizon Forecasting mit dem Temporal Fusion Transformer]{Multi-horizon Forecasting mit dem Temporal Fusion Transformer~\cite{Lim.19.12.2019}.}
 {\includegraphics[width=0.7\textwidth]{\figdir/TFT_multi-horizon-forecasting}}
 \label{FIG:tft-forecasting}
\end{figure}
Die Ausgabe des Modells ist nicht ein einziger Wert, sondern ein Werteintervall (Prediction Interval).
Die Multi-Horizon Vorhersage ist deshalb so komplex, weil trotz weniger Informationen darüber, das Zusammenspiel und die Abhängigkeit der Daten untereinander erlernt werden müssen~\cite{Lim.19.12.2019}.

Jeder Wert der Zielvariablen $i$ ist an eine oder mehrere statische Kovariaten $s_i$, an zeitabhängige Variablen $\chi_{i,t}$ und skalare Zielvariablen $y_{i,t}$ zu jedem Zeitschritt $t$ gebunden.
Wie in vorheriger Auflistung deutlich wurde, können die zeitabhängigen Variablen weiter unterteilt werden in Known Inputs ($x_{i,t}$) und Observed Inputs ($z_{i,t}$).
Wichtig sind außerdem sogenannte Quantile, denn sie ermöglichen die Vorhersage von Ergebnisintervallen, anstatt nur einzelner Werte und bieten auf diese Weise die Aussage über Best- und Worst-Case Werte (z.B. Ausgabe des 10., 50. und 90. Perzentil jedes Zeitschrittes)~\cite{Lim.19.12.2019}.

Formel~\ref{eq:tft} beschreibt die Prognose des $q$-ten Quantils des $\tau$-ten Vorhersageschrittes zum Zeitpunkt $t$.
Dabei werden alle Informationen innerhalb des endlichen Look-Back-Windows $k$ mit einbezogen, inklusive der Zielvariablen $y$ bis einschließlich des Startpunkts der Vorhersage $t$ ($y_{i,t-k:t} = \{y_{j,t-k}, \ldots, y_{j,t}\}$) und bekannter Variablen $x$ für den gesamten Zeitraum ($x_{i,t-k:t+\tau} = \{x_{i,t-k}, \ldots, x_{i,t}, \ldots, x_{i,t+\tau}\}$)~\cite{Lim.19.12.2019}.
\begin{equation}
 \label{eq:tft}
 \hat{y}_i(q, \tau, T) = f_q(\tau, y_{j,t-k:t}, z_{i,t-k:t}, x_{j,t-k:t+T}, s_i)
\end{equation}

Abbildung~\ref{FIG:tft-architecture} zeigt den Aufbau eines \acp{TFT}, wovon die wichtigsten Bestandteile im Folgenden kurz erklärt werden.
\begin{figure}
 \caption[Aufbau eines Temporal Fusion Transformers]{Vereinfachte Darstellung des Aufbaus eines Temporal Fusion Transformers~\cite{Labiadh.2023}.}
 {\includegraphics[width=0.90\textwidth]{\figdir/TFT-architecture}}
 \label{FIG:tft-architecture}
\end{figure}

Gating-Mechanismen in Form von \acp{GRN} verfolgen das Ziel, nicht-lineare Verarbeitung nur dort anzuwenden, wo sie nötig ist.
Sie dienen dazu, ungenutzte Komponenten zu überspringen, sodass die Tiefe und Komplexität des Netzwerkes anpassungsfähig ist, um ein breites Spektrum an Szenarien und Datensätzen abdecken zu können~\cite{Lim.19.12.2019}.

\acp{VSN} dienen dazu, die relevanten Input-Variablen für jeden Zeitschritt auszuwählen.
Dadurch, dass eine Vielzahl von Variablen verfügbar ist, ist es essenziell, die Relevanz und den Beitrag der einzelnen Variablen zu untersuchen.
Die Variablen Auswahl, in der Architektur-Abbildung unten zu finden, wird auf statische und zeitabhängige Kovariaten angewendet.
Sie hilft, die auffälligsten Merkmale herauszufinden und diejenigen, die sich negativ auf das Ergebnis auswirken, herauszufiltern.
Dadurch kann sie die Leistung des Modells erheblich verbessern~\cite{Lim.19.12.2019}.

Ein wichtiger Bestandteil sind außerdem die statischen Kovariaten-Kodierer (Static Covariate Encoders), die in Abbildung~\ref{FIG:tft-architecture} links zu finden sind.
Mit Hilfe von diesen werden verschiedene Kontextvektoren erstellt, nämlich für die Auswahl zeitlicher Variablen, die lokale Verarbeitung zeitlicher Variablen und das Anreichern von zeitlichen Merkmale mit statischen Variablen.
Diese werden an den verschiedenen Stellen im Decoder verknüpft, um statische Merkmale in das Netzwerk zu integrieren.
Das unterscheidet das \ac{TFT}-Netzwerk von anderen Architekturen für die Zeitreihenprognose, die für statische Metadaten keine separaten Kodierer verwenden~\cite{Lim.19.12.2019, Labiadh.2023}.

Sowohl kurzfristige als auch langfristige zeitliche Zusammenhänge zu erfassen ist eine der Hauptaufgaben bei der Zeitreihenvorhersage.
Um beides ausreichend abdecken zu können, werden bei einem \ac{TFT} Sequence-to-Sequence-Schichten (kurzfristige Prognosen) und ein Multi-Head-Attention-Block (langfristige Prognosen) eingesetzt.
Die Sequence-to-Sequence-Schichten sind die Alternative zur Positionskodierung, die in Transformern üblicherweise verwendet wird.
Solche Schichten überzeugen durch ihre Fähigkeit, zeitliche Muster durch rekurrente Verbindungen festzuhalten~\cite{Labiadh.2023}.
Der in anderen Transformer-Architekturen verwendete Multi-Attention-Block wurde leicht abgewandelt, um die zeitlichen Beziehungen über verschiedene Zeitschritte zu erlernen.
Dafür wird ein sogenannter Self-Attention-Mechanismus eingesetzt, der es ermöglicht, Beziehungen über mehrere Zeitschritte hinweg zu erlernen.
Bei diesem wurde im Vergleich zu anderen Transformer-basierten Architekturen die Erklärbarkeit verbessert~\cite{Lim.19.12.2019}.
Er bewertet die Wichtigkeit einzelner Werte anhand von Beziehungen zwischen Schlüsseln und Abfragen~\cite{Labiadh.2023}.

Besonders ist zudem die beim \ac{TFT} standardmäßig verwendete Loss-Funktion.
Ein \ac{TFT} wird dadurch trainiert, indem die Quantilverluste über alle prognostizierten Quantile hinweg minimiert werden.
Diese Quantile geben Auskunft über die Verteilung des Zielwerts.
Die verwendete spezielle Quantile Loss-Funktion, auch Pinball-Verlust genannt, wird durch die Formel~\ref{eq:tft-quantile-loss} beschrieben~\cite{Labiadh.2023}.
\begin{equation}
 \label{eq:tft-quantile-loss}
 QL(y, \hat{y}, q) = q \max(0, y - \hat{y}) + (1 - q) \max(0, \hat{y} - y)
\end{equation}
Diese Funktion gewichtet Unterschätzungen, also wenn die Prognose $\hat{y}$ kleiner als der tatsächliche Wert $y$ ist, proportional zum Quantil $q$.
Dies bedeutet, dass für Quantile unterhalb des Medians (also $q < 0,5$) Unterschätzungen stärker bestraft werden.
Umgekehrt werden Überschätzungen, wenn $\hat{y}$ größer als $y$ ist, mit $1 - q$ gewichtet, wodurch für Quantile oberhalb des Medians Überschätzungen stärker ins Gewicht fallen~\cite{Efimov.29.10.2023}.

Die neue Form der Interpretierbarkeit eines \ac{TFT} ermöglicht ein einfaches Zurückverfolgen der relevantesten Zeitschritte für jede Prognose, was traditionelle Saisonalitäts- und Autokorrelationsanalysen ersetzen kann.
Außerdem hilft sie, signifikante Veränderungen zu erkennen, indem ein durchschnittlicher Aufmerksamkeitswert pro Vorhersagebereich berechnet wird und zu jedem Zeitpunkt als Vergleich dient~\cite{Labiadh.2023}.

Die Vorteile der Architektur eines \ac{TFT} sind unter anderem die Möglichkeit zur Verwendung der Kovariaten, was auch dazu führt, dass es auch für kurze Zeitreihen gute Ergebnisse erzielen kann.
Weitere Vorteile sind, dass mehrere Ziele gleichzeitig und sogar heterogene Ziele mit kontinuierlichen und kategorischen Variablen, also Regression und Klassifikation zur selben Zeit, unterstützt werden.
Die Anwendung von sowohl Regression als auch Klassifikation unterscheidet ihn zum Beispiel von DeepAR, einem Modell zur Zeitreihenprognose von Amazon, welches ausschließlich die Regression anwendet.
Der \ac{TFT} berücksichtigt zudem die Unsicherheit in Form von Quantilen, was ihn von RecurrentNetwork mit einfacher \ac{LSTM}- oder \ac{GRU}-Schicht und Ausgabeschicht oder von NBeats und NHiTS unterscheidet.
Durch die Nutzung statischer Kovariaten kann der \ac{TFT} auch mit sogenannten \glqq Cold-Start-Problemen \grqq{}, also Prognosen ohne oder mit nur sehr wenigen historischen Daten, umgehen.
Die komplexe Architektur kann mit Blick auf Anforderungen an die Rechenleistung zum Nachteil werden.
Im Vergleich zu anderen Aufgaben zum Beispiel im Bereich Computer Vision oder Sprachverarbeitung sind Zeitreihendaten meist kleiner, wodurch eine höhere Komplexität des verwendeten Modells den positiven Effekt haben kann, dass die verwendete Hardware ausgereizt wird~\cite{PytorchForecastingDocumentation.20230410T20:05:43.000Z}.
Dies ist auch im Hinblick auf Nachhaltigkeit ein guter Ansatz, weil die Hardware effizient genutzt wird.
%!
%!
\chapter{Prognose der Kohlenstoffintensität}\label{CAP:prediction}
\section{Auswahl und Vorverarbeitung der Daten}\label{CAP:data-preparation-analysis}
Allgemein hängt die Kohlenstoffintensität von Strom stark von der Verfügbarkeit erneuerbarer Energien ab und diese, vor allem Wind- und Solarenergie sind wiederum auf bestimmte Wetterverhältnisse angewiesen.
Um präzise Vorhersagen zu treffen, ist es daher wichtig, sowohl historische Kohlenstoffintensitätswerte zu analysieren, als auch den Einfluss des Wetters zu berücksichtigen.
Auf dieser Grundlage kann dann eine möglichst genaue Prognose für zukünftige Zeitpunkte erstellt werden.

Als Datengrundlage für die Kohlenstoffintensität dient das Datenmodell zur \ac{MOER} von WattTime.
Die \ac{MOER} wird für einen bestimmten Zeitpunkt und Ort angegeben und kann dadurch gut verglichen werden.
Wie in Kapitel~\ref{CAP:ci} beschrieben, ist für den vorliegenden Anwendungsfall die durchschnittliche Kohlenstoffintensität des Stromnetzes, die sich schlichtweg aus Anteil der Energiequellen multipliziert mit dem Intensitätsfaktor der jeweiligen Energiequelle errechnen lässt, das falsche Maß.
Realistische Werte für die marginale Kohlenstoffintensität zu errechnen, ist jedoch weitaus anspruchsvoller.
Die Schwierigkeit bei der Datenerfassung zur \ac{MOER} besteht darin, dass zwei Fälle miteinander verglichen werden müssen.
Zum Einen, dass eine Lastverschiebung stattfindet und zum Anderen dass sie nicht stattfindet, von denen nicht beide gleichzeitig umgesetzt werden können.
Durch schwankende Bedingungen im Stromnetz ändert sich die \ac{MOER} ständig, weshalb man den Fall der Lastverschiebung und den Fall, in dem keine Last verschoben wird, nicht einfach hintereinander ausprobieren kann.
Eine Lösung dieses Problems ist, auch die Netzbedingungen zu betrachten und davon auszugehen, dass bei gleichen Bedingungen auch die \ac{MOER} gleich ist.
Diesen Ansatz nutzt WattTime und überprüft dessen Genauigkeit durch verschiedenste Experimente~\cite{WattTime.2022}.
Die Daten werden durch eine sogenannte Binned Regression modelliert.
Diese unterscheidet sich von einer Linearen Regression darin, dass die Daten in Bereiche unterteilt werden, um \glqq Bins\grqq{}, also wörtlich Behälter zu erstellen.
Der eigentliche Datenwert wird dann durch den Bereich, in den er fällt, ersetzt.
Auf diese Weise kann das \glqq Binning\grqq{} dem linearen Modell helfen, nichtlineare Beziehungen in den Daten besser abzubilden~\cite{Muller.2017}.
Bei WattTime passt sich das Binning-basierte Modell an ein lineares Modell zwischen Emissionen und Last an und ergibt so die marginale Emissionsbeziehung~\cite{WattTime.2022}.
% Gegenüber reinen Differenzierungsmodellen bietet es den Vorteil, dass kleine Laständerungen nicht zu starken Verzerrungen führen.
Das Datenmodell basiert auf historischen Zeitreihendaten der in Tabelle~\ref{TAB:moer-sources} aufgelisteten Quellen~\cite{WattTime.20231121T18:47:09+00:00}.
\begin{table}[t]
 \caption[Datenquellen MOER Modellierung]{Die Datenquellen, die WattTime für die Modellierung der MOER verwendet (\cite{WattTime.20231121T18:47:09+00:00})}
 \label{TAB:moer-sources}
 \input{\tabledir/MoerSources.tex}
\end{table}
% pro kWh wird soviel mehr g CO2 ausgestossen
% Electricity Maps Beitrag zu MOER !

Die Wahl für Deutschland und Norwegen als die beiden Länder, für die die Kohlenstoffintensität prognostiziert werden soll, wurde aufgrund folgender Kriterien getroffen:
\begin{itemize}
 \item \textbf{Repräsentativer Charakter}: In die Entscheidung fließt die Verfügbarkeit von Rechenzentren der drei größten öffentlichen Cloud-Anbieter - \ac{AWS}, Microsoft Azure und \ac{GCP} - in diesen Ländern ein.
 Alle drei Anbieter haben Verfügbarkeiten in Deutschland.
 In Norwegen sind zum jetzigen Stand keine Datencenter von \ac{AWS} stationiert.
 Google kündigt für \ac{GCP} an, demnächst in dieser Region verfügbar zu sein und Microsoft Azure haben dort bereits Datencenter~\cite{AmazonWebServices.20240318T18:30:19.000Z}\cite{GoogleCloud.20240311T07:04:17.000Z}\cite{Microsoft.20240307T01:34:37.000Z}.
 \item \textbf{Verfügbarkeit}: Für beide Länder bietet WattTime eine ausreichende Datenverfügbarkeit.
 Ziel war es, eine Datenbasis von mindestens einem kompletten Jahr für das Training zu haben.
 Für Deutschland ist dieses Ziel mit Beginn der Datenerfassung im Oktober 2022 gegeben und für Norwegen sind mehr als ausreichend Daten vorhanden (Beginn der Datenerfassung ist Januar 2021).
 \item \textbf{Diversität}: Durch die Auswahl von Deutschland und Norwegen können zwei Stromnetze mit unterschiedlicher Zusammensetzung der Energiequellen (mehr dazu im Verlauf dieses Kapitels) analysiert werden.
 Neben Deutschland mit einem im internationalen Vergleich mittelhohen Niveau an Kohlenstoffintensität, wird mit Norwegen eines der Länder mit der niedrigsten Kohlenstoffintensität für eine gute Vergleichbarkeit gewählt~\cite{ElectricityMaps.20240305T20:54:29.000Z}.
\end{itemize}
Die \ac{MOER}-Werte werden über die \ac{API} von WattTime pro Monat für die Länder Deutschland und Norwegen abgefragt.
Um auf die Daten zugreifen zu können, registrieren sich berechtigte Nutzer mit ihrer E-Mail-Adresse über einen Registrierungsendpunkt und loggen sich anschließend über einen Login-Endpunkt ein, um einen Token zu erhalten.
Dieser Token ist eine halbe Stunde lang gültig und ermöglicht durch Setzen im Header die Abfrage der Daten über die \ac{API}~\cite{.20240220T17:59:19.000Z}.
Der Vorgang der Datenabfrage und Speicherung ist in Code-Ausschnitt~\ref{CODE:watttime_get_moer} zu sehen.
Die Daten werden ab dem frühst möglichen Zeitpunkt, also dem Zeitpunkt ab dem WattTime mit der Datenerfassung begonnen hat, abgefragt.
Für Deutschland ist das ab Oktober 2022 und für Norwegen ab Januar 2021.
Sie konnten nur monatsweise abgefragt werden, weil die Abfrage sonst zu lange dauern würde und der Token während der Abfrage ungültig wird.

Zur Verwendung der Daten für die Vorhersage sind ein paar Vorverarbeitungsschritte nötig.
Allgemeine Vorverarbeitungsschritte werden an dieser Stelle beschrieben, wohingegen modellspezifische Anpassungen der Daten im jeweiligen Abschnitt zur Modellentwicklung behandelt werden.
Die Daten stehen in einem Intervall von fünf Minuten zur Verfügung, was insgesamt zu einer sehr großen Datenmenge führen würde, z.B. für Deutschland $12 * 24 * 517 = 148 896$ Datenpunkte.
Außerdem wird ein stündliches Intervall für die Vorhersage als ausreichend angesehen.
Die \ac{MOER}-Werte werden von der Einheit Pfund (lbs) pro Megawattstunde (MWh) in Gramm (g) pro \ac{kWh} umgerechnet und die Informationen über die Zeitzone werden entfernt, da sich die Daten bereits in \ac{UTC} befinden.
Diese Zeitzone wird als Standard für alle Daten verwendet, sodass später ein einfaches Zusammenfügen aus einzelnen Datenquellen und Vergleichen der Daten untereinander ohne Konvertierung möglich ist.
Die \ac{MOER}-Werte der beiden Länder werden separat in eine CSV-Datei eingelesen.

Als beeinflussende Variablen werden Wetterdaten zum Datensatz hinzugefügt.
Dafür wird auf den Copernicus Climate Change Service~\cite{Copernicus.20231212T14:09:40.000Z} zurückgegriffen, der umfangreiche Datensätze rund um das weltweite Klima und den Klimawandel zur Verfügung stellt.
Konkret wurden vier mögliche Einflussfaktoren für die Kohlenstoffintensität auf Basis der Energiequellen-Zusammensetzung, die im nächsten Kapitel näher betrachtet wird, als wichtig identifiziert und ausgewählt:
die Lufttemperatur in 2m Höhe, die Windgeschwindigkeit in 100m Höhe, der Niederschlag und die globale horizontale Bestrahlungsstärke (im Englischen als \ac{GHI} bezeichnet).
Letztere wird durch die atmosphärische Zusammensetzung mit Aerosolen, Wasserdampf und Ozon, hauptsächliche jedoch von Wolken beeinflusst~\cite{KallioMyers.2020}.
Für die Stromerzeugung durch Photovoltaik ist in erster Linie die Sonneneinstrahlung, die auf das Solarmodul einfällt, entscheidend~\cite{James.}.
Die Erzeugung durch Windkraft hängt hauptsächlich von der Windgeschwindigkeit ab.
Für die Wasserkraft ist unter anderem der Niederschlag ausschlaggebend~\cite{Copernicus.20231212T14:09:40.000Z}.
Bei den Werten handelt es sich um Daten des ERA5 Reanalysis Datensatzes.
Dies sind von verschiedenen Quellen, wie Satelliten oder Wetterstationen, gemessene Daten, bei denen fehlende Werte mit durch physikalische Gesetze oder historische Verlaufsmuster generierten Daten ergänzt wurden.
Grundsätzlich sind die Daten also echt gemessene Daten, die zur Vollständigkeit mit generierten Daten ergänzt wurden~\cite{CopernicusClimateChangeService.2020}\cite{CopernicusKnowledgeBase.20231009}.
Die durchschnittlichen Werte über den Datenzeitraum (Oktober 2022 bzw. Januar 2021 bis Februar 2024) pro Monat sind in Abbildung~\ref{FIG:weather-data-by-month} zusammengefasst.
\begin{figure}[ht]
 \centering
 \subfloat[\centering GHI]{{\includegraphics[width=.5\textwidth]{\figdir/ghi_by_month} }}%
 \subfloat[\centering Niederschlag]{{\includegraphics[width=.5\textwidth]{\figdir/precipitation_by_month} }}%
 \par\medskip
 \subfloat[\centering Temperatur]{{\includegraphics[width=.5\textwidth]{\figdir/temperature_by_month} }}%
 \subfloat[\centering Windgeschwindigkeit]{{\includegraphics[width=.5\textwidth]{\figdir/wind-speed_by_month} }}%
 \caption[Durchschnittswerte der Wetterdaten]{Die Durchschnittswerte der verwendeten Wetterdaten pro Monat (eigene Darstellung)}%
 \label{FIG:weather-data-by-month}%
\end{figure}
Die \ac{GHI}, zu sehen in Abbildung~\ref{FIG:weather-data-by-month}a, steigt in beiden Ländern ab Januar bis zum Höhepunkt im Sommer und fällt dann wieder gegen Ende des Jahres ab.
In Deutschland ist die Sonneneinstrahlung im Sommer stärker als in Norwegen.
Die Abbildung des durchschnittlichen Niederschlags pro Monat (Abbildung~\ref{FIG:weather-data-by-month}b) zeigt, dass sich das der Verlauf und die Höhe der Werte pro Land unterscheiden.
In Deutschland ist ein ausgeprägter Ausschlag im Juni und ein weiterer im Oktober zu erkennen.
In Norwegen hingegen ist der Niederschlag gleichmäßig über das Jahr verteilt, mit Spitzen im Juli und September.
In Abbildung~\ref{FIG:weather-data-by-month}c ist die durchschnittliche Temperatur der beiden Länder abgebildet.
Sowohl in Deutschland, als auch in Norwegen, steigen die Temperaturen von Januar bis Juli und sinken anschließend wieder ab.
Deutschland verzeichnet durchweg wärmere Monate als Norwegen, was auf die unterschiedlichen Klimazonen der beiden Länder zurückzuführen ist.
Die vierte der verwendeten Wettervariablen ist die Windgeschwindigkeit, dargestellt in Abbildung~\ref{FIG:weather-data-by-month}d).
In Deutschland sind die Windgeschwindigkeiten besonders im Winter höher und zeigen einen Rückgang während des Sommers.
In Norwegen gibt es weniger saisonale Schwankungen in der Windgeschwindigkeit, aber es ist ein Anstieg zum Ende des Jahres hin zu erkennen.

Wie in ähnlichen Arbeiten (\cite{Giacomazzi.06202023},~\cite{Labiadh.2.8.2023}) üblich, werden zum Datensatz zeitliche Variablen hinzugefügt, die den MOER-Wert ebenfalls beeinflussen können:
Stunde, Wochentag, Jahreszeit und, ob es sich um einen Wochenend- oder Feiertag handelt.
Eine Übersicht über die verwendeten Variablen inklusive deren Wertebereich und Horizont zeigt Tabelle~\ref{TAB:features}.
\begin{table}[ht]
 \centering
 \begin{threeparttable}
  \caption{Verwendete Variablen mit Wertebereich und Horizont}
  \label{TAB:features}
  \input{\tabledir/Features.tex}
  \begin{tablenotes}
   \item[a] G: Bekannt bis zur Gegenwart, Z: Bekannt in der Zukunft
  \end{tablenotes}
 \end{threeparttable}
\end{table}

Um möglichst genaue Vorhersagemodelle konzipieren zu können, ist es generell notwendig, alle die Zielvariable beeinflussenden Variablen als Eingabe zu verwenden.
Haben diese jedoch eine gegenseitige Abhängigkeit oder Einfluss aufeinander, so können sie nicht als Eingabevariablen verwendet werden.
Für den betrachteten Anwendungsfall kann eine gegenseitige Beeinflussung für das Wetter und für die Zeitvariablen ausgeschlossen werden.
Die verschiedenen Wetterdaten sind nicht von der \ac{MOER} abhängig und die Zeit ist sowieso feststehend.
Bei der Stromnachfrage, den Preisen und den Drosselungsmengen ist die Beurteilung weniger trivial.
Aus diesem Grund wurde für die vorliegende Arbeit entschieden, ausschließlich die Wetter- und Zeitvariablen heranzuziehen.
Wohingegen das Wetter einen direkten Einfluss auf die Energieerzeugung vor allem durch Solaranlagen und Windparks hat, ist der Zusammenhang mit Nachfrage, Preis und Drosselung nicht eindeutig.
Die Reduzierung der Eingabevariablen auf Wetter- und Zeitdaten dient außerdem der Komplexitätsreduktion des Modells und soll sicherstellen, dass sich das Modell auf diese fokussieren kann.
Zusätzliche zu trainierende Parameter führen vor allem beim \ac{TFT} zu einer erhöhten Größe und Trainingsdauer des Modells, sodass ein Gleichgewicht zwischen Modellgenauigkeit und Trainingsaufwand gefunden werden muss.

Aufgrund der teilweise sehr starken Ausreißer der \ac{MOER} vor allem in Deutschland, was im folgenden Abschnitt noch detaillierter thematisiert wird, soll ein zusätzlicher Datensatz erstellt werden, bei dem die Extremen entfernt werden.
Laut WattTime sind diese Ausreißer höchstwahrscheinlich durch Drosselungen verursacht\cite{WattTime.2022}.
Da diese Phenomene in den Daten möglicherweise eine Schwierigkeit bei der Vorhersage sein können, ist es interessant, einen Vergleich zwischen den gefilterten und den ungefilterten Daten anzustellen.
Dafür wird das 25. und das 75. Perzentil der \ac{MOER}-Werte berechnet, also die Werte, unter denen 25\% bzw. 75\% der Datenpunkte liegen.
Als Ausreißer werden diejenigen Werte betrachtet, die nicht den mittleren 50\% entsprechen.
Auf diese Weise werden für den gefilterten Datensatz 2630 der norwegischen und 882 der deutschen Datenpunkte entfernt.
Der Mittelwert verschiebt sich dadurch für Norwegen von 246 auf 237 und für Deutschland von 747 auf 766.

\section{Analyse der Kohlenstoffintensität des deutschen und norwegischen Stromnetzes}\label{CAP:ci-analysis}
%tbd: Analyse des Potenzials zeitlicher Verschiebungen, Analyse des Potenzials örtlicher Verschiebungen

Die Daten zur \ac{MOER} sollen zunächst für die beiden Länder analysiert werden, um ein Bild über deren Beschaffenheit zu erhalten.
Zwischen den beiden Ländern ist ein deutlicher Unterschied für die MOER-Werte zu erkennen.
In Abbildung~\ref{FIG:moer_distribution} wird ersichtlich, dass Norwegens Werte weitaus niedriger sind, als die Deutschlands.
\begin{figure}
 \caption[Verteilung MOER Deutschland und Norwegen]{Die Verteilung der MOER von Deutschland und Norwegen (eigene Darstellung)}
 {\includegraphics[width=0.8\textwidth]{\figdir/moer_distribution}}
 \label{FIG:moer_distribution}
\end{figure}
Der Mittelwert Deutschlands ist mit rund 747g/\ac{kWh} mehr als dreimal so hoch wie der Mittelwert Norwegens (rund 246g/\ac{kWh}).
Bei Betrachtung der Häufigkeit der Werte muss berücksichtigt werden, dass für die beiden Länder nicht die gleiche Anzahl an Datenpunkten vorliegt.
Für Deutschland liegen insgesamt 11046 Werte und für Norwegen 26340 Werte vor.

Eine Zerlegung in Trend, Saisonalität und Noise, wie in Kapitel~\ref{CAP:intor-time-series-forecasting} beschrieben, soll mehr Aufschluss über die Daten geben.
Bei einem additiven Modell, um das es sich bei den vorliegenden Daten handelt, ist die Dekomposition folgendermaßen beschrieben:
$Y[t] = T[t] + S[t] + e[t]$, wobei $T$ der Trend, $S$ die Saisonalität und $e$ ein Fehlerwert (Noise) ist.
Der Trend wird durch Anwendung eines Convolutional Filter geschätzt und anschließend von der Zeitreihe entfernt.
Die Saisonalität ergibt sich durch den Durchschnitt jeder Periode dieser Zeitreihe~\cite{JosefPerktold.2023}.
Die Zerlegung für die Werte von Deutschland ist in Abbildung~\ref{FIG:moer_decomposition_DE} zu sehen.
\begin{figure}
 \caption[Dekomposition der MOER Deutschlands]{Die Dokomposition der MOER-Werte von Deutschland (eigene Darstellung)}
 {\includegraphics[width=0.99\textwidth]{\figdir/moer_decomposition_DE}}
 \label{FIG:moer_decomposition_DE}
\end{figure}
Bei der Trend-Komponente ist zu erkennen, dass die Werte zwar teils starke, teils weniger starke Schwankungen haben, jedoch über die Zeit kein klarer Auf- oder Abwärtstrend erkennbar ist.
Für die Saisonalität wird lediglich ein Ausschnitt der Daten (Oktober 2022 bis Dezember 2022) abgebildet, weil sonst die Anzahl der Datenpunkte zu groß wäre, um die Saisonalität zu erkennen.
Die Daten weisen deutlich eine tägliche Saisonalität auf, weil sich je Tag immer der gleiche Zyklus wiederholt.
Zuerst fällt der Wert stark ab und steigt anschließend wieder stark an.
Die Residuen zeigen deutliche, durch Trend und Saisonalität nicht abgedeckte Schwankungen, doch insgesamt, dass die Daten stationär sind.
Für Norwegen zeigt die Dekomposition (siehe Abbildung~\ref{FIG:moer_decomposition_NO}) ähnliche Muster auf.
Die \ac{MOER}-Werte haben keinen klaren Auf- oder Abwärtstrend.
Sie verfügen ebenfalls über ein sich täglich wiederholendes Muster, dessen Verlauf sich jedoch von dem der deutschen Werte unterscheidet.
Das Rauschen ist erneut sehr stark ausgeprägt und zeigt, dass viele Schwankungen nicht durch die Trend- und Saisonalitätskomponente abgedeckt werden können.

%Interessant ist unter anderem der wöchentliche Verlauf der \ac{MOER}.
%Anhand dessen kann das Potenzial der Verschiebung innerhalb einer Woche analysiert werden.
%In Abbildung~\ref{FIG:moer_weekly_cycle} ist der durchschnittliche wöchentliche Verlauf der \ac{MOER} für Deutschland und Norwegen dargestellt.
%\begin{figure}
% \caption{Der wöchentliche Verlauf der MOER des deutschen und norwegischen Stromnetzes (eigene Darstellung)}
% {\includegraphics[width=0.8\textwidth]{\figdir/moer_weekly_cycle}}
% \label{FIG:moer_weekly_cycle}
%\end{figure}
%In beiden Ländern können wöchentliche Muster und zyklische Trends beobachtet werden.
%Zwar haben die Werte in Deutschland Mitte der Woche ihren Höhepunkt und am Wochenende ihre Tiefpunkte, doch sind die Unterschiede zwischen Wochentagen und Wochenende nur sehr gering
%In Norwegen sind die täglichen Schwankungen weniger ausgeprägt und die Werte sind generell auf einem weitaus niedrigeren Niveau.

Für Vorhersagen der \ac{MOER} soll die Zusammensetzung der Stromproduktion betrachtet werden, um wichtige Faktoren ausfindig machen zu können.
Die Energiequellen der Stromproduktion in Deutschland und Norwegen im Jahr 2023 sind in Tabelle~\ref{FIG:energysources-de-no} aufgelistet.
\begin{figure}
 \caption[Energiequellen der Strompoduktion von Deutschland und Norwegen 2023]{Der Anteil der Energiequellen an der Stromproduktion in Deutschland und Norwegen im Jahr 2023 (eigene Darstellung, nach~\cite{ElectricityMaps.20240305T20:54:29.000Z})}
 {\includegraphics[width=0.99\textwidth]{\figdir/energysources-de-no}}
 \label{FIG:energysources-de-no}
\end{figure}
Ersichtlich wird, dass Deutschland eine Vielfalt an Energiequellen einschließlich Kohle, Gas, Kernenergie und erneuerbaren Energien hat.
Norwegen ist durch einen sehr großen Anteil fast vollständig auf Wasserkraft als erneuerbare Energiequelle angewiesen.
Der Strommix verschiedener Länder unterscheidet sich aufgrund des jeweiligen Klimas und des Zugangs zu natürlichen Ressourcen.
Regionen mit großen Flüssen haben oft einen hohen Anteil an Wasserkraft.
Die stärkeren Schwankungen in den Werten des deutschen Stromnetzes sind vermutlich auf die Schwankungen in der Verfügbarkeit von Solar- und Windenergie zurückzuführen.
Regionen mit einem hohen Anteil dieser beiden Energiequellen zeigen tendenziell eine größere Schwankung in der Kohlenstoffintensität auf~\cite{Sukprasert.2023}.

Interessant ist unter anderem der monatliche Verlauf der \ac{MOER}, zu sehen in Abbildung~\ref{FIG:moer_trend_month}.
\begin{figure}
 \caption[Monatlicher Verlauf der MOER von Deutschland und Norwegen]{Der monatliche Verlauf der MOER des deutschen und norwegischen Stromnetzes (eigene Darstellung)}
 {\includegraphics[width=0.9\textwidth]{\figdir/moer_trend_month}}
 \label{FIG:moer_trend_month}
\end{figure}
Erneut ist das unterschiedliche Niveau beider Länder zu beachten.
Darüber hinaus werden saisonale Schwankungen deutlich.
Intuitiv wäre zu erwarten, dass in Deutschland die \ac{MOER} in Monaten mit starkem Wind und starker Sonneneinstrahlung am niedrigsten ist.
Diese Erwartung wird für Januar (viel Wind) und Juli (viel Sonne) erfüllt.
Vergleicht man die \ac{MOER} der beiden Länder mit dem in Abbildung~\ref{FIG:weather-data-by-month} dargestellten Verlauf des Wetters, können ein paar Parallelen zwischen Wetter und Kohlenstoffintensität gezogen werden.
Zu einigen Zeitpunkten besteht jedoch kein Zusammenhang, was auf die vielschichtige Zusammensetzung der \ac{MOER} zurückzuführen \textbf{ist}.
Die Berechnung dieser Werte ist weitaus komplexer, als dass man ihren Verlauf lediglich auf das Wetter zurückführen kann.
Das deutsche Stromnetz besteht zu einem großen Anteil aus Wind- und Sonnenenergie (vgl. Abbildung~\ref{FIG:energysources-de-no}).
Die Sonneneinstrahlung in Deutschland erreichte ihre Höchstwerte in den Monaten Mai bis Juli und die Windgeschwindigkeit in den Monaten November bis März.
Das könnten Gründe dafür sein, wieso im Januar und im Juli die \ac{MOER} ihre Tiefstwerte erreicht.
Die hohen Werte im April, Mai und September könnten unter anderem auf die geringe Windstärke in diesen Monaten zurückzuführen sein.
Das norwegische Stromnetz wird von Windkraft dominiert und erhält knapp 10\% seines Stroms aus Windkraftanlagen.
Der Niederschlag war im betrachteten Zeitraum in den Monaten Juli bis Oktober und Januar am stärksten und der Wind in den Monaten Oktober bis März.
Kombiniert könnten diese Faktoren die niedrige \ac{MOER} von August bis Dezember erklären.

In einer Correlation Matrix soll analysiert werden, ob und wie die einzelnen Variablen des Datensatzes zusammenhängen.
Für Deutschland und Norwegen wird je eine dieser Matrizen erstellt, zu sehen in Abbildung~\ref{FIG:correlation_matrix}.
\begin{figure}
 \caption[Correlation Matrix der Eingabevariablen für die Prognose]{Der Zusammenhang zwischen den verwendeten Variablen für Deutschland und Norwegen dargestellt in je einer Correlation Matrix (eigene Darstellung)}
 {\includegraphics[width=0.99\textwidth]{\figdir/correlation_matrix_new}}
 \label{FIG:correlation_matrix}
\end{figure}
Die Beziehungen der Werte werden durch Koeffizienten repräsentiert, die von -1 bis +1 reichen können.
Ein Wert nahe +1 bedeutet eine starke positive Korrelation, ein Wert nahe -1 eine starke negative Korrelation und ein Wert nahe 0 bedeutet einen nur sehr schwachen oder keinen Zusammenhang.
Insgesamt geht hervor, dass der lineare Zusammenhang zwischen den \ac{MOER}-Werten, also der Zielvariable, und den anderen Variablen eher gering ist.
Der größte Zusammenhang besteht mit der Windgeschwindigkeit (-0,20) und der Sonneneinstrahlung (-0,24) in Deutschland.
Das bedeutet, dass sich bei höheren Werten für diese beiden Variablen, niedrigere Werte für die \ac{MOER} verzeichnen lassen.
Vor dem Hintergrund der Energiequellen des deutschen Stromnetzes scheint die Interpretation sinnvoll, dass bei steigender Windstärke und Sonneneinstrahlung mehr Strom durch Solarstrom und Strom aus Windkraftwerken gewonnen werden kann und somit die Emissionsgröße sinkt.
Eine schwache Abhängigkeit besteht auch zum Wochentag und zur Variable, die aussagt, ob es sich um einen Wochenend- oder Feiertag handelt.
Für Norwegen zeigt der Tag des Jahres und die Windgeschwindigkeit die höchsten Abhängigkeiten aller analysierten Werte, die mit Werten von -0,12 und -0,10 jedoch trotzdem nicht hoch sind.

\section{Modellentwicklung und Training mit SARIMAX}\label{CAP:SARIMAX-training}
Für die Verwendung von auf \ac{ARIMA} basierenden Modellen ist es zu aller Erst nötig, die Daten auf Stationarität zu untersuchen, weil diese eine Voraussetzung für die Modelle ist.
Stationär ist eine Zeitreihe dann, wenn ihr Mittelwert, ihre Varianz und ihre Autokorrelation konstant und zeitunabhängig sind~\cite{Peixeiro.2022}.
Die vorherige Dekomposition der Daten (Abbildung~\ref{FIG:moer_decomposition_DE}) weist darauf hin, dass sie stationär sind, jedoch soll diese Vermutung nochmals bestätigt werden.
Mit Hilfe eines \ac{ADF} Tests werden die Daten auf eine Einheitswurzel getestet (Nullhypothese).
Gibt es keine Einheitswurzel in der Zeitreihe (Alternativhypothese), so ist sie stationär.
Dafür wird die Implementierung vom Python Package \textit{statsmodels} verwendet, bei der die Daten lediglich in die \lstinline[columns=fixed]{adfuller} Funktion eingesetzt werden müssen.
Der Test liefert die ADF-Statistik, eine negative Zahl, die je kleiner sie ist desto stärker die Nullhypothese ablehnt und den p-Wert, der der Nullhypothese widerspricht, wenn er kleiner als 0,05 ist.
Code-Ausschnitt~\ref{CODE:adf-test_moer-de} zeigt den \ac{ADF} Test für die \ac{MOER}-Zeitreihe für Deutschland.
\lstinputlisting[language=Python, caption=Python-Code zur Überprüfung der MOER-Werte für Deutschland auf Stationarität mithilfe des ADF-Tests, label=CODE:adf-test_moer-de]{\codedir/adf-test_moer-de.m}
Das Ergebnis zeigt, dass die Nullhypothese aufgrund des p-Wertes abgelehnt wird und die Daten somit stationär sind.
Die \ac{MOER}-Werte für Norwegen sind ebenfalls stationär, jedoch ist die Ablehnung der Nullhypothese mit einer ADF-Statisik von rund -4,5 weniger stark.
Wären die Daten nicht stationär, müssten sie solange differenziert werden, bis sie Stationarität aufweisen~\cite{Peixeiro.2022}~\cite{Rahmadhan.8.5.2023}.

Zur Anwendung des SARIMAX-Modells wird die Implementierung von \textit{statsmodels} verwendet.
Ein SARIMAX-Modell muss, anders als beim \ac{TFT} mit der Möglichkeit separater Gruppen-IDs, für jedes der beiden Länder separat erstellt werden
Definiert wird das Modell durch Angabe der Zielvariable, der exogenen Variablen, der Parameter für den nicht-saisonalen Teil des Modells (\lstinline[columns=fixed]{p,d,q}) und derjenigen für den saisonalen Teil des Modells (\lstinline[columns=fixed]{P,D,Q,s}).

Die Festlegung der Parameter für \ac{SARIMAX} kann unter anderem durch Analyse der \ac{ACF} und der \ac{PACF} erfolgen.
Korrelation ist ein Maß, um die lineare Beziehung zweier Variablen zu messen.
Die \ac{ACF} misst für eine Zeitreihe diese Beziehung zwischen zwei um eine bestimmte Anzahl an Zeitschritten versetzten Werten.
Sie misst dadurch die Korrelation der Zeitreihe mit sich selbst und gibt Hinweise auf die Moving Average Komponente eines \ac{ARIMA} Modells~\cite{Peixeiro.2022}.
Das \ac{ACF}-Diagramm für die \ac{MOER}-Werte für Deutschland ist in Abbildung~\ref{FIG:acf_pacf_moer_de}a zu sehen.
\begin{figure}
 \centering
 \subfloat[\centering ACF]{{\includegraphics[width=.4\textwidth]{\figdir/acf_moer_de} }}%
 \qquad
 \subfloat[\centering PACF]{{\includegraphics[width=.4\textwidth]{\figdir/pacf_moer_de} }}%
 \caption[ACF und PACF der MOER Deutschlands]{Die ACF und PACF der MOER-Zeitreihe für Deutschland (eigene Darstellung)}%
 \label{FIG:acf_pacf_moer_de}%
\end{figure}
Die x-Achse des Diagramms repräsentiert die Anzahl der zeitlichen Verzögerungen, während die y-Achse das Ausmaß der Korrelation zwischen den Zeitpunkten darstellt.
Zu sehen ist eine Abnahme der Korrelation bei den ersten Zeitversetzungen, was auf eine starke Korrelation der zeitlich nahe beieinander liegenden Werte hinweist.
Das bedeutet, dass die Werte aus den vergangenen Stunden einen signifikanten Einfluss auf die der folgenden Stunden ausüben.
Die \ac{ACF} zeigt außerdem, dass die Daten trotz ihrer Stationarität saisonale Muster aufweisen.
Nach 24 Zeitschritten, also einem Tag, ist eine wiederkehrende Spitze in den Daten erkennbar und das schwingende Muster scheint sich mit einem täglichen Rhythmus zu wiederholen.
Dies weist auf einen täglichen Zyklus der Zeitreihe hin.
Gegen Ende der aufgezeigten Zeitverzögerungen nähern sich die Korrelationswerte der Nulllinie an und liegen innerhalb des Konfidenzintervalls (als hellblauer Bereich gekennzeichnet).
Statistisch signifikante Werte außerhalb des Konfidenzintervalls deuten darauf hin, dass die Daten mithilfe vergangener Werte vorhersagbar sind.

Die \ac{PACF} kann genutzt werden, um die autoregressive Komponente eines \ac{ARIMA}-Modells zu identifizieren.
Sie misst die Korrelation zwischen der Zeitreihe und zeitlich verschobenen Werten der Zeitreihe, jedoch unter Kontrolle der Zwischenzeitpunkte.
Dies bedeutet, dass die \ac{PACF} die direkte Korrelation zwischen Beobachtungen darstellt, die um eine bestimmte Anzahl von Zeitschritten verschoben sind, unabhängig von den Korrelationen kürzerer Verzögerungen~\cite{Peixeiro.2022}.
In der \ac{PACF} der \ac{MOER}-Werte für Deutschland (siehe Abbildung~\ref{FIG:acf_pacf_moer_de}b) ist eine starke partielle Auto-Korrelation bei einer Stunde zu beobachten.
Nach dieser ersten Zeitverzögerung sinken die Korrelationswerte sehr stark ab und nähern sich der Nulllinie, was darauf hindeutet, dass eine autoregressive Komponente erster Ordnung für das \ac{ARIMA}-Modell geeignet sein könnte.
Die näherungsweise Null-Korrelation bei höheren Zeitverzögerungen stützt diese Annahme, da keine weiteren signifikanten Korrelationen ersichtlich sind, die eine höhere Ordnung der AR-Komponente rechtfertigen würden.
Die einzelnen Ausschläge deuten auf Unregelmäßigkeiten in den Daten hin.

Die Diagramme der \ac{ACF} und \ac{PACF} für Norwegen, zu sehen in Abbildung~\ref{FIG:acf_pacf_moer_no}, weisen ähnliche Muster auf.
Die Korrelationen der \ac{ACF} werden zuerst kleiner, je höher die Zeitverzögerung ist, steigen aber anschließend wieder bis zu einem lokalen Maximum bei 24 Zeitschritten.
Die partielle Auto-Korrelation ist nach der ersten Zeitverzögerung nahe 1, was bedeutet, dass die Werte einer Stunde denen der vorherigen Stunde ähnlich sind.
Danach fällt die Korrelation stark ab, was darauf hinweist, dass der direkte Einfluss der vorherigen Messungen mit zunehmender Verzögerung abnimmt.
Bei Schritt 24 erreicht sie nochmal einen lokalen Höchstwert, was auf das periodische Muster der Daten hinweist.
Anschließend sind die Korrelationen nur noch sehr gering und liegen meist innerhalb des Konfidenzintervalls, was darauf hinweist, dass keine partielle Autokorrelation für diese Verzögerungen besteht.

Die Parameter für das SARIMAX-Modell können anhand der beiden Funktionen nicht immer eindeutig bestimmt werden.
Für \lstinline[columns=fixed]{p} sollte laut Analyse der \ac{PACF} wie erwähnt der Wert 1 verwendet werden.
Nachdem der \ac{ADF}-Test bestätigt hat, dass die Daten stationär sind, wird \lstinline[columns=fixed]{d} auf 0 gesetzt.
Aufgrund der täglichen Saisonalität der Daten wird \lstinline[columns=fixed]{s} gleich 24 gesetzt.
Bei den restlichen Parameter ist eine eindeutige Entscheidung nicht gegeben, sodass die optimalen Werte zum Beispiel durch Ausprobieren ausfindig gemacht werden können.
\lstinline[columns=fixed]{q} wäre durch einen starken Abfall der Korrelation im \ac{ACF}-Diagramm definiert, welcher im gezeigten Diagramm aber nicht ausfindig gemacht werden kann.

Eine Alternativmethode ist, verschiedene Werte für die Parameter automatisch überprüfen zu lassen.
Dafür kann die \lstinline[columns=fixed]{auto_arima} Funktion der \textit{pmdarima} Bibliothek verwendet werden (Codeausschnitt~\ref{CODE:sarimax_auto-arima}).
\lstinputlisting[language=Python, caption=Python-Code zur Parameterauswahl des SARIMAX-Modells, label=CODE:sarimax_auto-arima]{\codedir/sarimax_auto-arima.m}
Sie hilft, die optimalen Parameterwerte zu finden, indem sie die Kombination ausfindig macht, bei der das \ac{AIC} am niedrigsten ist~\cite{Rahmadhan.8.5.2023}.
Das \ac{AIC} bewertet die Qualität eines Modells im Vergleich zu anderen Modellen durch Messen der durch das jeweilige Modell verlorenen Informationen.
Ein niedrigerer \ac{AIC}-Wert bedeutet demnach, dass weniger Informationen verloren gingen und das Modell deshalb besser abschneidet.
% Für die Funktion kann unter anderem die Testmethode angegeben werden, hier wird der \ac{ADF} Test verwendet.
Für \lstinline[columns=fixed]{p} und \lstinline[columns=fixed]{q} werden Start- und Maximalwerte definiert.
Als Startwert wird für beide Parameter 0 gewählt und als Maximalwert 5~\cite{Peixeiro.2022}.
Die Frequenz wird in dieser Implementierung als Parameter \lstinline[columns=fixed]{m} angegeben, entspricht aber dem sonst verwendeten \lstinline[columns=fixed]{s} und wird der täglichen Saisonalität wegen auf 24 gesetzt.
Der Test ergibt, dass das Modell für Deutschland am besten als \lstinline[columns=fixed]{ARIMA(2,0,0)(1,0,0)[24] intercept} funktioniert.
Mit dieser Konfiguration wird laut Testergebnis der niedrigste \ac{AIC}-Wert von 43025,429 erreicht.
Für Norwegen bestimmt der Test das Modell \lstinline[columns=fixed]{ARIMA(2,0,1)(2,0,0)[24] intercept} als passendstes Modell mit einem \ac{AIC}-Wert von 36739,632.

Basierend auf diesen Ergebnissen, werden die Modelle mit diesen Parametern erstellt und trainiert, beispielhaft zu sehen in Code-Ausschnitt~\ref{CODE:sarimax_model-train}
\lstinputlisting[language=Python, caption=Python-Code zum Erstellen und Trainieren des SARIMAX-Modells für die deutsche MOER, label=CODE:sarimax_model-train]{\codedir/sarimax_model-train.m}

Verschiedene Konfigurationsmöglichkeiten wurden erprobt, darunter eine unterschiedliche Menge an Trainingsdaten.
Als am geeignetsten hat sich erwiesen, für das Training des Modells zur Vorhersage einer Woche die Datenpunkte beginnend im September 2023 zu verwenden.
Außerdem wurden verschiedene Kombinationen von exogenen Variablen probiert, was jedoch wie zu erwarten zu keinen Änderungen der Vorhersageleistung führte.
Ist eine dieser Variablen nicht aussagekräftig für die Prognose, werden sie automatisch ignoriert, weshalb alle Wetter- und Zeitvariablen herangezogen wurden.

Die verwendeten Parameter pro Modell sind in Tabelle~\ref{TAB:sarimax-hyperparameters-used} zusammengefasst.
\begin{table}[t]
 \centering\small
 \caption{SARIMAX Parameter}
 \label{TAB:sarimax-hyperparameters-used}
 \input{\tabledir/SarimaxHyperparameterUsed.tex}
\end{table}

Die Parameter der Modelle für den gefilterten Datensatz entsprechen dabei denen für den Datensatz bei dem die Ausreißer nicht entfernt wurden.
Zur Anwendung von \ac{SARIMAX} dürfen keine Datenpunkte fehlen, was nur für die ungefilterte Version der Daten gegeben ist.
Durch lineare Interpolation werden die fehlenden Werte im gefilterten Datensatz ergänzt.

\textit{statsmodels} bietet eine einfache Möglichkeit, die Diagnostik für die Residuen, also die Differenzen zwischen beobachteten und vom Modell vorhergesagten Werten, einzusehen.
Abbildung~\ref{FIG:sarimax_diagnostics_de} zeigt die Ergebnisse für das Modell der \ac{MOER}-Vorhersage des deutschen Stromnetzes.
\begin{figure}
 \caption[SARIMAX Diagnostik MOER Deutschland]{Diagnostik des SARIMAX-Modells zur Prognose der MOER des deutschen Stromnetzes (eigene Darstellung)}
 {\includegraphics[width=0.8\textwidth]{\figdir/sarimax_diagnostics_de}}
 \label{FIG:sarimax_diagnostics_de}
\end{figure}
Das Diagramm links oben zeigt die standardisierten Residuen über die Zeit.
Es wird ersichtlich, dass die Daten keinen durchgängigen auf- oder abwärts Trend haben, jedoch sind einige stärkere Ausreißer zu erkennen.
Rechts oben ist ein Histogramm der standardisierten Residuen abgebildet, das mit der Normalverteilung und einer sogenannten Kerndichteschätzung (KDE) verglichen wird.
Letztere schätzt die kontinuierliche Dichtefunktion.
An der als Referenz dienenden Linie der Normalverteilung kann die Normalität der Residuen gemessen werden.
Idealerweise sollten sich die Histogramme und die Spitzen der Dichte mit der Normalverteilung decken.
Im Diagramm wird ersichtlich, dass sich die Spitzen der Kurven unterscheiden, was darauf hindeutet, dass das Modell Schwächen aufweist.
Links unten ist das Quantil-Quantil-Diagramm dargestellt, bei dem die Quantilen der Residuen gegenüber der Quantilen einer Normalverteilung zu sehen sind.
Die Abweichungen des Modells, vor allem zu Beginn und zum Schluss hin weisen darauf hin, dass das Modell von der Normalität abweicht.
Im Diagramm rechts unten ist die Autokorrelation abgebildet, bei der lediglich bei Zeitschritt 0 ein starker Ausschlag zu verzeichnen ist.
Das bedeutet, dass die Residuen nicht korrelieren, was von einem guten Modell erwartet wird~\cite{Peixeiro.2022}.

\section{Modellentwicklung und Training mit Temporal Fusion Transformer}
Für die Implementierung des \ac{TFT} wird das Modell von \textit{pytorch forecasting}, einem Package für Zeitreihenvorhersage mit neuronalen Netzen, verwendet~\cite{PytorchForecastingDocumentation.20230410T20:05:46.000Z}.
Zunächst muss dem Zeitreihen-Datensatz ein Zeitindex in Form eines Integers hinzugefügt werden, der sich bei jedem Zeitschritt (hier eine Stunde) um eins erhöht.
Wie in der Beispielimplementierung von \textit{pytorch forecasting} (\cite{GitHub.20240307T20:56:16.000Z}) werden für das Training alle Daten abzüglich zweimal der Vorhersagelänge, einmal für den Validierungsdatensatz und einmal für den Testdatensatz.
Durch die Berechnung eines spezifischen Cut-off-Werts pro Land auf Grundlage der Vorhersagelänge, wird sichergestellt, dass künftige Daten den Lernprozess des Modells nicht beeinflussen.
Dadurch kann ein realistischer Trainingsaufbau und eine faire Bewertung der Vorhersagefähigkeit des Modells gewährleistet werden.
Eine Abfrage des maximalen Zeitindex des Trainingsdatensatzes und des minimalen Zeitindex des Validierungsdatensatzes gruppiert nach Land garantiert, dass sich die Daten nicht überschneiden und kein Leck enthalten ist.

Alle Zeitvariablen, bis auf die binäre Information über Feiertag oder Wochenende, weisen ein zyklisches Muster auf.
Um dieses Muster für das neuronale Netz verständlicher zu machen, werden sie wie in ähnlichen Arbeiten (\cite{Giacomazzi.06202023},~\cite{Labiadh.2.8.2023}) zyklisch mit Sinus- und Cosinus-Funktion kodiert.

Code-Ausschnitt~\ref{CODE:tft_training-dataset} zeigt die Erstellung des Trainingsdatensatzes.
\lstinputlisting[language=Python, caption=Python-Code zur Erstellung des Trainingsdatensatzes für den TFT, label=CODE:tft_training-dataset]{\codedir/tft_training-dataset.m}
Dem Trainingsdatensatz werden Parameter mitgegeben, um den Aufbau und die Struktur der Eingabedaten für das Modell zu definieren.
Bei Zeitreihenprognosen mit mehreren Zeitreihen, die einen unterschiedlichen Datenbereich haben, ist es entscheidend, den Trainingsdatensatz für jede der Reihen anzupassen.
Wichtig ist darüber hinaus eine korrekte Zuordnung der Variablen zu den verschiedenen Variablenarten~\cite{GitHub.20240307T20:56:16.000Z}.
Im betrachteten Anwendungsfall der Vorhersage von \ac{MOER}-Werten, werden diese als Zielvariable festgelegt.
Das Land wird als Gruppen-Id gesetzt, um zu definieren, dass ausgehend vom Land zwei verschiedene Zeitreihen existieren.
Zwei für die Vorhersage wichtige Angaben sind die Anzahl der zu verwendenden Vergangenheitswerte (\lstinline[columns=fixed]{max_encoder_length}) und wie weit in die Zukunft vorhergesagt wird (\lstinline[columns=fixed]{max_predicition_length}).
Das Land wird außerdem auch als statische, kategorische Variable \lstinline[columns=fixed]{static_categoricals} angegeben, da sich die Werte im Lauf der Zeit nicht verändern.
Kategorische Werte, die sich über die Zeit ändern, aber im Voraus bekannt sind (Jahreszeit, Wochentag und, ob es sich um einen Feiertag handelt), werden als \lstinline[columns=fixed]{time_varying_known_categoricals} gesetzt.
Sich über die Zeit ändernde, im Voraus bekannte, aber kontinuierliche Werte, sind die Wetterdaten (Temperatur, Windgeschwindigkeit, \ac{GHI} und Niederschlag).
Sie werden als \lstinline[columns=fixed]{time_varying_known_reals} gesetzt, nachdem sie durch Normalisierung mithilfe des \lstinline[columns=fixed]{StandardScaler} vom Package \glqq sktlearn\grqq auf eine ähnliche Werteskala gebracht wurden.
Gibt es für diese Art von Werten Variablen, die im Voraus nicht bekannt sind, werden sie als \lstinline[columns=fixed]{time_varying_unknown_reals} gesetzt.
Welche Variable zu dieser Kategorie auf jeden Fall dazu gehört ist die die Zielvariable.
Es wird außerdem eine Normalisierung anhand der Gruppen durch die Softplus-Funktion durchgeführt, um positive Ausgabewerte zu garantieren.
Durch die Angabe sogenannter \lstinline[columns=fixed]{lags} wird die Eingabe um vergangene Werte der Zielvariablen, verschoben um den angegebenen Zeitraum, erweitert.
%Ausgehend von der Saisonalität der Daten werden hier die Werte verschoben um einen Tag und um eine Woche verwendet.
Zuletzt werden dem Modell zusätzliche Merkmale und Kontext hinzugefügt, wie ein relativer Zeitindex um das Verhältnis jeden Datenpunkts zum Vorhersagepunkt zu erfassen, das Ausmaß der Zielvariable in Form von Mittelwert und Standardabweichung, und die Größe des Vorhersagefensters.
Außerdem wird festgelegt, dass fehlende Zeitschritte geduldet werden, obwohl der Datensatz zuvor auf fehlende Werte untersucht wurde.
Die Datensätze zur Validierung und zum Testen werden dann ausgehend vom Trainingsdatensatz erstellt, wobei für diese angegeben wird, dass eine Vorhersage ausgeführt werden soll (\lstinline[columns=fixed]{predict=True})~\cite{Labiadh.2023}~\cite{GitHub.20240307T20:56:16.000Z}.
Wichtig zu beachten ist, dass beim Training in keiner Weise Daten der Zukunft eingesehen werden können.
Das wird sichergestellt, indem während des Trainings ausschließlich der Datensatz abzüglich der Werte der maximalen Vorhersagelänge zur Verfügung gestellt werden.
Sind die verwendeten Lags länger, als die Vorhersagelänge, so muss diese Länge dem Trainingsdatensatz vorenthalten werden, sodass durch die zeitlichen Verschiebungen der Lags keine Einsicht in die Zukunft bietet.
Die entsprechenden Cut-Off-Zeitpunkte werden die beiden Länder separat anhand des maximalen Zeitindex definiert.

Zur Einordnung der Vorhersageleistung der Modelle soll der \ac{MAE} eines äußerst simplen Basismodells, dem Baseline-Modell von \textit{pytorch forecasting} errechnet werden, um eine Ausgangslage zu haben~\cite{PytorchForecastingDocumentation.20230410T20:05:46.000Zb}.
Dieses verwendet den letzten bekannten Wert der Zielvariable, um eine Vorhersage für den nächsten Zeitschritt zu machen.

Die wichtigsten Parameter der \lstinline[columns=fixed]{TemporalFusionTransformer}-Klasse in \textit{pytorch forecasting} werden im Folgenden erklärt~\cite{Joseph.2022}:
\begin{itemize}
  \item \lstinline[columns=fixed]{attention_head_size}: Legt die Anzahl der Attention-Heads fest, die das Modell  nutzen kann, um aus den Daten zu lernen.
 Mehrere Attention-Heads können dem Modell helfen, eine vielfältigere Reihe von Zusammenhängen in den Daten zu erkennen.
  \item \lstinline[columns=fixed]{dopout}: Ein Wert zwischen 0 und 1, der die Dropout-Rate innerhalb angibt.
  Dropout ist eine Regularisierungstechnik, die dazu dient das Modell robuster zu machen, indem zufällig ausgewählte Neuronen während des Trainings ignoriert werden.
  \item \lstinline[columns=fixed]{hidden_size}: Wahrscheinlich der wichtigste Hyperparameter im Modell, der die versteckte Dimension im gesamten Modell definiert.
 Diese Dimension wird für alle \acp{GRN}, \ac{LSTM}-Einheiten und Selbst-Attention verwendet.
 Dieser Hyperparameter ist zentral für die Architektur des Modells, da er die Kapazität zum Verarbeiten und Speichern von Informationen bestimmt.
 \item \lstinline[columns=fixed]{hidden_continous_size}: Gibt die Standardgröße der Einbettung für kontinuierliche Features an.
 Dadurch wird angegeben, wie detailliert kontinuierliche Variablen im Modell repräsentiert werden.
 \end{itemize}

Zum Vorschlagen einer passenden Learning-Rate wird der Tuner von pytorch lightning verwendet.
Dieser schlägt den geeignetsten Wert für die Learning-Rate unter Angabe des \ac{TFT}-Modells, der Trainings- und Validierungsdaten und eines Mindest- und Höchstwerts, vor.
Für die optimierte Einstellung weiterer Parameter des Modells wird das Hyperparameter-Tuning von pytorch forecasting getestet~\cite{PytorchForecastingDocumentation.20230410T20:05:46.000Z}.
Dafür werden die Trainings- und Validierungsdaten benötigt und Werteräume für die zu analysierenden Parameter definiert.
Die getesteten Parameter mit den jeweils definierten Bereichen sind in Tabelle~\ref{TAB:hyperparameter-trained} aufgelistet
Die schlussendlich verwendeten Parameter für jedes der Modelle sind in Tabelle~\ref{TAB:hyperparameter-used} zusammengefasst.
\begin{table}[t]
 \centering\small
 \caption{TFT Hyperparameter Tuning}
 \label{TAB:hyperparameter-trained}
 \input{\tabledir/TftHyperparameterTrained.tex}
\end{table}

\begin{table}[t]
 \centering\small
 \caption{TFT Verwendete Hyperparameter}
 \label{TAB:hyperparameter-used}
 \input{\tabledir/TftHyperparameterUsed.tex}
\end{table}

\section{Evaluierung und Vergleich der Modelle}
Die Evaluation der Modelle erfolgt anhand des \ac{MAE}, des \ac{MSE} und des \ac{MAPE}, um einen umfassenden Vergleich zu ermöglichen.
Diese Fehlermetriken wurden unter anderem aufgrund ihrer Relevanz in Bezug auf die jeweiligen Loss-Funktionen ausgewählt.
Die beim \ac{TFT} verwendete Quantile-Loss-Funktion entspricht bei Anwendung auf das mittlere Quantil von 0,5 dem \ac{MAE}, da Überschätzungen und Unterschätzungen dann gleich gewichtet werden.
Außerdem ermöglicht der \ac{MAE} eine intuitive Interpretation der Abweichungen, weil er Fehler in absoluten Werten angibt.
Im Gegensatz dazu basiert das \ac{SARIMAX}-Modell auf der Optimierung mittels des Kalman-Filters.
Dieser basiert auf dem \ac{MSE}, welcher größere Fehler stärker gewichtet~\cite{Lacey.1998}.
Um zudem eine prozentuale Fehlermetrik zur besseren Einschätzung der Verhältnismäßigkeit der Fehler zu haben, wird der \ac{MAPE} als dritte Metrik herangezogen.
Dieser ist gut für Zeitreihenvorhersagen geeignet, weil er einen Vergleich verschiedenster Zeitreihen und Skalen ermöglicht~\cite{Lazzeri.2021}.

Grafische Darstellungen der Vorhersagen der verschiedenen Modelle können im Anhang der Arbeit (\ref{CAP:append-figures}) eingesehen werden.

Die Vorhersageleistung der Modelle ist in Tabelle~\ref{TAB:forecasting-performance} zusammengefasst.
Die Werte sind auf zwei bzw. vier Nachkommastellen gerundet angegeben.
\begin{table}[htbp]
 \centering
 \caption{Vorhersageleistung der Modelle}
 \label{TAB:forecasting-performance}
 \input{\tabledir/ForecastingPerformance.tex}
\end{table}
Mit Blick auf die Werte von \ac{SARIMAX} wird ersichtlich, dass die Prognose für das deutsche Stromnetz weitaus besser funktioniert, als für das norwegische.
Grund dafür könnte die sehr geringe oder fehlende Abhängigkeit zu den Eingabevariablen sein.
Wie in der Korrelationsmatrix (Abbildung~\ref{FIG:correlation_matrix}) sichtbar wurde, ist die \ac{MOER} Norwegens nur kaum linear von den Zeit- und Wetterdaten abhängig, was eine Vorhersage enorm erschwert.
Für Deutschland werden bessere Vorhersagewerte erzielt, was zum einen an der Abhängigkeit von der Sonneneinstrahlung und der Windstärke und zum anderen dass das periodisch abfolgende Muster sich größtenteils gleich verhält.
Um auszuschließen, dass die mangelhaften Ergebnisse nicht durch eine unzureichende Datenvorbereitung oder eine unpassende Modellauswahl zu begründen sind, wurden folgende Untersuchungen unternommen:
\begin{itemize}
 \item \textbf{Filtern der Daten}: Zusätzlich zum vollständigen Datensatz wurde ein gefilterter Datensatz erstellt, aus dem Werte unterhalb des ersten und überhalb des dritten Quantils entfernt wurden.
 \item \textbf{Stationarität der Daten}: Schlechte Ergebnisse können durch eine fehlende Stationarirät der Daten entstehen~\cite{}. Um das zu verhindern, wurde die $auto_arima$-Funktion zusätzlich mit einer Differenzierung der Werte ($d > 0$ oder $D > 0$) ausgeführt, was jedoch höhere \ac{AIC}-Werte zur Folge hatte.
 \item \textbf{Parameterprobe}: Die Parameter von \ac{SARIMAX} wurden mit unterschiedlichsten Werten erprobt.
 \item \textbf{Trainingsdatensatz}: Die Länge des Trainingsdatensatzes wurde variiert, um Overfitting und Underfitting zu verhindern.
 \item \textbf{Validierungsdatensatz}: Der Validierungsdatensatz wurde variiert, um auszuschließen, dass die mangelhaften Ergebnisse an untypischen Validierungsdaten liegen.
\end{itemize}

Der \ac{TFT} weist allgemein bessere Prognoseergebnisse auf, jedoch zeigt sich auch hier, dass die Vorhersage der deutschen Werte besser funktioniert als die der norwegischen.
% Vergleicht man die Vorhersage auf dem Testdatensatz mit den tatsächlichen Werten, fällt für Deutschland vor allem der extrem niedrige Wert zu Beginn des Zeitraums und das darauf folgende periodische Muster in den Daten auf, was beides vom \ac{TFT} sehr gut erfasst wird (siehe Abbildung~\ref{FIG:TFT-168-de-plot}).
Lediglich kleinere Schwankungen werden in der Vorhersage nicht erkannt.
Die Minima und Maxima im täglichen Verlauf werden durch die Vorhersage sehr gut getroffen, was wichtig für die spätere Anwendung der Vorhersagen ist.

Ein \ac{TFT} gibt Aufschluss darüber, wie die einzelnen von ihm verwendeten Variablen interpretiert wurden.
Aufgrund der \acp{VSN} besteht die Möglichkeit, direkt auf die Gewichtung von Features zugreifen zu können.
Durch die Self-Attention-Heads können zudem Aussagen über die Gewichtung einzelner Zeitschritte im Aufmerksamkeitsmechanismus~\cite{Joseph.2022}.

Betrachtet man die Wichtigkeit der einzelnen Variablen für den Encoder und den Decoder (siehe Abbildung~\ref{FIG:decoder-interpretation}), so wird ersichtlich, dass für das Erfassen der Werte (Encoder) vor allem der relative Zeitindex wichtig ist.
\begin{figure}
 \caption[TFT Vorhersage Einluss einzelner Variablen]{TFT Wichtigkeit der einzelnen Variablen für den Dencoder (eigene Darstellung)}
 {\includegraphics[width=0.6\textwidth]{\figdir/tft_de-no_168_decoder-interpretation}}
 \label{FIG:decoder-interpretation}
\end{figure}
Weitere als bedeutend interpretierbare Variablen sind die \ac{MOER}-Werte der vergangenen Woche und die Jahreszeit.
Für die Vorhersage (Decoder) stützt sich das Modell vor allem auf den Zeitindex, sowohl relativ zum Vorhersagezeitpunkt als auch absolut, und auf die Stunde des Tages, was im Hinblick auf das tägliche Muster durchaus sinnvoll scheint.
Im Attention-Graph wird die Wichtigkeit vergangener Werte dargestellt.
Der Graph zeigt, dass vor allem Werte der letzten weniger als 25 Zeitindizes wichtig sind.
Mit Blick auf den Verlauf der \ac{MOER} in Deutschland um den Vorhersagezeitpunkt scheint das logisch, denn zu diesem Zeitpunkts ist ein starker Abfall der Werte zu verzeichnen.
Gleichzeitig begründet die tägliche Saisonalität die Wichtigkeit der letzten 24 Stunden.
Für die norwegischen Werte zeigt dies einen Grund für die Qualität der Vorhersage.
Die letzten 25 Werte sind nicht aussagekräftig genug, um anhand dieser die nächste Woche vorherzusagen.
%!
%!
\chapter{Anwendungsmöglichkeiten der Strategien auf Basis der Prognose}\label{CAP:prediction-application}
\section{Anwendungsszenarien}\label{CAP:scenarios}
% even a static selection of where to host your workload can make a big difference~\cite{Norlander}
Die Analyse der \ac{MOER} hat eine deutliche tägliche Saisonalität aufgezeigt (siehe Kapitel~\ref{CAP:data-preparation-analysis}).
Für Deutschland zeigen die Daten für gewöhnlich folgendes Muster:
Zu Beginn des Tages ist die \ac{MOER} relativ hoch, bevor sie zuerst langsam, dann schneller absinkt.
Kurz vor dem Mittag erreicht sie ihren Tiefstwert und steigt danach erneut stark an bis zum Höchstwert kurz vor Mitternacht.
Diese täglichen Schwankungen der \ac{MOER} bieten wertvolle Ansatzpunkte für Lastmanagementstrategien.
Wenn ausführliche oder langfristige Prognosen nicht verfügbar sind, können diese regelmäßigen Muster als vereinfachte Grundlage genutzt werden, um den Energieverbrauch zeitlich so zu steuern, dass die Kohlenstoffintensität minimiert wird.
So könnte etwa eine Lastverlagerung in die Mittagszeit, wenn die \ac{MOER} am niedrigsten ist, eine effektive Methode sein, um die Umweltbelastung zu reduzieren.
Allerdings zeigen die Daten jedoch auch immer wieder starke Einbrüche.
Die in Kapitel~\ref{CAP:prediction} entwickelten Prognosemodelle bieten eine Vorhersage der \ac{MOER} für eine Woche bzw. einen Monat.
Diese können für die in Kapitel~\ref{CAP:strategies} beschriebenen Strategien angewandt werden.

Bevor mögliche Anwendungsszenarien diskutiert werden, soll als erstes das Potential zeitlicher und örtlicher Verschiebungen analysiert werden.
In Abbildung~\ref{FIG:location-shifting-potential} ist das Potenzial für die Standortverschiebung von Rechenlasten zwischen Deutschland (DE) und Norwegen (NO) anhand der \ac{MOER} dargestellt.
Aufgrund der stark abweichenden Werte wurde die Analyse für Verschiebungen von Deutschland nach Norwegen in zwei separate Diagramme für positive und negative Einsparungen unterteilt.
In den Abbildungen~\ref{FIG:location-shifting-potential}a und~\ref{FIG:location-shifting-potential}b wird ersichtlich, dass eine Verlagerung von Lasten von Deutschland nach Norwegen in der Mehrzahl der Fälle zu erheblichen Einsparungen von CO2-Emissionen führt, wobei oftmals eine Reduktion zwischen 60\% und 70\% zu verzeichnen ist.
Nur in einer geringen Anzahl von Zeitpunkten zeigt sich ein gegenteiliges Bild:
Hier übersteigt die \ac{MOER} Deutschlands diejenige Norwegens, was zu negativen Einsparungen, also zu einer Erhöhung der CO2-Emissionen führen würde.
Diese Zeitpunkte, die eine drastische Zunahme der Emissionen zwischen 200\% und 1000\% verursachen würden, sind jedoch als Ausnahmen zu betrachten.
In diesen seltenen Fällen ist die \ac{MOER} in Deutschland erheblich niedriger als in Norwegen.
Abbildung~\ref{FIG:location-shifting-potential}c stellt das umgekehrte Szenario dar und zeigt, dass Verschiebungen von Norwegen nach Deutschland selten sinnvoll sind, aufgrund der konstant niedrigen \ac{MOER} in Norwegen.
Aus den Daten geht hervor, dass theoretisch generelle Verlagerungen in Richtung Norwegen vorteilhaft sind und insgesamt mit hohen CO2-Einsparungen einhergehen.
Die Voraussetzungen sind aber oft nicht trivial, sodass die Umsetzung durch politische oder Infrastruktur-bezogene Einschränkungen nur begrenzt möglich sind.
Zeiten, in denen das deutsche Stromnetz besonders niedrige \ac{MOER}-Werte aufweist, könnten dennoch für Verschiebungen von Norwegen nach Deutschland genutzt werden, um die Gesamtemissionen weiter zu reduzieren.
Der durch die örtliche Verschiebung zusätzlich entstehende Aufwand wurde hier bei der Betrachtung des Potenzials nicht berücksichtigt.
Dieser darf aber bei Berechnung der tatsächlichen Einsparungen nicht vernachlässigt werden.

Geräte und Systeme mit flexibler Stromnachfrage zu den richtigen Zeitpunkten zu schalten, kann dazu führen, dass emissionsreiche Kraftwerke heruntergefahren, die Netzintegration erneuerbarer Energien gefördert und Emissionen reduziert werden.
Durch Vorhersagen, wann eine Kilowattstunde Strom wie viel CO2 emittiert, können Geräte so gesteuert werden, dass sie dann Strom beziehen, wenn diese Menge gering ist.
Diese zeitlichen Verschiebungen können Emissionen minimieren, ohne höhere Stromrechnungen zu verursachen oder den Komfort von Endnutzern zu beeinträchtigen.
Die Anwendungsbereiche können verschiedenster Natur sein.
Theoretisch kann jedes mit dem Internet verbundene Gerät und flexibler Stromnachfrage von den Lastverschiebungen Nutzen machen.
Dazu gehören zum Beispiel Elektrofahrzeuge, denn diese müssen regelmäßig geladen werden.
Das Ziel ist, dass die Fahrzeuge zwar immer fahrbereit sind, wenn vom Nutzer gewünscht, im Hintergrund der Zeitpunkt der Ladung jedoch an die Emissionen des Stroms ausgerichtet wird.
Im Gegensatz zum sofortigen Laden beim Anstecken des Fahrzeugs egal mit welcher Emissionsstärke, könnte das Laden erst dann gestartet werden, wenn die Emissionsstärke ein Minimum erreicht.
In beiden Fällen ist das Fahrzeug vollgeladen, wenn der Nutzer es wünscht, jedoch einmal durch kohlestoffreichen und einmal durch kohlestoffarmen Strom.
Für dieses Szenario gibt es schon Umsetzungsbeispiele.
Dabei hilft sogenanntes \glqq Smart-Charging\grqq{} mit intelligenten Ladesystemen, das Laden von Elektrofahrzeugen anhand der Kohlenstoffintensität zu optimieren.
Nicht nur der Zeitpunkt, sondern auch die Region, aus der der Strom bezogen wird, spielt dafür eine wichtige Rolle.
Bei Fahrzeugen ist die Stromnachfrage an den Standort des Fahrzeugs gebunden.
Im Gegensatz dazu bietet Software den Vorteil, dass diese an beliebigen Orten ausführbar ist.

Das zweite große Szenario, das für die vorliegende Arbeit weitaus relevanter ist, ist die Anpassung der Rechenlasten von Software an die Emissionen des Stroms.
Das ist sowohl für Public Clouds, als auch für bei Individual-Software oft verwendete Private Clouds interessant.
Die dafür verfügbaren Möglichkeiten und Tools werden im nächsten Unterkapitel analysiert.
Die Umsetzung der Strategien ist je nach Strategie auf bestimmte Workloads begrenzt, wie bereits in Kapitel~\ref{CAP:requirements} diskutiert.
Eine zeitliche Verschiebung eignet sich nicht gut für lang andauernde oder kontinuierliche Workloads, sowie solche mit strikten Fristen oder Notwendigkeit zur sofortigen Ausführung.
Nicht unterbrechbare Workloads, die an einem Stück ausgeführt werden müssen, bieten weniger Flexibilität für Verschiebungen.
Die richtige Auswahl der Zeit ist essenziell, da die Lastverschiebung anderenfalls dem Ziel nicht nützt oder diesem sogar schadet.
Viele Fälle von zeitlicher Lastverschiebung, die man noch vor 15 Jahren als hilfreich ansah, haben zum Teil die Emissionen erhöht, anstatt sie zu verringern~\cite{WattTime.12.3.2024}.
Örtliche Verlagerungen sind teilweise durch regulatorische Anforderungen oder möglicherweise durch mangelnde Ressourcen in der gewünschten Region beschränkt.
Bei Workloads, die eine große Datenmenge erfordern überwiegt der Energieaufwand für die Verlagerung der Daten in ein anderes Rechenzentrum möglicherweise den Einsparungen durch dieses umweltfreundlichere Rechenzentrum~\cite{Norlander.2023}.

Für einige der von Randstad Digital entwickelten und betreuten Softwaresysteme von Kunden aus verschiedensten Branchen ist eine Anwendung denkbar.
Die Beispiele für Anwendungsfälle aus dem Kundenumfeld sind vielfältig, dazu gehören zum Beispiel Batchverarbeitungs-Jobs, die oft eine hohe Flexibilität bezüglich ihres Startpunkts bieten.
Cron Jobs, die periodische Aufgaben wie Datenbank-Backups oder Berichtsgenerierungen zu festgelegten Zeiten ausführen, sind ein weiteres Beispiel.
Die Integration und Bereitstellung von Software über CI/CD-Pipelines kann, solange die Abläufe nicht kritisch sind, ebenso zeitlich verlagert werden.
Weitere Aufgaben, die oft zufällig ohne festes Muster ausgeführt werden, können sich ebenso für zeitliche Verschiebungen eignen.
Dazu gehören Datenbank-Indexierungen, die essentiell für die Performance-Optimierung von Datenbanksystemen sind oder Replikationsaufgaben, die im Hintergrund die Synchronisation von Daten zwischen verschiedenen Systemen garantieren.
Solche Workloads, die asynchron laufen, keine Abhängigkeiten oder Verzögerungen nach sich ziehen und deren Fälligkeitsdatum es erlaubt, sind demnach in einem realen Umfeld keine Ausnahme.

% Zum Beispiel enthalten die Softwarelösungen oft Batch-Jobs, die regelmäßig ausgeführt werden müssen.

% tbd: Inwiefern wird Green IT von unseren Kunden angefragt, welche Szenarien gibt es bei unseren Kunden, konkretes Fallbeispiel
%WattTime verspricht durch Load Shifting eine Reduzierung der induzierten Emissionen um bis zu 90\% und eine Reduzierung der Kürzungen erneuerbarer Energien um bis zu 20\%.
%
%Zur Einschätzung möglicher Auswirkungen von Load-Shifting sollen zwei beispielhafte Szenarien betrachtet werden.
%(1) Periodisch geplante Workloads mit kurzer Dauer
%(2) Lang dauernde Machine-Learning-Trainings
%Für die Betrachtung werden die Szenarien abstrahiert.
%Beim ersten Szenario handelt es sich um Workloads, z.B. Integration Tests oder Datenbank-Migrationen, die außerhalb der Arbeitszeiten laufen sollen.
%Es wird angenommen, dass sie periodisch jede Mitternacht ausgeführt werden.
%Sie benötigen je eine Stunde und sind nicht unterbrechbar.

\section{Verfügbare Tools}\label{CAP:tools}
Zur Anwendung der im vorherigen Kapitel beschriebenen Prognose der Kohlenstoffintensität sollen bereits verfügbare Tools, die zur Ausrichtung von Software an Emissionswerte verwendet werden können, betrachtet werden.
Dafür werden die Möglichkeiten der drei gängigsten Public-Cloud-Plattformen - \ac{AWS}, Azure und \ac{GCP} - und des Container-Orchestrierungstools Kubernetes untersucht.
Für Individual-Software, wie sie im Consulting Umfeld üblich ist, werden meist private Clouds mit eigener Konfiguration aufgesetzt.
Kubernetes findet in diesem Zusammenhang wegen seiner hohen Individualisierbarkeit und Unabhängigkeit oft Anwendung.
%Einen Überblick über die Tools dieser Anbieter zeigt Tabelle~\ref{TAB:tools}.
%\begin{table}[ht]
% \centering
% \caption{Tools}
% \label{TAB:tools}
% \input{\tabledir/Tools.tex}
%\end{table}

Diese vier Anbieter verfügen über umfassende Tools für das zeitliche Planen von Workloads basierend auf bestimmten Kriterien.
Die Cloud-Tools in der Kategorie \glqq Serverless\grqq{}, wie \textit{\ac{AWS} Lambda Scheduled Events} bieten aufgrund ihres Design die optimalen Voraussetzungen und ermöglichen die automatische Ausführung von Funktionen zu gewünschten Zeitpunkten und in regelmäßigen Abständen.
%Für wiederkehrende Aufgaben, wie Datenbackups oder das Erstellen von Berichten ist dieses Feature besonders nützlich.
Bei Microsoft bietet \textit{Azure Automation} eine ähnliche Funktionalität.
Workloads können zu einem vordefinierten Zeitpunkt ausgeführt und über verschiedene Azure-Dienste hinweg verwaltet werden.
Google bietet mit dem \textit{Google Cloud Scheduler} einen umfangreichen Cron-Job Scheduler.
% Ein praktisches Beispiel für die Anwendung von Demand Shifting ist die Optimierung von Cron Jobs.
Ein Cron Job dient dazu, Aufgaben gemäß einem wiederkehrenden Zeitplan zu erstellen.
Regelmäßig anfallende Aufgaben, wie das Erstellen von Backups oder das Generieren von Berichten, können dadurch automatisiert zu bestimmten Zeiten abgearbeitet werden.
Durch die Definition eines Zeitplans kann ein Job in festgelegten Intervallen ausgeführt werden~\cite{TheKubernetesAuthors.20240119T14:53:20+01:00}.
Auch Kubernetes bietet die Ausführung von Workloads zu vorgegebenen Zeiten durch Cron-Jobs.
\textit{Kubernetes CronJobs} können regelmäßige Aufgaben gemäß erstellter Zeitpläne abarbeiten.
Diese Zeitpläne nach der Kohlenstoffintensität des Stromnetzes auszurichten, kann eine Reduzierung der Emissionen begünstigen.
%Bei den drei Cloud-Anbietern und bei Kubernetes können Cron Jobs erstellt werden.
%Eine Möglichkeit, dies umzusetzen, ist mit dem Cloud Scheduler von \ac{GCP}.
%Mit diesem können Cronjobs erstellt werden, die Jobs regelmäßig zu bestimmten Zeiten ausführen.
%Auf diese Weise kann die Ausführung regelmäßig anstehender Aufgaben, wie zum Beispiel Batch-Jobs, durch einen Zeitplan konfiguriert werden.
%Die Idee ist, den Zeitplan gemäß der Kohlenstoffintensität auszurichten.


Eine weitere Möglichkeit besteht darin, Auto Scaling Tools an die Kohlenstoffintensität auszurichten.
Auto-Scaling zielt eigentlich auf eine automatische Skalierung der Anwendungsressourcen je nach Lastnachfrage ab.
Auf diese Weise soll sichergestellt werden, dass zwar immer ausreichend viele Ressourcen verfügbar sind, aber nicht mehr als nötig.
Die Optimierung ist dabei auf eine Reduzierung der Kosten ausgelegt, denn im Vordergrund steht, dass bei geringer Nachfrage die Kapazitäten reduziert werden, um Kosten zu sparen.
Zudem können zum Beispiel bei der Auto-Scaling Funktion von Amazon EC2-Instanzen mehrere Regionen für die Verteilung der Ressourcen angegeben werden, um eine hohe Ausfallsicherheit zu garantieren~\cite{AmazonWebServices.20240229}.

%Ziel ist es nicht, die Kapazität an die Nachfrage anzupassen, sondern die Kapazität an die Kohlenstoffintensität.
%Das hat Auswirkungen auf die Nachfrage, weil diese durch eine eingeschränkte Kapazität möglicherweise nicht mehr oder zumindest nicht zum gleichen Zeitpunkt gedeckt werden kann.
%Es gilt also, teils Kompromisse bei der Nachfrage zu machen, um eine Skalierung entlang der Kohlenstoffintensität zu ermöglichen.

Die Kohlenstoffintensität könnte als benutzerdefinierte Metrik für das Auto-Scaling eingeführt werden.
Amazon setzt dafür standardmäßig \textit{Amazon CloudWatch} ein, ein Tool zur Beobachtung bestimmter Metriken einer Instanz, mit Metriken wie z.B. der \ac{CPU}-Auslastung zur Überwachung der Last und entsprechender Ressourcenanpassung verwendet.
Für CloudWatch können aber auch eigene Metriken konfiguriert werden.
Auf diese Weise könnten für die prognostizierte Kohlenstoffintensität bestimmte Schwellwerte festgelegt werden, anhand dieser der Auto-Scaler die Anzahl der Instanzen erhöht oder verringert.
Die Funktionsweise von CloudWatch, inklusive Bestandteile und dem Ablauf, ist in Abbildung~\ref{FIG:aws-cloudwatch} dargestellt.
\begin{figure}
 \caption[Funktionsweise AWS Cloudwatch]{Die Funktionsweise von AWS Cloudwatch zur automatischen Skalierung von Cloud-Instanzen anhand spezifischer Metriken~\cite{AmazonWebServices.20240320T16:42:17.000Z}}
 {\includegraphics[width=0.8\textwidth]{\figdir/aws-cloudwatch}}
 \label{FIG:aws-cloudwatch}
\end{figure}
AWS-Ressourcen, z.B. EC2-Instanzen, generieren und senden Metriken an CloudWatch.
CloudWatch verwaltet die standard oder benutzerdefinierten Metriken und generiert Statistiken auf Basis dieser Daten.
Der CloudWatch Alarm wird ausgelöst, wenn ein bestimmtes Ereignis eintritt, wie dass ein Messwert einen definierten Schwellwert über- oder untertrifft.
Auf Basis dieses Alarms können spezifische Aktionen ausgeführt werden, wie das Senden einer Benachrichtigung per E-Mail oder vor allem die automatische Skalierung der Ressourcen.
Beispielsweise könnte ein niedriger \ac{MOER}-Wert das Scaling-Out bewirken, um vom grünen Strom zu profitieren.
Die AWS Management Konsole dient zur Beobachtung und Analyse der Metriken und Statistiken, sowie dem Konfigurieren des Alarms.
Außerdem können die von CloudWatch generierten Statistiken weiter verarbeitet werden durch einen Statistics Consumer.

Das Hinzufügen einer eigenen Metrik zu CloudWatch erfolgt durch die Angabe verschiedener Parameter.
Code-Ausschnitt~\ref{CODE:aws-cloudwatch} zeigt, wie ein Befehl in der Kommandozeile von AWS zur Definition eines vorhergesagten \ac{MOER}-Werts aussehen könnte.
\lstinputlisting[language=sh, caption=Kommandozeilenbefehl zur Definition eines prognostizierten MOER-Werts für CloudWatch, label=CODE:aws-cloudwatch]{\codedir/aws-cloudwatch_put-custom-metric.m}
Angegeben wird der Name der benutzerdefinierten Metrik, gefolgt vom Name der Metrik-Sammlung.
\lstinline[columns=fixed]{--value} ist der prognostizierte Wert, der für einen bestimmten Zeitpunkt und eine bestimmte Region angegeben wird~\cite{AmazonWebServices.20240320T16:42:17.000Z}.

Azure, \ac{GCP} und Kubernetes bieten ebenfalls Möglichkeiten für die automatische Skalierung von Workloads.
Das Prinzip ist dabei immer dasselbe:
Wenn bestimmte Bedingungen erfüllt sind, werden eine oder mehrere Autoskalierungsaktionen ausgelöst, sodass zusätzliche Ressourceninstanzen hinzugefügt oder entfernt werden.
\textit{Azure AutoScale} ist ein Teil des \textit{Azure Monitor}, der die Skalierung von Azure-Diensten erleichtert.
Die automatische Anpassung ist jedoch meist nur in Form von horizontaler Skalierung möglich, für vertikales Skalieren sind oft manuelle Eingriffe notwendig.
Neben bestimmten vordefinierten Metriken, wie zum Beispiel ac{CPU}-Auslastung oder Anzahl der Threads können auch benutzerdefinierten Metriken, die direkt von der jeweiligen Anwendung generiert werden, verwendet werden.
Darüber hinaus können auch Zeitpläne zur Auslösung der Skalierung dienen.
Auf diese Weise können Scheduling und Scaling zu einem geplanten Skalieren kombiniert werden.
Die verwendete Bedingung für die Skalierung kann also entweder Metrik- oder Zeit-basiert sein ~\cite{Microsoft.2023}.
Die Implementierung von Google, der \textit{Google Cloud Autoscaler}, bietet ähnliche Funktionalitäten und passt die Anzahl der \ac{VM}-Instanzen automatisch an die Anforderungen der Workloads an, um die Leistung zu optimieren und die Kosten zu minimieren.
Kubernetes verfügt über den \textit{Horizontal Pod Autoscaler}, der die Anzahl der Pod-Replikaten basierend auf einer ausgewählten Metrik skaliert.

Bei Kubernetes besteht zudem die Möglichkeit zur automatischen Skalierung anhand bestimmter Events.
Dieses Event-driven Autoscaling kann zum Beispiel mit dem \ac{KEDA} umgesetzt werden.
Es existiert außerdem bereits ein Carbon Aware \ac{KEDA} Operator, der die Skalierung von Workloads in Kubernetes-Umgebungen anhand der Kohlenstoffintensität steuert.

Der Carbon Aware KEDA Operator~\cite{Azure.20240321T11:03:47.000Z} ist eine Erweiterung zum \ac{KEDA}, um Kubernetes-Workloads anhand der Kohlenstoffintensität zu skalieren.
Mithilfe von \ac{KEDA}, einer Kubernetes-Komponente zur Ereignis-gesteuerten Skalierung, können Anwendungscontainer anhand bestimmter Ereginisse skaliert werden.
Es ermöglicht eine einfache Integration in ein Kubernetes-Cluster und bietet verschiedene Quellen zur Skalierung an.
Die Idee hinter dem Carbon Aware \ac{KEDA} Operator ist, die standardmäßig verfügbaren Ereignis-Metriken um die Kohlenstoffintensität zu erweitern.
Die Funktionsweise des Tools wird in Abbildung~\ref{FIG:carbon-aware-keda-operator} veranschaulicht.
\begin{figure}
 \caption[Funktionsweise Carbon Aware KEDA Operator]{Die Funktionsweise des Carbon Aware KEDA Operators zur Skalierung von Containern in Kubernetes anhand der Kohlenstoffintensität~\cite{Azure.20240321T11:03:47.000Z}}
 {\includegraphics[width=0.8\textwidth]{\figdir/CarbonAwareKedaOperator}}
 \label{FIG:carbon-aware-keda-operator}
\end{figure}
Daten zur Kohlenstoffintensität von Drittanbietern (z.B. WattTime oder Electricity Maps) werden dem Operator in Form einer ConfigMap zur Verfügung gestellt.
In Abbildung~\ref{FIG:carbon-aware-keda-operator} wurde der Kubernetes Carbon Intensity Exporter zur Bereitstellung der Daten verwendet, der auf dem Carbon Aware \ac{SDK} basiert.
Der Administrator erstellt eine Spezifikation über den CarbonAwareKedaScaler, die auf ein bestimmtes Skalierungsobjekt oder einen Skalierungsjob verweist.
Die maximale Anzahl von Replikaten (\lstinline[columns=fixed]{maxReplicaCount}) wird dann durch den Operator an die aktuelle Kohlenstoffintensität angepasst.
Dies führt zu einer Einschränkung der Workloads während Perioden mit hoher Kohlenstoffintensität und zur maximalen Skalierung zu Zeiten niedriger Kohlenstoffintensität~\cite{Azure.20240321T11:03:47.000Z}.

Standardmäßig wird dafür das Carbon Aware SDK mit Vorhersagen der nächsten 24 Stunden alle 12 Stunden herangezogen.
Der Eco-Modus, also die Skalierung anhand bestimmter Schwellenwerte der Kohlenstoffintensität, kann jedoch auch für bestimmte Zeiträume deaktiviert werden.
Ausgehend von dieser Funktion, könnte eine geplante Skalierung konfiguriert werden.
Anhand der Prognose der Kohlenstoffintensität könnte ein Zeitplan für die Deaktivierung des Eco-Modus erstellt werden.
So kann die ein-wöchige Prognose genutzt werden, um die Workloads nur dann zu Drosseln, wenn die Kohlenstoffintensität hoch ist.


Ein weiteres Tool ist der Low-Carbon Kubernetes Scheduler, der im Rahmen eines gleichnamigen Papers~(\cite{James.}) implementiert wurde.
Er verfolgt das Ziel, die Funktionalität von Kubernetes um umweltbewusstes Scheduling zu erweitern.
Dabei soll der geeignetste Ausführungsort von Workloads basierend auf der Kohlenstoffintensität der verschiedenen Rechenzentren ausfindig gemacht werden.
Für die Umsetzung wurde der Standard-Scheduler von Kubernetes erweitert, um noch nicht zugewiesene Workloads anhand vordefinierter Regeln und Prioritäten zu verfügbaren Knoten zuzuweisen.
Neben den Daten zu Kohlenstoffintensität wird auch die lokale Lufttemperatur berücksichtigt, um die Kühlungsanforderungen der jeweiligen Region in die Entscheidung mit ein zu beziehen.
Wurde der am besten geeignete Standort ermittelt, wird über die Kubernetes- oder Cloud-\ac{API} die Bereitstellung einer Ressourcengruppe und anschließend eines Kubernetes-Clusters in diesem Rechenzentrum angefragt.
Der Prozess der Cluster-Bereitstellung, Vereinbarung von Anmeldeinformationen und aller internen Kubernetes-Komponenten dauert üblicherweise ungefähr 12 Minuten.
Ist dieser abgeschlossen, wird die Ressourcengruppe in der ursprünglichen Region gelöscht.
Für den Fall, dass der Scheduler weit verbreitet wäre und deshalb möglicherweise eine Vielzahl an Anfragen an einem Rechenzentrum eintreffen, würde dieses bei Überlastung die anfragenden Rechenzentren darüber informieren und die Workloads würden am ursprünglichen Rechenzentrum ausgeführt werden.
Mit Hinblick auf einfache Erweiterbarkeit wurde der Low-Carbon Kubernetes Scheduler so konzipiert, dass eigene Metriken definiert werden können, um die Scheduling-Entscheidungen zu beeinflussen~\cite{James.}.

Eine weitere Möglichkeit der Optimierung bietet die Server-lose Bereitstellung von Anwendungen.
Cloud-Anbieter stellen solche Serverless Computing-Services teilweise zur Verfügung, wodurch die Ausführung von Code ohne explizite Serverbereitstellung oder -verwaltung ermöglicht wird.
Bei Amazon erfolgt die Umsetzung durch \textit{AWS Lambda}, welches eine dynamische Anpassung der Ressourcen an den tatsächlichen Bedarf und auf diese Weise eine effizientere Nutzung ermöglicht.
Der Code muss lediglich in einer von Lambda unterstützten Laufzeitumgebung zur Verfügung gestellt werden, ohne die benötigten Ressourcen verwalten zu müssen.
\textit{Azure Functions} und \textit{\ac{GCP} Functions} sind die parallelen zu \ac{AWS} Lambda.
In einem solchen Server-losen Setup kann beispielsweise ein Cloud-Dienst automatisch mehr Funktionen ausführen, wenn die Kohlenstoffintensität niedrig ist, und die Ausführung einschränken, wenn sie hoch ist.

All diese Tools bieten das Potenzial zur Reduzierung von CO2-Emissionen, indem sie eine angepasste Nutzung von Ressourcen ermöglichen.

\section{API zur Abfrage der Prognose}
Die in Kapitel~\ref{CAP:prediction} vorgestellten Prognosen zur \ac{MOER} bieten wertvolle Einblicke für die Optimierung des Energieverbrauchs.
Um diese Prognosen nutzbar zu machen, sollen sie über eine \ac{API} bereitgestellt werden.
Im Rahmen dieser Arbeit ist die Implementierung nur auf lokaler Ebene erfolgt.
Für einen Einsatz in einem produktiven Umfeld, der eine breite Verfügbarkeit und Zuverlässigkeit erfordert, ist es notwendig, die \ac{API} in einer skalierbaren und sicheren Umgebung zu hosten.
Dies könnte auf einer Cloud-Plattform oder auf einem dedizierten Server erfolgen.

Mit Hilfe der \ac{API} kann für einen fiktiven Workload die Startzeit ausfindig gemacht werden, bei der die Summe der \ac{MOER} über die Dauer des Workloads am geringsten sind.
Der Ansatz entspricht dem in Kapitel~\ref{CAP:strategies} beschriebenen Scheduling-Verfahrens \glqq Flexibler Start\grqq{}.

Um die \ac{API} mit so wenig Overhead wie möglich umzusetzen, wird Flask, ein Mikro-Frontend zur einfachen Erstellung von Webanwendungen und \acp{API} mit Python, verwendet~\cite{.20240203T21:13:11.000Z}.
Zur Konfiguration der Abfrage sollen folgende Parameter angegeben werden:
\begin{itemize}
 \item Modell: Welches Prognose-Modell soll verwendet werden, \ac{SARIMAX} oder \ac{TFT}, default ist \ac{TFT}
 \item Vorhersagelänge: Wie weit in die Zukunft soll die Vorhersage möglich sein.
 Gewählt werden kann eine Woche oder ein Monat, wobei die Angabe durch die Anzahl der Stunden erfolgt (168 oder 720).
 Dieser Parameter entscheidet ebenfalls über die Art des Modells, weil je nach Angabe entweder das Modell für die Vorhersage einer Woche oder eines Monats herangezogen wird.
 Dabei ist zu beachten, dass mit längerer Vorhersage Abstriche bei der Vorhersagequalität gemacht werden.
 \item Dauer: Die geschätzte Bearbeitungszeit des Workloads in Minuten
 \item Fälligkeitsdatum: Bis zu welchem Zeitpunkt muss der Workload abgearbeitet sein.
 Der letzte mögliche Startzeitpunkt ergibt sich demnach aus dem Fälligkeitsdatum abzüglich der erwarteten Dauer.
 \item Land: Für welches Land soll die Vorhersage abgerufen werden, zur Auswahl stehen Deutschland oder Norwegen.
\end{itemize}
Eine Beispielanfrage zeigt Code-Ausschnitt~\ref{CODE:api-call}.
\lstinputlisting[language=HTML, caption=Beispielanfrage der Vorhersage-API, label=CODE:api-call]{\codedir/api-call.m}
Werden falsche Parameter verwendet, so liefert die Anfrage eine passende Meldung, die auf den falsch definierten Parameter mit Aufforderung zur Korrektur hinweist.

Bei erfolgreicher Anfrage umfasst die Ausgabe zwei wesentliche Informationen.
Zum einen wird der ermittelte ideale Startzeitpunkt für den fiktiven Workload mitgeteilt.
Dafür wird für jeden möglichen Startzeitpunkt die erwartete Bearbeitungszeit mit der jeweiligen \ac{MOER} zu diesem Zeitpunkt multipliziert.
Der ideale Startzeitpunkt ist derjenige, bei dem die Summe der benötigten \ac{MOER} am niedrigsten ist.
Zum anderen wird der höchste Wert dieser Summe verwendet, um die Ersparnis zwischen dem niedrigsten und dem höchsten \ac{MOER}-Gesamtwert auszugeben.

Die Auswahl des Landes ermöglicht einen Vergleich der \ac{MOER}-Werte der beiden Länder, was wiederum als Basis für eine örtliche Verschiebung dienen kann.
Die Erstellung des Prognose-Endpunkts mit Parametern ist in Code-Ausschnitt~\ref{CODE:prediction_endpoint} zu sehen.
\lstinputlisting[language=Python, caption=Python-Code zur Erstellung der Prognose API mit Flask, label=CODE:prediction_endpoint]{\codedir/flask_prediction_endpoint.m}

Es handelt sich um ein Konzept, für das folgenden Annahmen und Einschränkungen bestehen:
\begin{itemize}
 \item Bearbeitungszeit des Workloads: Die ungefähre Dauer des Workloads ist bekannt.
 Sie wird in Minuten und speziell für die verwendete Umgebung angegeben.
 \item Stand der Daten: Die beiden Modelle wurden mit Daten bis einschließlich Februar 2024 gefüttert.
 \item Aktualisierung der Modelle: Bei den erstellten Modellen handelt es sich um statische Modelle.
 Bei einer realen Umsetzung müssten die Modelle ständig erneuert werden, um aktuelle Prognosen treffen zu können.
 Dafür werden alle verwendeten Eingabedaten benötigt.
 \item Hosting der \ac{API}: Für den produktiven Gebrauch muss die \ac{API} gehostet werden.
\end{itemize}

Die \ac{API} könnte unter anderem für die in Kapitel~\ref{CAP:scenarios} beschriebenen Szenarien eingesetzt und so die emissionsgetriebene Steuerung von Stromnachfrage übernehmen.
Die konzipierte \ac{API} könnte direkt in bestehende Systeme integriert werden, indem sie von diesen angefragt wird.
Auf diese Weise könnte der Zugang zu \ac{MOER}-Vorhersagen automatisiert werden.
Zudem könnten bestimmte Schwellenwerte definiert werden, anhand derer entschieden wird, ob ein Workload ausgeführt wird oder nicht.
Darüber hinaus könnte die \ac{API} in Kombination mit dem Carbon Aware \ac{SDK} verwendet werden, sodass Systeme sowohl die aktuelle, als auch die vorhergesagte Kohlenstoffintensität des Stromnetzes abfragen und nutzen können.

\section{Herausforderungen und Grenzen}
%Für welche Architektur geeignet
%Echtzeit
In diesem Abschnitt sollen die Herausforderungen und Grenzen, die sich bei der parktischen Anwendung von kohlenstoffbewussten Strategien ergeben, beleuchtet werden.
Es handelt sich nicht um allgemein gültige, immer anwendbare Lösungen, sondern vielmehr um Vorschläge oder Richtlinien, die unter bestimmten Umständen durchaus positive Auswirkungen haben können.

Die Herausforderungen und Grenzen beim Anwenden der Prognosen für Software haben zwei Quellen.
Zum einen ergeben sie sich aus der Prognose und zum anderen aus den Strategien zur Optimierung der Software.

Bezüglich der Prognosen, stellen vor allem Vorhersagen weit in die Zukunft Schwierigkeiten dar.
Mit größerer Vorhersagelänge steigen auch die Fehleranfälligkeit der Vorhersagen.
Diese beginnen beim Wetter, dessen Vorhersagegenauigkeit nach sieben Tagen stark abnimmt~\cite{Lam.2022}.
Sowohl eine unzureichende Vorhersage des Wetters, als auch Fehler der Zeitreihenvorhersage selbst sind mögliche Fehlerquellen~\cite{Wiesner.2021}.
Die Vorhersage der \ac{MOER} des norwegischen Stromnetzes hat sich zudem als schwierig herausgestellt.
Die unzureichende Genauigkeit stellt eine hohe Fehlerquelle dar, weil vorhergesagte Minimalwerte der \ac{MOER} in Wirklichkeit Höchstwerte sein könnten.
Nicht zuletzt diese Gefahr, sondern auch das allgemein niedrige Niveau der Kohlenstoffintensität rechtfertigen den Aufwand für eine emissionsgesteuerte Ausrichtung von Workloads anhand Vorhersagen für das norwegische Stromnetz nicht.
Für solche Stromnetze, deren Vorhersage aufgrund ihrer Beschaffenheit nur schwer möglich ist, die aber allgemein niedrige Werte verzeichnen, bietet sich eine örtliche Verlagerung von ortsunabhängigen Aufgaben in dieses Stromnetz an.

Die Qualität der Vorhersagen ist stets von der Datenqualität- und verfügbarkeit abhängig.
WattTime erfasst seine Daten zur \ac{MOER} nach aktuellem Wissensstand und überprüft die Qualität regelmäßig.
Es handelt sich aber dennoch um modellierte Werte, deren Qualität nicht direkt durch Vergleichsberechnungen gemessen werden kann~\cite{WattTime.2022}.
Außerdem ist das Anwendungsgebiet ein hochaktueller Forschungsbereich, der stets neue Erkenntnisse hervorbringt, aber dadurch besteht auch die Möglichkeit, dass der aktuelle Kenntnisstand oder die Art der Datenerfassung und -erstellung korrigiert werden wird.

Darüber hinaus schränkt die Beschaffenheit der \ac{MOER}-Werte des deutschen und norwegischen Stromnetzes die Anwendbarkeit ein.
Entgegen anderer Ergebnisse~(\cite{Wiesner.2021}), die einen großen Unterschied zwischen unter der Woche und am Wochenende gemessenen Werten verzeichneten, weisen sie keine großen Unterschiede zwischen diesen Zeiträumen auf.
Die \ac{MOER} des deutschen Stromnetzes ist durchschnittlich über den betrachteten Zeitraum am Wochenende um 0,53\% geringer als unter der Woche und die des norwegischen Stromnetzes ist am Wochenende sogar um 0,74\% höher.
Beides sind zu vernachlässigende Unterschiede, die das pauschale Verschieben von Workloads auf das Wochenende nicht rechtfertigen würden.
Bei~\cite{Wiesner.2021} wurde gemessen, dass die Kohlenstoffintensität Deutschlands am Wochenende ungefähr 25\% geringer ist, was eine Verlagerung hin zum Wochenende durchaus sinnvoll macht.
Die verschiedenen Messungen kommen höchstwahrscheinlich daher, dass bei~\cite{Wiesner.2021} die durchschnittliche Kohlenstoffintensität des Stromnetzes verwendet wurde, die sich nach aktuellem Wissensstand wie bereits beschrieben nur schlecht für die Analyse von Laständerungen eignet.

Die Integration der Tools zur Nachhaltigkeitsoptimierung von Softwaresystemen ist stets mit Herausforderungen und Aufwand verbunden.
Dieser Aufwand wird von Unternehmen nur eingegangen, wenn ein ausreichend hohes Potenzial und ein Nutzen besteht.
Der Aufwand kann doch eine einfache Verfügbarkeit und Zugänglichkeit der Technologien und Lösungen minimiert werden.
Sind die Tools leicht integrierbar in Entwicklung und Betrieb bestehender Softwaresysteme, so ist die Hürde zur Anwendung gering.

Die Skalierungsstrategien von Softwaresystemen sind eng an die jeweilige Systemlast und die spezifischen Anforderungen gekoppelt.
Insbesondere zu Zeiten mit einer hohen Last kombiniert mit der Anforderung von minimalen Ausfallquoten kann der Wunsch nach Nachhaltigkeit in direktem Konflikt mit der Notwendigkeit stehen, Lastspitzen bewältigen zu können.
Eine Möglichkeit, dem Abhilfe zu schaffen, ist die Prognosen zur Kohlenstoffintensität mit Erwartungen der Last auf das System zu kombinieren.
Die Integration der erwarteten Systemlast und der Schwankungen der Kohlenstoffintensität zu integrieren, könnte eine Lösung sein, um beiden Anforderungen gerecht zu werden.

%tbd: Weitere Herausforderungen und Grenzen

%Es gilt zu beachten, dass die analysierten Optimierungen ihre Grenzen haben.
%Es handelt sich nicht um allgemein gültige, immer anwendbare Lösungen, sondern vielmehr um Vorschläge oder Richtlinien, die unter bestimmten Umständen durchaus positive Auswirkungen haben können.
%Eine wichtige Einschränkung kann ein mögliches Fälligkeitsdatum oder eine hohe Dringlichkeit sein~\cite{Dodge.06212022}.
%Ist bekannt, dass eine Aufgabe zu einem bestimmten Zeitpunkt angefangen oder sogar bereits abgeschlossen sein muss, hat es keinen Nutzen, wenn der nächste kohlenstoffarme Zeitraum nach diesem Fälligkeitsdatum liegt.
%Allgemein können die zeitlichen Verschiebungen zu Verspätungen führen und diese können wiederum einen Anstieg von Emissionen durch andere Teile des Projekts verursachen.
%Es ist deshalb stets wichtig, die Vorteile der eingesetzten Maßnahmen gegenüber ihrem Aufwand abzuwägen~\cite{Dodge.06212022}.
%
%Die örtliche Verlagerung ist nicht immer sinnvoll.
%

\chapter{Ergebnisse und Diskussion}\label{CAP:results}
Für eine Auswertung der Ergebnisse, sollen die Forschungsziele aus Kapitel~\ref{CAP:goals} aufgegriffen werden.
Die Ergebnisse können folgendermaßen zusammengefasst werden:
\begin{itemize}
 \item \textbf{Entwicklung eines Vorhersagemodells} \\
 Für die Prognose der \ac{MOER} des deutschen und norwegischen Stromnetzes wurden verschiedene Modelle entwickelt.
 Diese unterscheiden sich in der grundsätzlichen Art der Vorhersage, in der möglichen Vorhersagelänge und im verwendeten Land.
 Es hat sich gezeigt, dass sich \ac{SARIMAX}-Modelle zwar für den betrachteten Anwendungsfall eignen, sie aber die Schwankungen der Werte nicht genau abbilden können.
 Der \ac{TFT} als \ac{KI}-basiertes Modell eignet sich besser und erzielt - zumindest für Deutschland - zufriedenstellende Ergebnisse.
 Als wichte Eingabedaten haben sich vor allem die Stunde, der Zeitindex, der Wind und die zeitverzögerte \ac{MOER} herausgestellt.
 Der Niederschlag und ob ein Feiertag oder Wochenendtag vorliegt waren unwichtig für die Vorhersage.
 \item \textbf{Nachhaltigkeitsoptimierung von Softwareanwendungen} \\
 Für die Verbesserung der Nachhaltigkeit von Software wurden verschiedene Strategien untersucht.
 Dabei hat sich herausgestellt, dass die Effektivität der Strategien stark von der Beschaffenheit der Daten abhängt.
 Wohingegen die \ac{MOER}-Werte des norwegischen Stromnetzes schwer vorhersagbar sind, haben diese allgemein ein niedriges Niveau.
 Ein Workload-Scheduling anhand der Prognosen für diese Werte ist deshalb nur bedingt sinnvoll.
 Vielversprechender ist die örtliche Verschiebung von ortsunabhängigen Rechenlasten in das norwegische Stromnetz.
 Die deutschen Werte sind im Durchschnitt fast dreimal so hoch, sie bieten jedoch den Vorteil, dass sie unter anderem aufgrund der Zusammensetzung des Stromnetzes besser vorhersagbar sind.
 Außerdem sind immer wieder starke Minimalausschläge in den Daten zu verzeichnen, die sich nach Vorhersage gut für eine zeitliche Verschiebung von Rechenlasten eignen.
 \ac{AWS}, Azure, \ac{GCP} und Kubernetes bieten umfangreiche Möglichkeiten, die zur Emissionsreduzierung von Software beitragen können und in die Vorhersagen der Kohlenstoffintensität integriert werden können.
 \item \textbf{Beitrag zu nachhaltigen Softwaresystemen} \\
 Die Arbeit hat gezeigt, dass komplexe Zusammenhänge die Kohlenstoffintensität des Stroms für zusätzliche Lastanforderungen bestimmen.
 Die Verwendung der \ac{MOER} als Maß für die Kohlenstoffintensität hat außerdem einen Einfluss auf die Ergebnisse im Vergleich zur Verwendung der durchschnittlichen Kohlenstoffintensität, die bei vorherigen Forschungsarbeiten meist verwendet wurde.
 Es empfiehlt sich, den gesamten Lebenszyklus einer Software für Nachhaltigkeitsoptimierungen zu betrachten.
\end{itemize}
%Eine große Herausforderung besteht darin, das Thema Nachhaltigkeit in der \ac{IT} anzubringen.
%Lange wurde diesem keine Aufmerksamkeit geschenkt und Optimierungen fanden hauptsächlich für anfallende Kosten statt.
%Wenn der Umweltaspekt nicht Motivation genug ist, können noch zwei weitere Aspekte hilfreich sein.
%Zum einen gehen mit der Optimierung des Energieverbrauchs und der Umstellung auf grüne Energiequellen oft verminderte Stromkosten einher, was als Anreiz dienen kann.
%Zum anderen werden immer mehr Vorschriften und Berichtsanforderungen in Bezug auf verursachte Emissionen eingeführt~\cite{GreenSoftwareFoundation.2023}.
%Dies sind weitere Gründe, sich mit der Energienutzung zu beschäftigen.
%
%Der primäre Antrieb für den Übergang zu erneuerbaren Energien mag ökonomischer, nicht ökologischer Natur sein
%
%Inwiefern ist die Vorhersage für Länder sinnvoll auf die Regionen der Datencenter übertragbar?

An dieser Stelle soll die Berechnung der Emissionen einer Software durch den \ac{SCI}-Index (Formel~\ref{eq:sci}) erneut aufgegriffen werden.
Neben der marginalen Kohlenstoffintensität, auf der der Fokus dieser Arbeit liegt, fließen noch weitere Größen in den Wert mit ein.
Es ist deshalb wichtig zu berücksichtigen, dass sich die betrachteten Optimierungsmöglichkeiten auf einen Wert der Formel konzentrierten.
Die Gesamtemissionen von Softwareanwendungen umfassen weitere relevante Faktoren, die im \ac{SCI}-Index berücksichtigt werden.
Die benötigte Energie zu reduzieren oder die verwendete Hardware effizienter zu nutzen kann die Kohlenstoffintensität der Software ebenfalls verringern.
Nachhaltige Software sollte also bestenfalls für alle diese Einflussfaktoren optimiert sein.

Varianz der Stromnetze: Durch die Energiewende werden sich die Stromnetze verändern - sei es kurzfristig durch betriebliche Änderungen oder langfristig durch strukturelle Änderungen.
Diese Einflüsse machen die Emissionen schwer vorhersagbar.
Die Eröffnung eines neuen Rechenzentrums könnte zum Beispiel dazu führen, dass die Grenzstromnetze hochgefahren werden müssen, um die zusätzliche Last zu decken.
Solche zusätzlichen neuen Nachfragen könnten Netzbetreiber aber auch zum Ausbau der Erzeugungskapazitäten führen.
Abhängig von den dafür verwendeten Energiequellen könnten diese neuen Kapazitäten „grüner“ oder auch „brauner“ sein~\cite{WattTime.2022}.
Insgesamt ist bei den globalen Stromnetzen ein Wachstum sauberer Energietechnologien zu verzeichnen.
Dieses ist nötig für das Ziel, bis 2050 weltweit Netto-Null-Emissionen zu erreichen.
Um dieses Ziel zu erfüllen, sieht die Net Zero Roadmap der IEA vor, dass sich die weltweite Kapazität zur Stromerzeugung aus erneuerbaren Energien von 2022 bis 2030 verdreifachen soll.
Nicht zuletzt deshalb werden sich die Stromnetze in den nächsten Jahren ändern.
In Deutschland gibt die Energiewende vor, dass bis 2030 80\% der gesamten Stromversorgung aus erneuerbaren Energiequellen stammen.
Bis 2050 sollen fossile Energiequellen vollständig abgeschafft werden und die gesamte Energie aus erneuerbaren Quellen erzeugt werden.
Außerdem soll im Rahmen des Energieeffizienzgesetzes der Energieverbrauch bis 2030 nur noch etwa einem Fünftel des Verbrauchs im Jahr 2022 entsprechen~\cite{InternationalEnergyAgengy.2023}.

Die Umsetzung der Strategien zur Optimierung der Nachhaltigkeit von Software ist deshalb möglicherweise als Übergangslösung betrachtet werden, um die Zeit hin zu kohlenstoffärmeren Stromnetzen zu überbrücken~\cite{WattTime.12.3.2024}.

Gleichzeitig hat die U.S. Energy Information Administration in ihrem Bericht \textit{International Energy Outlook 2023}~\cite{U.S.EnergyInformationAdministration.2023} zur Untersuchung der weltweiten Energietrends bis zum Jahr 2050 herausgefunden, dass die wachsende Bevölkerung die Auswirkungen der sinkenden Energie- und Kohlenstoffintensität auf Emissionen ausgleichen.
Außerdem wird darin beschrieben, dass die Bedenken bezüglich der Energiesicherheit die Abkehr von fossilen Brennstoffen zwar beschleunigen, in anderen Ländern jedoch zu einem höheren Verbrauch dieser Energiequellen führen.
Diese zwei Erkenntnisse bilden einen Gegensatz gegenüber dem erwarteten Trend zum Sinken der Kohlenstoffintensität der Stromnetze weltweit.


WattTime stellt klar, dass langfristige Vorhersagen der marginalen Emissionen zwar bedeutend, aber auch äußerst herausfordernd sind.
Grund dafür ist unter anderem die hohe zu erwartende Varianz der Stromnetze~\cite{WattTime.2022}.
Bestimmte Ereignisse, seien sie politischer, ökonomischer oder ökologischer Natur, werden immer einen starken Einfluss auf das Stromnetz und dessen vielschichtige Zusammensetzung und Abhängigkeiten haben.
Solche Ereignisse sind kaum oder meistens gar nicht vorhersagbar.
Beispiele aus der Gegenwart und nahen Vergangenheit sind der Russlandkrieg und die Corona-Pandemie oder auch kleinere Ereignisse wie die Abschaltung von Anlagen.

Die Arbeit hat nur einen kleinen Bereich der Nachhaltigkeit von Software beleuchtet.
Neben dem bewussten Verbrauch von Kohlenstoff, gibt es eine Reihe weiterer Prinzipien, wie in Kapitel\ref{CAP:sustainable-software} beschrieben.
Eine im  Bezug auf Nachhaltigkeit optimale Software würde alle diese Prinzipien beachten.
Außerdem sollte Nachhaltigkeit auf alle Ebenen der Software angewandt werden.
Beginnend bei der Architektur, über Programmiersprachen und Code-Prinzipien, bis hin zu Bereitstellung und Betrieb sollte die Software vollumfänglich für Nachhaltigkeit optimiert werden.
Eine wichtige Rolle spielt dabei auch die regelmäßige Messung und Überprüfung, um eine Einordnung zu ermöglichen.
%!
%!
\chapter{Diskussion}
\section{Daten}
%!
%!
\chapter{Fazit und Ausblick}\label{CAP:resumee}
Angesichts des rasanten Anstiegs der Rechenleistung, bedingt unter anderem durch Anwendungen in den Bereichen \ac{KI}, Bitcoin oder allgemeine Computernutzung, wird sich der Energiebedarf von Rechenzentren in den nächsten zwei Jahren voraussichtlich verdoppeln.
% Voraussichtlich wird sich der Energieverbrauch von Rechenzentren durch den starken Anstieg der Rechenleistung in den nächsten zwei Jahren verdoppeln.
Vor diesem Hintergrund haben sich die Lastanpassung und -verschiebung als vielversprechende Methoden herausgestellt~\cite{WattTime.12.3.2024}.
Diese Arbeit untersuchte das Potenzial der Nachhaltigkeitsoptimierung von Software anhand der Zeitreihenvorhersage der marginalen Kohlenstoffintensität zwei spezifischer Stromnetze.
Neben der Vorhersage durch ein statistisches Modell wurde mit dem \ac{TFT} auch ein Neuronales Netz trainiert und die Ergebnisse der beiden Modelle verglichen.
Für die Optimierungsmöglichkeiten wurden unterschiedliche Strategien untersucht und die Anwendbarkeit der zeitlichen und örtlichen Verschiebung für verschiedene Workloads bewertet.

Die Ergebnisse zeigen sowohl für Deutschland, als auch für Norwegen, ein sich täglich wiederholendes Muster im Verlauf der marginalen Kohlenstoffintensität, wobei sich die Muster je Land geringfügig unterscheiden.
Die Minimalwerte zur Mittagszeit können für eine generelle Verschiebung in diese Zeiten genutzt werden.
Darüber hinaus kann die Vorhersage der Intensität eine Woche oder einen Monat im Voraus jederzeit unter Angabe von Dauer und Fälligkeitsdatum des betrachteten Workloads abgerufen werden.

Cloud- und Service-Anbieter sollten Nutzer ermutigen, ihre Workloads möglichst zeitlich flexibel und unterbrechbar zu gestalten und sie dementsprechend zu deklarieren.
Spot-Instanzen sind bereits bei vielen Anbietern verfügbar, sodass die Cloud besser ausgelastet und zugleich niedrige Kosten angeboten werden können.
Nicht zuletzt zwingt die Bepreisung von Kohlendioxid, die laut World Bank 2023 ein Rekordhoch erreicht hat~\cite{WorldBank.2023}, Anbieter und Nutzer dazu, sich mit diesem Thema zu beschäftigen.
Neben finanziellen Anreizen, sollten Anbieter vermehrt Wissen über Kohlenstoffintensität und die adäquate Gestaltung von Workloads zur Verfügung stellen.
Eine einfache Änderung wäre die Angabe von Zeitfenstern für Scheduling-Startzeitpunkte (z.B. nachts) anstelle konkreter Zeitpunkte (z.B. um 1 Uhr) für Workloads, sodass das Potenzial für Kohlenstoffeinsparungen genutzt werden kann~\cite{Wiesner.2021}.

Qualitativ hochwertige Prognosen sowohl der Kohlenstoffintensität als auch von Workloads stehen im Mittelpunkt erfolgreicher kohlenstoffbewusster Verlagerungen.
Wichtig sind dabei vor allem auch Informationen über die Workloads, wie zeitliche Beschränkungen, erwartete Dauer und, ob eine Unterbrechung möglich ist.
Zur Unterstützung könnten Tools zur Bereitstellung dieser Informationen eingesetzt werden.
So könnten Schnittstellen bereitgestellt werden, um zeitliche Beschränkungen und andere Eigenschaften programmatisch zu deklarieren oder diese automatisch zu erkennen.
Die benötigte Zeit für das Anhalten und Wiederaufnehmen eines Workloads könnte erfasst und dieser anhand dessen automatisch als unterbrechbar oder nicht unterbrechbar gekennzeichnet werden
Zeitliche Beschränkungen könnten zum Beispiel vom Abhängigkeitsgraphen eines Workloads abgeleitet werden~\cite{Wiesner.2021}.

Anschließende Forschungsarbeiten könnten die regionale Auflösung der Prognosedaten erhöhen.
Verwendete die vorliegende Arbeit Daten auf Länderebene, so wäre ein sinnvoller nächster Schritt, dies noch weiter in verschiedene Zonen einzuteilen.
Limitierender Faktor ist dabei momentan die Verfügbarkeit von Daten zur marginalen Kohlenstoffintensität, die gegenwärtig nur Länder-spezifisch zur Verfügung stehen.
Eine solche feinere Untergliederung könnte die Präzision und Relevanz der Daten für den Einsatz in Rechenzentren wesentlich verbessern.

Darüber hinaus könnten zukünftige Untersuchungen von einer Verbesserung und realitätsnäheren Modellierung der \ac{MOER}-Daten profitieren.
Da sich die Methoden zur Modellierung und Überprüfung momentan noch in einem kontinuierlichen Entwicklungsprozess befinden und mit WattTime als einzigem Anbieter solcher spezifischen Daten - zur marginalen Emissionsrate, nicht allgemein zur Kohlenstoffintensität - derzeit noch ein Monopol besteht, ist in Zukunft mit einer erhöhten Qualität der Daten zu rechnen.
Außerdem wird mit fortschreitender Zeit eine größere Datenmenge zur Verfügung stehen, weil der Beginn der Datenmodellierung dann schon weiter zurückliegen wird.

Darüber hinaus könnten die Prognosemodelle mit weiteren, die \ac{MOER} beeinflussenden Eingabevariablen erstellt und trainiert werden.
Auf diese Weise könnte ein Vergleich von Modellgenauigkeit und Trainingsaufwand je verwendeter Variablen aufgestellt werden, um herauszufinden, welche Kombination dieser Eingabevariablen die besten Ergebnisse unter akzeptablem Trainingsaufwand erreichen.


